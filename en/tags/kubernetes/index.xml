<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on nanta - Data Engineering</title><link>https://nanta-data.dev/en/tags/kubernetes/</link><description>Recent content in Kubernetes on nanta - Data Engineering</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2026 nanta</copyright><lastBuildDate>Fri, 27 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://nanta-data.dev/en/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Adopting Trino Gateway: Zero-Downtime Deployments and Multi-Cluster Routing</title><link>https://nanta-data.dev/en/posts/trino-gateway-zero-downtime/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/trino-gateway-zero-downtime/</guid><description>Trino doesn&amp;rsquo;t support coordinator HA. Redeploying the coordinator means downtime. We adopted Trino Gateway to enable Blue/Green deployments with zero downtime and route BI/OLAP queries to separate clusters based on HTTP headers.</description></item><item><title>Trino Alluxio Cache PoC: EBS Throughput Was the Bottleneck</title><link>https://nanta-data.dev/en/posts/trino-alluxio-cache-poc/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/trino-alluxio-cache-poc/</guid><description>We ran a PoC of Trino&amp;rsquo;s Alluxio-based file system cache in a production OLAP environment. Adding the cache alone didn&amp;rsquo;t help much. The default EBS throughput of 125 MiB/s was the bottleneck. After bumping it to 1000 MiB/s, query performance improved visibly and S3 API costs dropped by ~$3,200/month.</description></item><item><title>Building an ALB Log Pipeline with Filebeat, Flink, and Iceberg</title><link>https://nanta-data.dev/en/posts/alb-log-collection-system/</link><pubDate>Wed, 25 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/alb-log-collection-system/</guid><description>We needed to turn CSV ALB logs sitting in S3 into a queryable Iceberg table. The first attempt with Flink&amp;rsquo;s filesystem connector ran out of memory. This post covers the redesign with Filebeat + SQS + Kafka, autoscaling, checkpoint tuning, and the operational surprises we hit along the way.</description></item><item><title>Building a Proxy Server with Apache APISIX: De-identifying 1.3 Billion Daily Logs</title><link>https://nanta-data.dev/en/posts/apisix-proxy-server-guide/</link><pubDate>Tue, 24 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/apisix-proxy-server-guide/</guid><description>We needed to strip PII from app logs before sending them to an overseas tracking server. This post covers our journey from Fluent-bit to Kong to APISIX, custom Lua plugin development, 55K TPS load testing, and Kubernetes deployment.</description></item><item><title>NVIDIA MIG Setup Guide: Splitting One A100 GPU Into Many</title><link>https://nanta-data.dev/en/posts/nvidia-mig-setup-guide/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/nvidia-mig-setup-guide/</guid><description>Four A100 GPUs can yield up to 28 independent GPU instances. This guide covers MIG concepts, mig-parted installation, diverse slice configurations, and Kubernetes integration based on hands-on experience.</description></item></channel></rss>