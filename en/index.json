[{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/tags/alluxio/","section":"Tags","summary":"","title":"Alluxio","type":"tags"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/tags/cache/","section":"Tags","summary":"","title":"Cache","type":"tags"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/categories/data-engineering/","section":"Categories","summary":"","title":"Data Engineering","type":"categories"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/tags/ebs/","section":"Tags","summary":"","title":"Ebs","type":"tags"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/","section":"nanta - Data Engineering","summary":"","title":"nanta - Data Engineering","type":"page"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/tags/performance/","section":"Tags","summary":"","title":"Performance","type":"tags"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/tags/s3/","section":"Tags","summary":"","title":"S3","type":"tags"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"27 February 2026","externalUrl":null,"permalink":"/en/tags/trino/","section":"Tags","summary":"","title":"Trino","type":"tags"},{"content":"We were running Trino as the primary query engine for our OLAP workloads. While evaluating ClickHouse as an alternative, we decided that improving Trino was the more practical path — keeping the tech stack unified is worth a lot when you\u0026rsquo;re the team maintaining it. So we upgraded Trino from v433 to v451. The main goal was to use the Alluxio-based file system cache.\nThe previous Rubix cache had been deprecated. Alluxio cache was added in v439 and had a race condition bug fixed in v445, making it production-ready.\nThis post covers the PoC process and the bottleneck we didn\u0026rsquo;t expect.\nWhat Alluxio Cache Does # Every time Trino reads data from S3, it goes over the network. Even if the same file is read multiple times, each read triggers an S3 API call. Alluxio cache stores fetched data on the worker node\u0026rsquo;s local disk so subsequent reads come from local storage.\nA few lines in the catalog config enable it.\nfs.cache.enabled=true fs.cache.max-sizes=50GB fs.cache.directories=/mnt/cache It works with Hive, Iceberg, and Delta Lake connectors. We enabled it on two catalogs: hive_zeppelin and iceberg.\nConstraints # Before starting the PoC, there were several constraints worth noting.\nCache Is Not Shared Across Catalogs or Clusters # Even if multiple catalogs point at the same S3 bucket, each catalog maintains its own cache. The connector architecture instantiates the file system independently per catalog. Cross-cluster sharing is out of the question.\nA native Alluxio file system PR was merged in September 2024 (Trino 460). It enables cache sharing between catalogs and even between clusters. Whether accessing a shared cache file system is actually faster than hitting S3 directly still needs separate validation.\nSpot Instances and Caching Don\u0026rsquo;t Mix Well # Cache data lifetime is tied to the worker node lifetime. Node goes away, cache goes with it.\nOur environment runs nearly 99% on spot instances. Spot reclamation wipes the cache on that worker. Scale-in from autoscaling does the same. Cache warm-up takes time, and if nodes churn frequently, you barely get to benefit from it.\nNo Schema-Level Control # Cache activation is all-or-nothing at the catalog level. Tables under temp schemas that are almost never reused still get cached. Selective caching for specific schemas would be ideal, but it\u0026rsquo;s not supported yet.\nA PR to add fs.cache.skip-paths existed but was closed due to design disagreements. Reviewers argued that control should happen at the schema/table level, not the file path level.\nPoC Setup # Apply to One Side of Blue/Green Only # Building a staging environment identical to production wasn\u0026rsquo;t feasible cost-wise. Instead, we applied the cache to only the Blue cluster in both our OLAP and BI setups, then compared query performance metrics over 1-2 weeks.\nA Trino Gateway sits in front, distributing queries based on the current query count per backend. The queries hitting Blue vs. Green aren\u0026rsquo;t identical, but over two weeks the sample size is large enough for a meaningful comparison.\nVolume Configuration # Mapping one worker pod to one node is the Trino best practice. You could use StatefulSet with volumeClaimTemplate for per-pod volumes, but we chose to mount a dedicated EBS cache volume on each node and access it via hostPath from the worker pod.\nInitial cache volume: 50 GB per worker. 60% allocated to the hive_zeppelin catalog, 30% to iceberg.\nRollout Order # OLAP Blue cluster (2024-07-30) BI Blue cluster (2024-07-31) Added Grafana panels for p25/p50/p75/p99/avg/max query execution time comparison Monitored cache utilization via JMX metrics BI clusters were expected to benefit more from caching. Dashboard queries tend to hit the same tables with the same date ranges across multiple charts.\nThe EBS Throughput Bottleneck # We turned on the cache and watched for a few days. No dramatic difference. Cache hit rates looked fine, so why wasn\u0026rsquo;t performance improving?\nWe dug into the cache volume metrics. EBS write throughput was constantly hitting the configured maximum of 125 MiB/s. The gp3 volume\u0026rsquo;s baseline throughput is 125 MiB/s, and cache writes were saturating it.\nIt doesn\u0026rsquo;t matter how fast the cache is if the disk can\u0026rsquo;t keep up with writes.\nBumping Throughput # We raised it to 1000 MiB/s, the maximum configurable throughput for gp3.\nBI cluster: applied around 2:45 PM OLAP cluster: applied around 3:45 PM The effect was immediate. After the throughput bump, the cache-enabled cluster (Blue) showed clearly better query performance. Write throughput reached around 200 MiB/s at peak but had plenty of headroom under the 1000 MiB/s ceiling. Combined read + write throughput was roughly 300 MiB/s.\nWe confirmed that io2 volumes weren\u0026rsquo;t necessary — io2 caps out at the same 1000 MiB/s maximum throughput, and we didn\u0026rsquo;t need the high IOPS it offers.\nCache Volume Sizing # The initial 50 GB filled up fast. The allocated maximum (hive 60% + iceberg 30% = 45 GB) was fully utilized. When cache space runs out, the oldest data gets evicted. If eviction happens too aggressively, cache effectiveness drops.\nWe scaled up the volumes.\nCluster Before After BI workers 50 GB 150 GB → 1.5 TB OLAP workers 50 GB 2 TB OLAP coordinator - 1 TB (for iceberg metadata cache) We also adjusted the per-catalog allocation ratios based on actual usage. Hive was consuming far more cache than iceberg, so we shifted from hive 60% / iceberg 30% to hive 85% / iceberg 10%.\nEBS storage cost was a concern, but it turned out to be small. The ~20% reduction in worker count from improved cache performance more than offset the additional EBS cost.\nOur approach: overprovision initially, add Grafana charts for per-node cache volume utilization, observe, then trim if wasteful.\nFIFO Can Beat LRU # Trino\u0026rsquo;s Alluxio cache doesn\u0026rsquo;t use a sophisticated eviction algorithm like LRU. It uses FIFO — first cached, first evicted.\nIntuitively LRU seems better, but FIFO has advantages in certain scenarios.\nRemoves one-hit wonders quickly. Data read once and never again gets evicted fast. LRU would update the access timestamp, potentially keeping such data around longer. SSD-friendly. FIFO minimizes random access patterns. On SSD/flash storage, this means less write amplification. The simple eviction policy reflects the feature\u0026rsquo;s early stage, but for OLAP workloads, FIFO is not necessarily a bad choice.\nResults # Query Performance # After enabling the cache and bumping EBS throughput, the Blue cluster was consistently faster than Green.\nThe interesting part: Blue recorded higher query throughput with fewer workers. Because Blue processed queries faster, the gateway routed more queries to it. Blue ended up handling more total queries while running ~20% fewer workers than Green.\nCache warm-up took about 3 hours. Performance differences only became visible after workers had been up long enough to fill their caches.\nS3 API Cost Reduction # S3 GetObject costs dropped by roughly $3,200/month after cache activation. Other factors like query volume changes may have contributed, but the cost reduction was substantial.\nCost Summary # Item Change S3 API costs ~$3,200/month reduction EBS storage costs Small increase (cache volumes) Node costs Reduction from ~20% fewer workers Instance Store Consideration # EBS is remote block storage. Compared to direct S3 access, the cache performance improvement might not be dramatic. If EBS caching hadn\u0026rsquo;t shown sufficient results, we planned to switch to node types with NVMe SSD instance stores.\nInstance Type Instance Store r7gd.4xlarge 1 x 950 GB NVMe SSD m7gd.8xlarge 1 x 1,900 GB NVMe SSD The two types have different instance store sizes, but Trino\u0026rsquo;s cache config supports percentage-based sizing relative to total disk capacity, so this isn\u0026rsquo;t a problem. In the end, bumping EBS throughput alone was sufficient, and we didn\u0026rsquo;t proceed with the instance store switch.\nBonus: Project Hummingbird # While upgrading to v451, we noticed another development worth watching. Project Hummingbird is Trino\u0026rsquo;s ongoing performance improvement initiative using Java 22\u0026rsquo;s Vector API for vectorized execution.\nVectorized decoding for Parquet file reads landed in v448. It requires 256-bit or larger vector registers, so it\u0026rsquo;s disabled on Graviton 2 instances (r6g, m6g). Graviton 3 and later enable it automatically.\nTrino requiring Java 22 starting from v447 is part of this same initiative.\nTakeaways # The PoC taught us one clear lesson.\nAdding a cache isn\u0026rsquo;t enough. If disk I/O is the bottleneck, cache hit rate doesn\u0026rsquo;t matter. The default EBS gp3 throughput of 125 MiB/s was the ceiling for cache performance. The moment we raised it to 1000 MiB/s, the difference showed.\nBI workloads benefit most from caching. Dashboard queries that repeatedly hit the same tables see high cache hit rates.\nCaching on spot instances requires compromise. Frequent node churn means cache warm-up time is effectively wasted. Still, after the 3-hour warm-up period, the effect was there. Not useless, but not ideal.\nLooking ahead, the native Alluxio file system (Trino 460) will enable cross-catalog and cross-cluster cache sharing once it stabilizes. That could also mitigate cache loss from spot reclamation.\nReferences:\nA cache refresh for Trino Trino File System Cache Documentation Alluxio cache PR (v439) Cache race condition fix (v445) Cross-catalog cache sharing issue Native Alluxio file system PR (v460) Project Hummingbird Vectorized Parquet decoding PR (v448) ","date":"27 February 2026","externalUrl":null,"permalink":"/en/posts/trino-alluxio-cache-poc/","section":"Posts","summary":"We ran a PoC of Trino’s Alluxio-based file system cache in a production OLAP environment. Adding the cache alone didn’t help much. The default EBS throughput of 125 MiB/s was the bottleneck. After bumping it to 1000 MiB/s, query performance improved visibly and S3 API costs dropped by ~$3,200/month.","title":"Trino Alluxio Cache PoC: EBS Throughput Was the Bottleneck","type":"posts"},{"content":"","date":"25 February 2026","externalUrl":null,"permalink":"/en/tags/alb/","section":"Tags","summary":"","title":"Alb","type":"tags"},{"content":"The cloud infra team came to us with a request. They wanted ALB logs available as a queryable table for post-incident analysis.\nALB logs were already landing in S3 as CSV files. When someone needed them, they\u0026rsquo;d query the files through Athena directly. Slow and painful. Text format meant query performance had a hard ceiling. Convert those logs to Parquet via Iceberg and you get something you can build a real-time dashboard on.\nThis post covers what we built and everything that went wrong along the way.\nFirst Design: Flink Filesystem Connector # We started with the simplest possible architecture.\nS3 files → Flink (filesystem connector) → Iceberg table Flink\u0026rsquo;s filesystem connector watches an S3 bucket. New file appears, connector reads it, emits each line as a record. Attach an Iceberg sink and you\u0026rsquo;re done.\nThe problem was memory. The filesystem connector keeps the entire list of processed files in Flink state. ALB logs generate tens of thousands of files per day. State kept growing. Memory kept climbing. Eventually it crashed. We hit a scalability wall and had to rethink the architecture.\nCurrent Architecture: S3 → SQS → Filebeat → Kafka → Flink → Iceberg # We pulled file monitoring out of Flink. A message queue and a lightweight collector sit in front now.\nS3 bucket → SQS (file events) → Filebeat → Kafka → Flink → Iceberg Each stage does one thing.\nS3 → SQS: Bucket notification config pushes a message to SQS every time a new ALB log file lands SQS → Filebeat → Kafka: Filebeat consumes SQS messages, fetches the S3 file directly, and sends each line to a Kafka topic Kafka → Flink → Iceberg: Flink reads from Kafka, parses CSV, extracts metadata from the file path (account number, service name, role), and writes to an Iceberg table Flink only talks to Kafka now. No more file lists in state.\nHow We Ended Up on Filebeat # We didn\u0026rsquo;t start with Filebeat. We tried Kafka Connect Filesystem Connector first. That failed. Here\u0026rsquo;s what happened.\nKafka Connect Filesystem Connector — Abandoned # This was a third-party connector with several problems.\nNot on Maven. Had to build from source Last GitHub commit was 4 years old. Java 8 codebase Messages were published more than once. Duplicate delivery with no clear fix S3 file move (cleanup) didn\u0026rsquo;t work. FileUtil.copy() breaks on S3A filesystems We decided that forcing an unmaintained tool to work was worse than picking something well-supported.\nFilebeat Bucket Listing Mode — Can\u0026rsquo;t Scale # Filebeat supports two modes for S3 input. We tried bucket listing mode first. It periodically scans S3 for new files.\nIt worked. But it couldn\u0026rsquo;t scale horizontally. Multiple Filebeat instances would process the same files because they don\u0026rsquo;t know about each other. The official docs say it explicitly: bucket listing mode means a single instance with vertical scaling only.\nThere was another issue. The backup feature (backup_to_bucket_arn) that moves processed files to a separate bucket didn\u0026rsquo;t work in most Filebeat versions with bucket listing mode. A GitHub Issue showed the fix had been merged then accidentally reverted. Only version 8.15.3 worked correctly.\nSwitching to SQS Mode # SQS mode supports horizontal scaling. SQS handles message distribution, so multiple Filebeat pods process different files without overlap. S3 API costs go down too.\nThe tradeoff: no backfill of existing files. SQS mode only processes files that trigger events after setup. If you need historical files, you publish SQS events manually. Not a problem for our ALB log use case.\nOne more thing. SQS mode flat-out rejects backup_to_bucket_arn. Config validation fails at startup. Makes sense when you think about it — with SQS there\u0026rsquo;s no need to separate new files from processed ones.\nFinal Filebeat config:\nfilebeat.inputs: - type: aws-s3 queue_url: https://sqs.ap-northeast-2.amazonaws.com/xxxx/s3-elb-logs-events file_selectors: - regex: \u0026#34;.*\\\\.log\\\\.gz\u0026#34; number_of_workers: 64 decompress: true codec.line: format: message output.kafka: hosts: - kafka-cluster:9092 topic: cloudinfra.prod.streaming.alb-access-log.csv codec.format: string: \u0026#39;%{[aws.s3.object.key]} %{[message]}\u0026#39; compression: zstd The key detail is codec.format. It prepends the S3 file path (aws.s3.object.key) to each message. Flink parses this path downstream to extract account numbers and service names.\nAutoscaling # Filebeat: SQS-Based KEDA Scaling # We configured a KEDA ScaledObject using SQS Visible Messages as the trigger.\nspec: cooldownPeriod: 300 maxReplicaCount: 128 minReplicaCount: 1 pollingInterval: 30 triggers: - type: aws-sqs-queue metadata: queueLength: \u0026#39;50\u0026#39; queueURL: https://sqs.ap-northeast-2.amazonaws.com/xxxx/s3-elb-logs-events First attempt failed. KEDA\u0026rsquo;s operator couldn\u0026rsquo;t read SQS metrics due to IAM permissions. Setting identityOwner: operator in the ScaledObject makes KEDA use its own role instead of the pod\u0026rsquo;s. We added sqs:GetQueueAttributes to that role and it worked.\nFlink: K8s Operator Autoscaling # Flink uses the Kubernetes Operator\u0026rsquo;s built-in autoscaler.\njob.autoscaler.target.utilization: \u0026#34;0.75\u0026#34; job.autoscaler.target.utilization.boundary: \u0026#34;0.15\u0026#34; pipeline.max-parallelism: \u0026#39;480\u0026#39; Flink Checkpoint Tuning # We bumped the checkpoint interval from 1 minute to 5 minutes. A chain of failures followed.\nHeartbeat Timeout # Longer checkpoints meant TaskManagers couldn\u0026rsquo;t send heartbeats in time. Default timeout was too short.\nheartbeat.interval: \u0026#39;60000\u0026#39; heartbeat.timeout: \u0026#39;300000\u0026#39; pekko.ask.timeout: 10m OOM: Java Heap Space # Longer checkpoint intervals meant more Kafka fetch data sitting in the heap. We bumped pod memory from 2G to 4G to 6G. Same crash every time.\nThe real problem: JVM Task Heap was only allocated 2.32GB even though the pod had plenty of memory overall. The Kafka fetch buffer filled that up. Fix was setting taskmanager.memory.task.heap.size explicitly.\ntaskmanager.memory.task.heap.size: 4608m taskmanager.memory.managed.size: 512m taskmanager.memory.network.fraction: \u0026#39;0.02\u0026#39; Final Config # After several rounds of tuning we got the checkpoint interval up to 10 minutes.\nexecution.checkpointing.interval: 10m execution.checkpointing.timeout: 5m heartbeat.interval: \u0026#39;60000\u0026#39; heartbeat.timeout: \u0026#39;300000\u0026#39; pekko.ask.timeout: 10m taskmanager.memory.task.heap.size: 4608m taskmanager.memory.managed.size: 512m taskmanager.memory.network.fraction: \u0026#39;0.02\u0026#39; Compaction Failures and Dropping Upsert # Even after the 10-minute checkpoint interval, Iceberg compaction kept failing. We decided to eliminate every suspected cause.\nCreated a new Iceberg table. The old table\u0026rsquo;s metadata might have been in a bad state. Fresh table, clean start. Removed upsert logic. We\u0026rsquo;d been using UPSERT mode to prevent duplicates. That was causing compaction conflicts. Dropping upsert means duplicates can appear. We checked. Duplicates did exist — but they weren\u0026rsquo;t from the pipeline. The source ALB logs themselves had duplicates. We switched to append-only. Compaction started succeeding immediately.\nThe Column Count Incident # One morning around 4 AM, Flink\u0026rsquo;s CSV parsing started failing. ALB log columns had jumped from 31 to 34. AWS changed the format with no advance notice.\nFirst Fix: Dead Letter Queue # We applied a csv-dlq format to route parse failures to a DLQ topic. The app recovered, but a new problem appeared. Format errors flooded the TaskManager logs. Ephemeral storage filled up. Pods got evicted in a loop.\nThe node was low on resource: ephemeral-storage. We fixed it by adjusting log4j to suppress format error logging.\nlogger.format.name = com.woowahan.dataservice.format logger.format.level = ERROR Second Problem: DLQ Topic Overload # The DLQ topic received so many messages it put load on the entire Kafka cluster. We rolled back the DLQ approach and switched to csv.ignore-parse-error instead. Added 3 dummy columns to the source table definition so it accepts both 31-column and 34-column logs.\nDeduplication: UPSERT and Primary Keys # We initially used Iceberg UPSERT for deduplication. The challenge: ALB logs don\u0026rsquo;t have a unique identifier.\nWe analyzed 467K log records and found that a combination of file_path, time, http_request, client_addr, target_addr, and request_creation_time was nearly unique. Only 1 duplicate existed across the entire dataset, and those 2 records were identical in every field — genuinely indistinguishable logs.\nSetting a primary key in Iceberg required Flink SQL\u0026rsquo;s SET IDENTIFIER FIELDS. Spark SQL doesn\u0026rsquo;t support PRIMARY KEY syntax. Flink SQL doesn\u0026rsquo;t support bucketing or hidden partitioning. Creating the table was harder than expected.\nAs described above, we ultimately dropped UPSERT for compaction stability.\nMonitoring # Key Metrics # Metric What It Tells You SQS Approximate Number Of Messages Visible Files waiting to be processed SQS Approximate Number Of Messages Not Visible Files Filebeat is currently processing Kafka consumer lag How far behind Flink is Flink job status Whether the app is running Alerts # SQS: Visible Messages above threshold (backlog building up) Filebeat: High CPU/memory usage, pod not running Flink: Job transitions to non-running state Takeaways # Three lessons stand out.\nDon\u0026rsquo;t use Flink for file watching. The filesystem connector holds processed file lists in state. Run it long enough and memory blows up. Let SQS + Filebeat handle file discovery. Let Flink do what it\u0026rsquo;s good at — stream processing.\nUpsert and compaction don\u0026rsquo;t mix well. This is the Iceberg position delete problem. For log data, append-only is far more stable operationally. Handle source-level duplicates on the consumer side.\nAWS can change log formats without warning. ALB log columns went from 31 to 34 overnight. If your pipeline parses CSV, defensive options like ignore-parse-error aren\u0026rsquo;t optional.\nReferences:\nFilebeat AWS S3 Input Flink Filesystem Connector Apache Iceberg - Flink Writes KEDA AWS SQS Queue Scaler ","date":"25 February 2026","externalUrl":null,"permalink":"/en/posts/alb-log-collection-system/","section":"Posts","summary":"We needed to turn CSV ALB logs sitting in S3 into a queryable Iceberg table. The first attempt with Flink’s filesystem connector ran out of memory. This post covers the redesign with Filebeat + SQS + Kafka, autoscaling, checkpoint tuning, and the operational surprises we hit along the way.","title":"Building an ALB Log Pipeline with Filebeat, Flink, and Iceberg","type":"posts"},{"content":"","date":"25 February 2026","externalUrl":null,"permalink":"/en/tags/filebeat/","section":"Tags","summary":"","title":"Filebeat","type":"tags"},{"content":"","date":"25 February 2026","externalUrl":null,"permalink":"/en/tags/flink/","section":"Tags","summary":"","title":"Flink","type":"tags"},{"content":"","date":"25 February 2026","externalUrl":null,"permalink":"/en/tags/iceberg/","section":"Tags","summary":"","title":"Iceberg","type":"tags"},{"content":"","date":"25 February 2026","externalUrl":null,"permalink":"/en/tags/kafka/","section":"Tags","summary":"","title":"Kafka","type":"tags"},{"content":"","date":"25 February 2026","externalUrl":null,"permalink":"/en/tags/sqs/","section":"Tags","summary":"","title":"Sqs","type":"tags"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/apisix/","section":"Tags","summary":"","title":"Apisix","type":"tags"},{"content":"We had to send app logs to an overseas tracking server. Problem: the logs contained personally identifiable information — user IDs, device IDs, order numbers. Privacy law says you can\u0026rsquo;t send that abroad in plaintext.\nThe fix: put a proxy in the middle. It receives logs from the app, hashes PII fields, forwards the sanitized data overseas. At the same time it publishes the original data to an internal Kafka cluster so teams like ads and recommendations can still use it.\nThis post documents how we built that proxy with Apache APISIX, handling 1.3 billion logs per day.\nWhy APISIX # We Started with Fluent-bit # Log collection, so Fluent-bit felt natural. But once we listed the actual requirements, it was clear we needed an API gateway, not a log forwarder.\nParse and modify HTTP request bodies (hash PII fields) Pass the upstream server\u0026rsquo;s responses (including 400 errors) back to the client Publish original data to Kafka at the same time Fluent-bit can\u0026rsquo;t do any of that cleanly.\nKong vs APISIX # Both are nginx-based and support Lua scripting. We tried Kong first. It failed at the Helm chart deployment stage due to a known issue.\nAPISIX won for three reasons:\nLightweight: Lower memory footprint and higher throughput per core than Kong Declarative routing: Send config via Admin API, it gets stored in etcd permanently Clean plugin pipeline: Requests flow through rewrite → access → header_filter → body_filter → log stages. Easy to inject logic at exactly the right point Custom Plugin Development # Why Built-in Plugins Weren\u0026rsquo;t Enough # APISIX ships with a kafka-logger plugin that sends request data to Kafka. The catch: if we run our de-identification plugin (encrypt-pii) first, kafka-logger sees the hashed data, not the original.\nWhat we actually needed:\nApp → APISIX → (1) Publish original to Kafka → (2) Hash PII fields → (3) Forward hashed data to overseas server → (4) Return server\u0026#39;s response to app Order matters. Original goes to Kafka first, then we hash. No existing plugin combo guarantees that ordering, so we built a single custom plugin combining kafka-logger and de-identification.\nPII Hashing in Lua # APISIX runs on LuaJIT. We used lua-resty-string for SHA-256:\nlocal resty_sha256 = require \u0026#34;resty.sha256\u0026#34; local str = require \u0026#34;resty.string\u0026#34; local function hash_value(value) local sha256 = resty_sha256:new() sha256:update(value) local digest = sha256:final() return str.to_hex(digest) end Three fields get hashed: user ID, device ID, and order number. The plugin parses the JSON body, finds target fields, hashes them, and re-serializes.\nKafka Publishing # Same plugin uses lua-resty-kafka to send the original body to Kafka:\nlocal producer = require \u0026#34;resty.kafka.producer\u0026#34; local broker_list = { { host = \u0026#34;kafka-broker-01\u0026#34;, port = 9092 }, } local p = producer:new(broker_list, { producer_type = \u0026#34;async\u0026#34; }) local ok, err = p:send(\u0026#34;app-log-topic\u0026#34;, nil, original_body) Dependencies needed:\nluarocks install lua-cjson luarocks install penlight We hit two issues during development:\nNo timestamp in Kafka messages: Fixed by bumping Kafka API version from 1 to 2 Metadata fetch failures: Happened on API version 2, so we dropped back to 1 Ended up using API version 2 for message publishing and version 1 for metadata queries.\nArchitecture Evolution # The final design looked nothing like our first sketch.\nInitial Design: Proxy → Kafka → Flink → Overseas # App → APISIX → Kafka → Flink (de-identification) → Tracking Server Proxy just forwards to Kafka. Flink handles PII hashing and sends data onward. Clean separation, but one fatal flaw: when the tracking server returns HTTP 400 for a bad request, there\u0026rsquo;s no way to pass that back to the client. Once you hand off to Kafka, the response path is gone.\nFinal Design: Proxy Does Everything # App → APISIX → (original → Kafka) → (hashed → Tracking Server → response → App) The proxy handles de-identification directly and forwards to the overseas server.\nUpside:\nClient gets the tracking server\u0026rsquo;s actual response, 400s included No Flink application needed Real-time processing Downside:\nAll load concentrates on the proxy Body parsing + hashing + Kafka publish happens in a single request The load concern was real, but APISIX handled it. Numbers below.\nLoad Testing # Setup # We used nGrinder. Proxy pods ran on 2 cores, 4GiB memory each.\nThroughput Per Core # APISIX docs claim 10,000 QPS per core. With our Lua scripting overhead (body parsing, hashing, Kafka publish), real numbers are lower.\nMeasured results (2 cores per pod, 90-second runs):\nVusers Peak TPS CPU Usage Errors 990 2,381 32% 0 1,980 4,907 48% 1 3,960 7,000 99% 0 Safe operating point: ~5,000 TPS on 2 cores. Our peak traffic hits 55,000 TPS, so 12 pods cover it.\nTuning That Mattered # Keepalive: Before enabling it, p95 latency jumped around on every request. After: p95 under 10ms, often under 1ms after warm-up.\nnginx worker_connections: APISIX defaults to auto, which counts node cores, not pod cores. In containers this creates way too many workers and causes CPU contention. Set it manually to match your pod\u0026rsquo;s core count. Small gain, but measurable — CPU dropped while TPS went up.\nScale-out strategy: When traffic spikes hit, reactive scaling is too slow. CPU pegs at 100% before new pods come up, and errors start flying. Unlike stream processing, there\u0026rsquo;s no backpressure here. We used two approaches:\nKEDA with CPU threshold at 50% — aggressive, on purpose Schedule-based scaling — pre-scale before known peak times (lunch, dinner) 502 Errors # Intermittent 502s showed up during testing. Root causes:\nThe overseas server\u0026rsquo;s staging environment ran on spot instances — responses were flaky CPU saturation before scale-out kicked in nGrinder artifact: too many vusers per agent causes client-side network bottlenecks that make server latency look worse than it is In production with conservative min-pod counts and schedule-based scaling, 502s disappeared.\nKubernetes Deployment # Helm Chart # We deployed APISIX in decoupled mode — separate control-plane and data-plane.\nControl-plane (apisix-control): Deploys with etcd. Manages routing config Data-plane (apisix-data): Handles actual traffic. Connects to control-plane\u0026rsquo;s etcd via externalEtcd config etcd uses a PVC (gp3) for persistent storage. Standard EKS defaults to gp3, so no extra config needed.\nAutoscaling (KEDA) # # KEDA ScaledObject (simplified) triggers: - type: prometheus metadata: query: avg(rate(container_cpu_usage_seconds_total{...}[1m])) * 100 threshold: \u0026#34;50\u0026#34; - type: memory metadata: value: \u0026#34;80\u0026#34; Scale out at 50% CPU or 80% memory. Combined with schedule-based scaling for peak hours.\nSecurity # The public-facing reverse proxy ALB got AWS WAF attached. Our security team reviewed it and confirmed WAF alone was sufficient for client-facing protection.\nMonitoring # We built a Grafana dashboard tracking:\nSystem: CPU and memory utilization per pod Nginx: Total requests, accepted/handled connections, connection state HTTP: RPS by status code, RPS per service/route Latency: APISIX latency, upstream latency, total request latency Bandwidth: Ingress/egress per service and route etcd: Modify indexes, reachability One gotcha: you need to add the prometheus plugin to your route config for per-request metrics. Without it you only get system-level metrics and all the HTTP dashboards stay empty.\nAlerts flow through Grafana → OpsGenie → Slack.\nProduction Results # After three months of operation:\nMetric Target Actual Cost reduction 20% vs. previous 29.8% reduction Latency (p95) 40ms 25ms Availability 99.99% 100% PII de-identification 100% 100% Cost beat the target because cutting Flink out of the pipeline eliminated stream processing costs entirely.\nEarly on we saw some 499 errors (client dropped connection) and 408 errors (server timeout). The SDK retries everything except 400s, so no data was lost.\nTakeaways # Sometimes an API gateway is the right proxy. If you need to modify request bodies and relay HTTP responses, a log forwarder won\u0026rsquo;t cut it. Reach for an API gateway. Build the custom plugin early. Don\u0026rsquo;t waste time trying to chain existing plugins into doing something they weren\u0026rsquo;t designed for. APISIX\u0026rsquo;s plugin structure makes custom development straightforward. Pre-scale beats reactive scaling. For bursty HTTP traffic with no backpressure, schedule-based scaling before peak hours is more reliable than waiting for KEDA to react. Check your nginx worker count. APISIX\u0026rsquo;s auto setting looks at node cores, not pod cores. In containers, set it manually or you\u0026rsquo;ll get CPU contention from too many workers. References:\nApache APISIX Documentation lua-resty-kafka (GitHub) APISIX Custom Plugin Development Apache APISIX Grafana Dashboard ","date":"24 February 2026","externalUrl":null,"permalink":"/en/posts/apisix-proxy-server-guide/","section":"Posts","summary":"We needed to strip PII from app logs before sending them to an overseas tracking server. This post covers our journey from Fluent-bit to Kong to APISIX, custom Lua plugin development, 55K TPS load testing, and Kubernetes deployment.","title":"Building a Proxy Server with Apache APISIX: De-identifying 1.3 Billion Daily Logs","type":"posts"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/cdc/","section":"Tags","summary":"","title":"Cdc","type":"tags"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/compaction/","section":"Tags","summary":"","title":"Compaction","type":"tags"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/deletion-vector/","section":"Tags","summary":"","title":"Deletion-Vector","type":"tags"},{"content":"Run compaction on an Iceberg CDC table and you\u0026rsquo;ll eventually see this:\norg.apache.iceberg.exceptions.ValidationException: Cannot commit, found new position delete for replaced data file Append-only tables don\u0026rsquo;t have this problem. CDC tables do. Understanding why requires digging into how Iceberg v2 handles deletes.\nIceberg v2 Row-Level Deletes # Two Ways to Delete Data: COW vs MOR # Iceberg offers two strategies for row-level changes.\nCopy-on-Write (COW): Rewrite the entire data file with the deleted rows removed. Great read performance, expensive writes. Good for batch updates on read-heavy tables.\nMerge-on-Read (MOR): Leave data files untouched. Record deletions in separate delete files. Fast writes, but reads pay the cost of merging originals with deletes at query time. This is what CDC/upsert tables use.\nCDC pipelines have constant updates flowing in, so MOR is the natural fit. The catch: the delete files MOR produces are exactly what breaks compaction.\nTwo Kinds of Delete Files # MOR uses two types of delete files.\nEquality delete: Records the key values of rows to delete. \u0026ldquo;Delete any row with this PK.\u0026rdquo; Can span multiple data files with a single delete file. Downside: readers must do key matching, which costs more at query time.\nPosition delete: Records a specific file path and row number. \u0026ldquo;Delete row 42 in A.parquet.\u0026rdquo; Readers just skip that exact position — faster than key matching.\nType What It Records Read Cost Best For Equality delete Key values Higher (key matching) PK-based bulk deletes Position delete File path + row number Lower (position skip) Frequent in-stream updates Engine Support Varies # The Iceberg spec defines both types, but not every engine implements everything.\nEngine Read Eq Delete Read Pos Delete Write Eq Delete Write Pos Delete Spark Yes Yes No Yes Flink Yes Yes Yes (UPSERT mode) Situational Trino Yes Yes No Yes Athena Yes Yes No Yes Hive/Impala Yes Yes No Yes Spark can read equality deletes but can\u0026rsquo;t write them. Same for Trino and Athena — they only produce position deletes. Flink is the exception with equality delete writes in UPSERT mode.\nThis matters for compaction strategy.\nThe Delta Writer: Where Delete Strategy Splits # The CDC sink uses iceberg-core\u0026rsquo;s BaseTaskWriter to process records. The delete logic is interesting:\npublic void deleteKey(T key) throws IOException { if (!internalPosDelete(asStructLikeKey(key))) { eqDeleteWriter.write(key); } } When building a single commit, the writer checks whether the record being deleted exists in the current stream (commit):\nRecord is in this stream → position delete Record is not in this stream → equality delete Why not use equality delete for everything? Simpler logic, right? The reason is read performance. Position deletes are cheaper to apply during MOR reads. The write path optimizes for future read performance by preferring position deletes whenever possible.\nThis design choice is what causes compaction headaches.\nSnapshot Types and Commit Conflicts # Four Snapshot Operations # Every Iceberg snapshot has an operation field describing what kind of change happened.\nOperation Meaning Typical Use append Add data files Batch loads, INSERT replace Swap data/delete files Compaction overwrite Logical overwrite Upsert, UPDATE, DELETE delete Remove data files or add delete files DROP PARTITION, etc. Compaction is replace. It merges small files into larger ones and swaps them in. CDC sink is overwrite. It produces delete files as part of the upsert process.\nOptimistic Concurrency # Iceberg handles concurrent writes with optimistic concurrency. Think of it like Git.\nBuild a new metadata tree based on the current snapshot Attempt an atomic commit (swap) If another commit landed in between, run validation Validation passes → commit succeeds. Fails → retry The validation rules in step 3 determine which snapshot type combinations conflict and which auto-merge.\nAppend-Only vs CDC: Why the Difference # Append-Only Tables: Almost No Conflicts # Append-only tables just add data. No delete files. When compaction (replace) runs alongside new inserts (append), they touch different files. Auto-merge works fine. Like merging two Git branches that edited different files — no conflicts.\nCDC Tables: Position Deletes Are the Problem # CDC tables are different. Upserts produce delete files. Here\u0026rsquo;s what goes wrong:\nTimeline: 1. Compaction starts: reads file A, building replacement file B 2. CDC sink: deletes row 42 in file A (creates position delete) 3. Compaction finishes: tries to commit A → B replacement 4. Validation fails: \u0026#34;file A has a new position delete, replacing it would lose that delete\u0026#34; ValidationException: Cannot commit, found new position delete for replaced data file Compaction already read file A and built a replacement. But a new position delete targeting file A appeared in between. Committing the replacement without that delete would break data correctness. Iceberg rejects the commit.\nWhy Equality Deletes Cause Fewer Conflicts # Equality deletes say \u0026ldquo;delete rows with this key\u0026rdquo; — a logical declaration not bound to any specific file. When compaction replaces files, key-based deletes still apply to the new files.\nIceberg has a mechanism for this: compaction preserves the starting sequence-number (PR #3480), which lets equality deletes pass validation automatically. This is enabled by default.\nPosition deletes point to a specific file at a specific row. Replace that file and the position becomes meaningless. No automatic workaround possible.\nThe Commit Interval Dilemma # \u0026ldquo;Can\u0026rsquo;t we just tune the sink commit interval?\u0026rdquo; You can try. It doesn\u0026rsquo;t solve the problem.\nLonger intervals: More records per commit. Higher chance that the same record gets inserted then updated or deleted within one stream. The delta writer handles these as position deletes. More position deletes = more conflict potential.\nShorter intervals: Fewer position deletes per commit, but commits happen more often. If compaction can\u0026rsquo;t finish before the next commit lands — and it only takes one position delete — conflict.\nNeither direction gets you to zero conflicts.\nv2 Fix Attempts — All Failed # Several PRs tried to solve this within the v2 framework. None were merged.\nPR Approach Outcome #4703 Skip position delete validation during compaction Reviewers deemed it \u0026ldquo;dangerous,\u0026rdquo; closed #4748 Exploit same sequence number between Flink upsert pos-deletes and their data files Closed #5760 Add min-data-sequence-number to manifest entries for safer filtering Closed by stale bot #7249 position-deletes-within-commit-only snapshot property Closed by stale bot No clean solution emerged within v2. The community converged on fixing this structurally in v3.\nIceberg v3: How Deletion Vectors Change the Picture # The v3 spec was ratified in early 2025. Implementation started shipping with Iceberg 1.8.0 (February 2025). The key change: Deletion Vectors (DVs).\nPosition Delete Files Are Gone # v3 bans new position delete file creation outright. DVs take their place.\nPosition delete files must not be added to v3 tables, but existing position delete files are valid.\nA DV is a Roaring bitmap stored in a Puffin file. Each bit marks whether a row in a specific data file has been deleted.\nv2 Position Delete v3 Deletion Vector Format Parquet (file path + row number columns) Puffin (Roaring bitmap) Per data file Unbounded (N files can accumulate) At most 1 On new delete Append a separate delete file Read existing DV, merge, replace Why This Reduces Compaction Conflicts # The v2 conflict happens because position delete files exist independently of the data files they reference. Compaction replaces a data file, and any position delete file pointing at the old file becomes a dangling reference.\nDVs work differently.\nTightly coupled to data files. A DV is a sidecar to its data file. When compaction rewrites a data file, the DV\u0026rsquo;s deletions get absorbed into the new file. The old DV is removed alongside the old data file. No dangling references.\nNo independent accumulation. One data file gets at most one DV. New deletions merge into the existing DV rather than creating a separate file. The \u0026ldquo;new position delete for replaced data file\u0026rdquo; scenario from v2 doesn\u0026rsquo;t arise the same way.\nLess compaction needed. DVs are compact bitmaps, not Parquet files. They don\u0026rsquo;t create the small-file problem that v2 position delete files did. Fewer compaction runs means a smaller window for conflicts.\nRow Lineage and Row-Level Conflict Detection # v3 also introduces Row Lineage. Every row gets a unique _row_id and _last_updated_sequence_number. Combined with DVs, this enables row-level conflict detection (Issue #14613).\nIn v2, touching the same data file meant a conflict. Period. In v3, two writers modifying different rows in the same file can auto-merge. If the CDC sink deletes row 42 while compaction reorganizes other rows, the commit can succeed without conflict.\nNot a Silver Bullet # OCC still applies. Two writers updating the same data file\u0026rsquo;s DV at the same time will still conflict — one has to retry. But the retry is cheap: just re-merge a bitmap. No data re-scan needed. And once engines implement row-level concurrency using Row Lineage, even same-file conflicts can resolve automatically when different rows are affected.\nEngine Support (as of 2025) # Engine v3 DV Support Spark (Iceberg 1.8.0+) Supported AWS EMR / Athena / Glue Announced November 2025 Databricks Supported (with row-level concurrency) Trino (Starburst) In progress Flink In progress Migrating is straightforward: ALTER TABLE ... SET TBLPROPERTIES ('format-version' = '3'). Existing position delete files stay valid. New deletes start using DVs.\nThe v2 Workaround: Pause CDC During Compaction # If you\u0026rsquo;re still on v2, the most reliable approach in production is straightforward: stop the CDC sink while compaction runs.\nCompaction pipeline: 1. Pause CDC sink connector during low-traffic hours (e.g., early morning) 2. Run compaction (rewrite_data_files) 3. Resume CDC sink after compaction completes Kakao\u0026rsquo;s tech blog describes a similar approach — pausing real-time CDC ingestion at roughly 12-hour intervals for table maintenance.\nThings to Watch # Compaction can take longer than expected. A full day\u0026rsquo;s worth of CDC data might take hours to compact. Measure your window empirically. Consider partition-level compaction. Don\u0026rsquo;t compact the entire table at once. Splitting by partition cuts wall time. Clean up delete files separately. Run rewrite_position_delete_files periodically for minor compaction. Watch for version-specific bugs — they\u0026rsquo;ve been reported. Operational Checklist # Check your runtime version: Verify rewrite_data_files validation rules and rewrite_position_delete_files support in your EMR/Spark/Flink/Trino version Monitor conflict rates: Measure CDC commit frequency vs compaction duration. Track how often conflicts actually occur Build a metadata dashboard: Visualize file counts, average sizes, and delete file accumulation from snapshot/manifest tables Secure a compaction window: Set up nightly CDC pause or partition-level compaction schedules Evaluate v3 migration: If your engine supports v3 DVs, migration eliminates most of the operational overhead described above Wrapping Up # Here\u0026rsquo;s the short version:\nIceberg v2\u0026rsquo;s MOR path uses position deletes to optimize read performance Position deletes are bound to specific files, so they conflict with compaction (replace) Equality deletes get auto-resolved via the sequence-number mechanism. Position deletes don\u0026rsquo;t Every attempt to fix this within v2 was closed without merging v3 Deletion Vectors are the structural fix. One bitmap per data file instead of unbounded position delete files. Row Lineage enables row-level conflict detection on top of that On v2, the pragmatic answer is pausing CDC during a compaction window The v3 spec is ratified and engine support is expanding fast. If you\u0026rsquo;re still on v2, planning the migration to v3 is the long-term play. Most of the operational overhead described in this post goes away once DVs are in place.\nReferences:\nRow-Level Changes on the Lakehouse: COW vs MOR in Apache Iceberg (Dremio) Apache Iceberg Spec - Row-level Deletes Apache Iceberg - Reliability (Concurrency) PR #3480: Core: support rewrite data files with starting sequence number PR #4703: API: Optionally ignore position deletes in rewrite validation PR #7249: Avoid conflicts between rewrite datafiles and flink CDC writes Iceberg Table Loading and Operation Strategy by Log Type (Kakao Tech) Improve Position Deletes in V3 (Issue #11122) Row Lineage for V3 (Issue #11129) Row-level concurrency (Issue #14613) What\u0026rsquo;s New in Apache Iceberg v3 (Google Open Source Blog) Iceberg V3 Deletion Vectors on Amazon EMR (AWS Blog) ","date":"24 February 2026","externalUrl":null,"permalink":"/en/posts/iceberg-cdc-compaction-challenge/","section":"Posts","summary":"Iceberg v2’s row-level delete implementation, the difference between position and equality deletes, and why compaction commits keep failing when a real-time CDC sink is running. Covers how v3 Deletion Vectors change the picture and the workaround we settled on for v2 in production.","title":"Iceberg - Why CDC Table Compaction Is So Tricky","type":"posts"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/iceberg-v3/","section":"Tags","summary":"","title":"Iceberg-V3","type":"tags"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/kafka-connect/","section":"Tags","summary":"","title":"Kafka-Connect","type":"tags"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/lakehouse/","section":"Tags","summary":"","title":"Lakehouse","type":"tags"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/lua/","section":"Tags","summary":"","title":"Lua","type":"tags"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/privacy/","section":"Tags","summary":"","title":"Privacy","type":"tags"},{"content":"","date":"24 February 2026","externalUrl":null,"permalink":"/en/tags/proxy/","section":"Tags","summary":"","title":"Proxy","type":"tags"},{"content":"Joe Reis published his 2026 data engineering trends based on a survey of 1,101 data practitioners. As someone leading a data engineering team at a large-scale platform, I wanted to contrast these trends against our team\u0026rsquo;s current architecture — taking an honest look at what we\u0026rsquo;re already doing well and what we still need to tackle.\nOur Architecture in a Nutshell # We run a hybrid architecture centered on S3 as the data lake, with Apache Iceberg as the table format, Trino for batch/ad-hoc analytics, and StarRocks for real-time OLAP. Data ingestion relies on Kafka + Debezium CDC and Flink streaming, while orchestration runs on a heavily customized Airflow setup.\n[Services] → Kafka + Debezium CDC → Flink → S3 (Iceberg) ↓ ┌─────┴─────┐ │ │ Trino StarRocks (Batch/Ad-hoc) (Real-time OLAP) │ │ └─────┬─────┘ ↓ Dashboard 1. AI Adoption — What We\u0026rsquo;re Already Doing and the Walls We Need to Climb # Trend Summary # 82% of survey respondents use AI daily, yet 64% are still stuck at the experimentation stage or using it only for simple tasks. Joe Reis predicts that by the end of 2026, the qualifier \u0026ldquo;AI-assisted\u0026rdquo; will disappear from job descriptions entirely.\nWhat We\u0026rsquo;re Doing # AI coding tools are already part of our daily pipeline development workflow. We actively use LLMs for SQL optimization, code review, and troubleshooting, and we\u0026rsquo;re experimenting with natural-language-based data exploration linked to our data catalog.\nWhat We Need to Do # The challenge is moving beyond individual AI usage to embedding AI into the team\u0026rsquo;s entire workflow.\nData pipeline anomaly detection Automated schema evolution handling Auto-generation of data quality rules To be counted among what Joe Reis calls the \u0026ldquo;10% of AI-mature teams,\u0026rdquo; we need a strategy that integrates AI not as a simple helper tool but as a core component of the platform.\n2. The Data Modeling Crisis and the Semantic Layer — Our Biggest Challenge # Trend Summary # 89% of respondents reported struggling with data modeling, and only 5% of teams are using semantic models. Joe Reis expects the semantic layer to become mainstream first, followed by an evolution toward LLMs interpreting schemas on the fly.\nWhat We\u0026rsquo;re Doing # We manage lineage and metadata through a data catalog and have defined a table layer hierarchy (L1/L2/L3) to manage data quality at multiple levels. We also run automated data validation through custom Airflow operators.\nWhat We Need to Do # We evaluated adopting dbt, but that effort stalled because it got caught up in our next-generation data platform migration strategy. Rather than migrating current pipelines to dbt, we\u0026rsquo;re considering moving them directly to the new platform. In the meantime, however, the standardization and modularization of data transformations remains a gap. This maps exactly to the \u0026ldquo;89% pain\u0026rdquo; that Joe Reis described.\nThe semantic layer is also uncharted territory for us. Business metric definitions vary across teams, and different teams write different SQL for the same KPI. Just as the survey showed 19% demand for semantic model training, the work of raising data literacy across the entire organization is urgent.\nEspecially if we\u0026rsquo;re preparing for a future where AI agents autonomously leverage data, a well-defined semantic layer isn\u0026rsquo;t optional — it\u0026rsquo;s essential. Even if the platform migration is delayed, modeling standards and semantic definitions can — and should — proceed independently.\n3. Orchestration Consolidation — The Future of Airflow # Trend Summary # Airflow remains dominant, but Dagster has grown from the bottom up, capturing 12% share among small companies. It\u0026rsquo;s also surprising that 20% of teams across all company sizes have no orchestration at all.\nWhat We\u0026rsquo;re Doing # We run a deeply customized Airflow setup. We\u0026rsquo;ve developed our own Provider packages and built platform-specific capabilities including automated data validation operators and custom transfer operators. We\u0026rsquo;re currently working on the Airflow 3.x major version upgrade, with a systematic plan covering the Python version upgrade and breaking change migration.\nWhat We Need to Do # Our deep investment in Airflow is a strength, but it\u0026rsquo;s also technical debt.\nMaintenance burden of custom Providers Compatibility issues during version upgrades Preparing for the new paradigm of AI agent orchestration As Joe Reis predicted, we should also keep an eye on the trend of orchestration being absorbed into platforms. Considering alignment with our next-generation data platform, establishing a mid-to-long-term orchestration strategy roadmap is urgent.\n4. Lakehouse vs. Warehouse — An Area Where We\u0026rsquo;ve Already Found Our Answer # Trend Summary # In the survey, 44% use a Warehouse, 27% use a Lakehouse, and 12% use a Hybrid approach. As Snowflake and Databricks converge in functionality, the debate itself is becoming moot. Joe Reis predicts that by the end of 2026, the \u0026ldquo;warehouse vs. lakehouse\u0026rdquo; debate will feel outdated.\nWhat We\u0026rsquo;re Doing # On this trend, our team is already close to the right answer. We adopted Iceberg as the standard open table format on S3 and selectively use Trino and StarRocks depending on the use case. This architecture is neither a Warehouse nor a Lakehouse — it takes the best of both worlds. Through CDC pipelines, we ingest real-time data into Iceberg tables and run both batch and real-time analytics on the same underlying data.\nWhat We Need to Do # To take full advantage of new Iceberg v3 features like Deletion Vectors and Row Lineage, we need to ensure compatibility across our query engines. Since Trino and StarRocks currently have limited Iceberg v3 support, our engine upgrade roadmap and Iceberg version strategy need to be aligned. We also need to strengthen the governance framework for our open table format architecture — catalog integration, access control, and data quality assurance.\n5. Leadership as a Bottleneck — The Hardest and Most Important Challenge # Trend Summary # 22% of data engineers cited \u0026ldquo;lack of leadership direction\u0026rdquo; as a major issue — nearly on par with legacy technical debt at 26%. Joe Reis warns that in 2026, more data teams will be dissolved or merged into engineering organizations.\nWhat We\u0026rsquo;re Doing # Our data platform team exists as an independent organization, owning end-to-end responsibility from infrastructure to ingestion, transformation, and analytics environments. We maintain direct communication channels with business teams and gather data requirements firsthand.\nWhat We Need to Do # Technical capability alone can\u0026rsquo;t prove a team\u0026rsquo;s value. As Joe Reis emphasized, \u0026ldquo;Only teams that prove business value will survive.\u0026rdquo;\nA framework for quantitatively measuring and communicating the ROI of the data platform A vision for what role the data platform should play in the AI era Reducing data downtime through data observability Presenting concrete business impact such as pipeline development productivity metrics Summary: What We\u0026rsquo;re Doing Well vs. What We Need to Do # Area Doing Well Need to Do AI Adoption Active use of AI coding tools at the individual level Embed AI into team workflows, operational automation Data Modeling Catalog-based metadata management, layer hierarchy defined Adopt semantic layer, standardize data transformations Orchestration Deep Airflow customization, 3.x upgrade in progress Long-term orchestration strategy, AI agent readiness Lakehouse/Warehouse Iceberg-based hybrid architecture in place Iceberg v3 compatibility, strengthen governance Leadership End-to-end platform team operation Quantify business impact, data observability Final Thoughts # The most striking sentence from Joe Reis\u0026rsquo;s survey was this:\n\u0026ldquo;In 2026, data engineering is less about picking the right tools and more about building the organizational muscle to use them well.\u0026rdquo;\nOur team is ahead of the curve when it comes to the tech stack. An Iceberg-based open data lake, a hybrid architecture spanning real-time and batch, deep Airflow customization — these are levels that many organizations haven\u0026rsquo;t reached yet.\nBut technical advantage alone isn\u0026rsquo;t enough. Standardized data transformations, a semantic layer, data observability, AI-native workflows, and above all, leadership that proves business value — this is where we need to focus in 2026.\nThe debts of the past are accruing interest, and payday is approaching.\nOriginal source: Where Data Engineering Is Heading in 2026 — Joe Reis\n","date":"23 February 2026","externalUrl":null,"permalink":"/en/posts/2026-data-engineering-trends/","section":"Posts","summary":"Analyzing 2026 data engineering trends from Joe Reis’s 1,101-respondent survey, contrasted with our team’s current architecture at a large-scale platform. An honest look at what we’re doing well and what we need to work on.","title":"2026 Data Engineering Trends and Where Large-Scale Platforms Stand","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/ai/","section":"Tags","summary":"","title":"Ai","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/airflow/","section":"Tags","summary":"","title":"Airflow","type":"tags"},{"content":"The End of Life for Airflow 2.x is approaching on April 22, 2026. Our team carried out an Airflow 3.x migration in a production environment running hundreds of DAGs. This post is a record of the breaking changes we encountered, the phased upgrade strategy we used, and the practical lessons we learned from migrating at scale.\nWhy Migrate Now? # Key Improvements in Airflow 3.x # Airflow 3.x is not just a major version bump. Fundamental changes have occurred at the architectural level.\nDAG versioning: No more hacking version suffixes onto dag_id or dealing with scheduling confusion when the schedule changes. Native backfill: Backfills that used to depend on the CLI or custom plugins are now supported directly from the web UI. Event/asset-based triggers: Scheduling options now go far beyond simple cron expressions. React-based web UI: The UI has been completely rebuilt from Flask App Builder to React, dramatically improving usability. Architectural Change: The Arrival of the API Server # The most significant architectural change in 3.x is that the API Server has become the sole gateway to the metadata database.\nAirflow 2.x: Webserver ─── MetaDB Worker ────── MetaDB Scheduler ─── MetaDB DAG Code ──── MetaDB (direct access possible) Airflow 3.x: API Server ── MetaDB (sole access path) Webserver ─── API Server Worker ────── API Server Scheduler ─── API Server DAG Code ──── API Server (no direct access) As a result, any pattern where DAG top-level code directly accessed the metadata database will break. This is the single most impactful change in the entire migration.\nPhased Upgrade Strategy # Jumping straight to the latest version is risky. We devised a four-phase strategy.\nPhase 1: Update to the Latest 2.x Version (2.11) (Optional) # This serves as a safety net in case issues arise during the jump to 3.x. Version 2.11 displays deprecation warnings for features that will be removed in 3.x, so you can identify which code needs to be modified ahead of time.\nPhase 2: Update to 3.0.x # If you\u0026rsquo;re on Python 3.9, only 3.0.x is supported, not the latest 3.1.x. Upgrade the Airflow major version first, before upgrading Python.\nPhase 3: Upgrade Python (3.9 → 3.12+) # Airflow 3.1.x does not support Python 3.9. Aim for Python 3.12 or later, but compromise with 3.10 or 3.11 if dependency compatibility issues arise.\nPhase 4: Update to 3.1.x # Finally, upgrade to the latest stable release.\nSequential Rollout Across Environments # DEV → BETA \u0026amp; Personal Environments → STAGE → PROD Proceed to the next environment only after thorough validation in each one. We spent about two weeks validating in DEV and one week in BETA.\nKey Breaking Changes and How to Handle Them # 1. schedule_interval → schedule # This is the most commonly encountered change. Simply pass the same cron expression you used for schedule_interval to schedule instead.\n# Before (Airflow 2.x) DAG( dag_id=\u0026#34;my_dag\u0026#34;, schedule_interval=\u0026#34;5 2 * * *\u0026#34;, ) # After (Airflow 3.x) DAG( dag_id=\u0026#34;my_dag\u0026#34;, schedule=\u0026#34;5 2 * * *\u0026#34;, ) It\u0026rsquo;s a straightforward substitution, but when you have hundreds of DAGs, every single one must be updated without exception. We\u0026rsquo;ll cover how to automate verification in CI later.\n2. Passing Non-Existent Operator Arguments Is No Longer Allowed # In Airflow 3.x, individual tasks now receive serialized DAGs from the metadata database for execution. As a result, the allow_illegal_arguments setting has been removed, and passing an argument not defined on the operator will cause the DAG import itself to fail.\n# Code like this worked silently in 2.x, but raises an error in 3.x MyOperator( task_id=\u0026#34;my_task\u0026#34;, num_partition=10, # Actual argument name is num_partitions (plural) ) TypeError: Invalid arguments were passed to MyOperator (task_id: my_task). Invalid arguments were: **kwargs: {\u0026#39;num_partition\u0026#39;: 10} This change actually serves as an opportunity to catch latent bugs. If a misspelled argument had been silently ignored for a long time, this migration is the moment to fix it.\n3. Deprecated Context/Template Variables Removed # Variables that only showed deprecation warnings in 2.x have been completely removed in 3.x. The most impactful one is execution_date.\nDeprecated Variable Replacement {{ execution_date }} {{ logical_date }} or {{ data_interval_start }} {{ next_execution_date }} {{ data_interval_end }} {{ prev_execution_date_success }} {{ prev_data_interval_start_success }} Both Jinja templates and Python code need to be updated.\n# Jinja templates # Before \u0026#34;SELECT * FROM table WHERE dt = \u0026#39;{{ execution_date }}\u0026#39;\u0026#34; # After \u0026#34;SELECT * FROM table WHERE dt = \u0026#39;{{ logical_date }}\u0026#39;\u0026#34; # Python context # Before execution_date = context[\u0026#34;execution_date\u0026#34;] # After logical_date = context[\u0026#34;logical_date\u0026#34;] 4. Per-Database Operators Unified → SQLExecuteQueryOperator # Individual operators for MySQL, PostgreSQL, Trino, etc. have been consolidated into a single SQLExecuteQueryOperator. Internally, it automatically selects the appropriate Hook based on the connection type.\n# Before (Airflow 2.x) from airflow.providers.mysql.operators.mysql import MySqlOperator MySqlOperator( task_id=\u0026#34;task\u0026#34;, mysql_conn_id=\u0026#34;my_conn\u0026#34;, sql=\u0026#34;SELECT 1\u0026#34; ) # After (Airflow 3.x) from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator SQLExecuteQueryOperator( task_id=\u0026#34;task\u0026#34;, conn_id=\u0026#34;my_conn\u0026#34;, # DB-specific conn_id → unified conn_id sql=\u0026#34;SELECT 1\u0026#34; ) 5. DummyOperator → EmptyOperator # You need to use an import path that works in both 2.x and 3.x.\n# Works only in v2 (errors in 3.x) from airflow.operators.dummy import DummyOperator # Works only in v3 from airflow.providers.standard.operators.empty import EmptyOperator # Compatible with both v2 \u0026amp; v3 (recommended) from airflow.operators.empty import EmptyOperator 6. SimpleHttpOperator → HttpOperator # # Before from airflow.providers.http.operators.http import SimpleHttpOperator # After from airflow.providers.http.operators.http import HttpOperator 7. Connection Getter Methods → Direct Property Access # The Connection class interface has been changed to be more Pythonic.\n# Before conn = BaseHook.get_connection(\u0026#34;my_conn\u0026#34;) password = conn.get_password() host = conn.get_host() # After conn = BaseHook.get_connection(\u0026#34;my_conn\u0026#34;) password = conn.password host = conn.host 8. Other Package Path Changes # # cached_property # Before: from airflow.compat.functools import cached_property # After: from functools import cached_property (Python built-in) # KubernetesPodOperator # Before: from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import ... # After: from airflow.providers.cncf.kubernetes.operators.pod import ... 9. Remove provide_context=True # Since Airflow 2.0, PythonOperator injects context automatically. provide_context=True does nothing, but a surprising number of DAGs still have it.\n# Before PythonOperator( task_id=\u0026#34;my_task\u0026#34;, python_callable=my_func, provide_context=True, ) # After PythonOperator( task_id=\u0026#34;my_task\u0026#34;, python_callable=my_func, ) 10. from airflow.models import DAG → from airflow import DAG # airflow.models.DAG is deprecated in v3. from airflow import DAG works in both v2 and v3.\n# Before from airflow.models import DAG # After from airflow import DAG Note: the pattern from airflow import models followed by models.DAG() does not need to change.\n11. Remove @apply_defaults Decorator # Since Airflow 2.0, BaseOperator handles this automatically. Remove it from custom operators.\n# Before from airflow.utils.decorators import apply_defaults class CustomOperator(BaseOperator): @apply_defaults def __init__(self, **kwargs): super().__init__(**kwargs) # After class CustomOperator(BaseOperator): def __init__(self, **kwargs): super().__init__(**kwargs) 12. concurrency → max_active_tasks # The DAG parameter concurrency is removed in v3. Same meaning, clearer name.\n# Before with DAG(dag_id=\u0026#34;my_dag\u0026#34;, concurrency=5) as dag: # After with DAG(dag_id=\u0026#34;my_dag\u0026#34;, max_active_tasks=5) as dag: 13. Remove Direct TaskInstance Construction # kwargs[\u0026quot;ti\u0026quot;] is already a TaskInstance object. There\u0026rsquo;s no reason to create another one. ti.execution_date access and the TaskInstance(task, execution_date) constructor are deprecated in v3.\n# Before def my_callable(**kwargs): ti = TaskInstance(kwargs[\u0026#34;ti\u0026#34;].task, kwargs[\u0026#34;ti\u0026#34;].execution_date) ti.xcom_push(key=\u0026#34;my_key\u0026#34;, value=result) # After def my_callable(**kwargs): ti = kwargs[\u0026#34;ti\u0026#34;] ti.xcom_push(key=\u0026#34;my_key\u0026#34;, value=result) Migration Strategies for Large-Scale DAG Environments # Detect Incompatibilities with ruff # Scanning hundreds of DAGs by eye is not feasible. ruff has Airflow-specific rules that catch incompatible code automatically.\n# Check a specific file ruff check --preview --select AIR dags/my_dag.py # Check an entire team directory ruff check --preview --select AIR dags/my_team/ Rule What It Catches AIR301 Deprecated decorators (@apply_defaults, etc.) AIR302 Deprecated parameters, import paths, template variables Add v3 Compatibility Checks to Your CI Pipeline # We added a CI job that automatically checks v3 compatibility at the MR (Merge Request) stage.\n# .gitlab-ci.yml example airflow-v3-compat-check: stage: test image: apache/airflow:3.0.6-python3.12 script: - pip install -r requirements.txt - ruff check --preview --select AIR dags/ - python -m py_compile dags/**/*.py - airflow dags list --output table allow_failure: true # Start as warning-only, then switch to required Start with allow_failure: true to get a picture of the current state, then switch to mandatory checks as the migration deadline approaches.\nThe Rollout: Three Rounds of Bulk Fixes # We fixed incompatible code across the entire DAG repository in three rounds.\nRound Files Changed Key Fixes 1st 252 schedule_interval→schedule, DummyOperator→EmptyOperator 2nd 35 Missed items from round 1, provide_context removal, misspelled parameter names 3rd 21 DAG import paths, Jinja template variables, concurrency→max_active_tasks, direct TaskInstance construction We changed 252 files in the first round alone, and there were still gaps. Patterns that ruff can\u0026rsquo;t detect (misspelled parameters, unnecessary arguments on custom operators) had to be found manually. Don\u0026rsquo;t expect to finish in one pass.\nIntentionally Keep the Migration Hurdle High # This was the most important lesson we learned.\nWe could have applied a blanket compatibility patch to all DAG code. But we deliberately chose to maintain the migration difficulty. The reason was clear:\nA significant number of DAGs were being operated out of inertia but weren\u0026rsquo;t actually in use.\nBy treating the migration as an opportunity, we nudged DAG owners into asking themselves \u0026ldquo;Do we really need this DAG?\u0026rdquo; As a result, a substantial number of unnecessary DAGs were cleaned up, which directly reduced operational overhead.\nThe specific process:\nCompile a list of inactive DAGs and organize them in a shared spreadsheet Notify DAG owners/departments and ask them to confirm whether each DAG is still needed by a deadline Deactivate DAGs with no response by the deadline DAG owners perform v3 compatibility patches themselves Proactively Update Custom Provider Packages # If you maintain in-house custom operators or utilities as a Provider package, the key is to prepare a compatibility layer that absorbs Airflow core\u0026rsquo;s breaking changes ahead of time.\nWe incrementally updated our custom Provider package across four releases:\nv3.0.0: Basic compatibility v3.0.1: Operator argument validation support v3.0.2: Compatibility layer for deprecated context variables v3.0.3: Documentation and minor bug fixes The approach was to minimize changes to user-facing code while handling v2/v3 branching logic internally within the Provider package.\nHelm Chart Updates # If you run Airflow on Kubernetes, the Helm chart needs to be updated as well. This is because the DAG Processor component and the API Server separation introduced in 3.x must be reflected.\nA safe two-step approach is to first verify compatibility with the existing chart version, then update to the latest stable version once things have stabilized.\nFAB Auth Manager Issue # With the full web UI rebuild to React in 3.x, the existing Flask App Builder (FAB)-based Auth Manager was removed from the default package. If you\u0026rsquo;re using a custom Security Manager, you\u0026rsquo;ll need to install the package separately and update your code.\nFailed to import WoowaSecurityManager, using default security manager If you see this error, you need to explicitly install the FAB Auth Manager package and update the import paths.\nClosing Thoughts # Migrating to Airflow 3.x is not a simple version upgrade. It requires extensive work spanning architectural changes (API Server-centric design), code compatibility updates, and infrastructure modifications.\nHere are the key lessons summarized:\nUpgrade in phases \u0026ndash; Don\u0026rsquo;t jump straight to the latest version. Follow the path: 2.11 → 3.0.x → Python upgrade → 3.1.x. Automate verification in CI \u0026ndash; It\u0026rsquo;s impossible for humans to manually check the compatibility of hundreds of DAGs. Treat the migration as a cleanup opportunity \u0026ndash; Intentionally maintain the migration hurdle to naturally filter out unnecessary DAGs. Proactively update custom Providers \u0026ndash; A compatibility layer that minimizes changes to user code is the key. Don\u0026rsquo;t be complacent just because there\u0026rsquo;s still time until the Airflow 2.x EOL. Migrations in large-scale environments take longer than expected. It\u0026rsquo;s not too late to start now.\nReferences:\nUpgrading to Airflow 3 - Apache Airflow Documentation Apache Airflow 3 is Generally Available! Airflow 3.x Release Notes ","date":"23 February 2026","externalUrl":null,"permalink":"/en/posts/airflow-3-migration-guide/","section":"Posts","summary":"Practical lessons from migrating to Airflow 3.x ahead of the 2.x EOL. Covers major breaking changes, a phased upgrade strategy, DAG compatibility approaches, and hard-won lessons from operating hundreds of DAGs in production.","title":"Airflow 3.0 Migration Guide: Lessons from a Large-Scale DAG Environment","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/bali/","section":"Tags","summary":"","title":"Bali","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/compression/","section":"Tags","summary":"","title":"Compression","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/data-engineering/","section":"Tags","summary":"","title":"Data-Engineering","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/data-pipeline/","section":"Tags","summary":"","title":"Data-Pipeline","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/digital-nomad/","section":"Tags","summary":"","title":"Digital-Nomad","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/family/","section":"Tags","summary":"","title":"Family","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/gpu/","section":"Tags","summary":"","title":"Gpu","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/infrastructure/","section":"Tags","summary":"","title":"Infrastructure","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/categories/life/","section":"Categories","summary":"","title":"Life","type":"categories"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/mig/","section":"Tags","summary":"","title":"Mig","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/migration/","section":"Tags","summary":"","title":"Migration","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/nvidia/","section":"Tags","summary":"","title":"Nvidia","type":"tags"},{"content":"GPUs are expensive. A single A100 costs tens of thousands of dollars, yet most workloads don\u0026rsquo;t use all 80GB of memory. Allocating an entire GPU for a simple Jupyter notebook experiment means 70GB sits idle.\nNVIDIA MIG (Multi-Instance GPU) fixes this. It splits a physical GPU into up to 7 independent instances so multiple workloads can share one card without stepping on each other. This post covers how we set up MIG on 4× A100s and wired it into Kubernetes.\nWhat Is MIG # MIG splits one physical GPU into several isolated instances. Each gets its own memory and compute units. They\u0026rsquo;re fully independent — an OOM in one instance won\u0026rsquo;t touch the others.\nFor the A100 80GB, the partition options look like this:\nProfile Memory SMs Max Instances per GPU 1g.10gb ~10GB 14 7 2g.20gb ~20GB 28 3 3g.40gb ~40GB 42 2 7g.80gb ~80GB 98 1 With 7-way partitioning (1g.10gb), four GPUs give you 28 instances. Simple math: 4x more GPU slots than before.\nChecking GPU Support # MIG only works on certain GPUs. A100 and H100 support it. A40 and V100 don\u0026rsquo;t. We tried A40 first, hit the wall, and moved to A100.\nCheck the MIG M. column in the nvidia-smi output:\n# GPU without MIG support (A40) | MIG M. | | N/A | ← Not supported # GPU with MIG support (A100) | MIG M. | | Disabled | ← Supported but inactive | Enabled | ← Active The full list is in the NVIDIA docs.\nSetting Up MIG # Prerequisite: Disable nvidia-mig-manager # Before anything else, disable nvidia-mig-manager.service. This daemon turns MIG off on every boot, which will undo your work.\nsudo systemctl disable nvidia-mig-manager.service Install mig-parted # nvidia-mig-parted is NVIDIA\u0026rsquo;s tool for managing MIG configs. Write a YAML file describing what you want, run one command, done.\n# Install deb package (Ubuntu/Debian) curl -LO https://github.com/NVIDIA/mig-parted/releases/download/v0.5.2/nvidia-mig-manager_0.5.2-1_amd64.deb sudo dpkg -i nvidia-mig-manager_0.5.2-1_amd64.deb Enable MIG Mode # sudo nvidia-smi -mig 1 This puts MIG mode in pending. To actually apply it, you need a reboot.\nsudo reboot VM caveat: GPU passthrough VMs don\u0026rsquo;t support nvidia-smi --gpu-reset. No reset means no MIG without a full reboot. We ended up moving from VMs to bare metal just for this.\nWriting config.yaml # mig-parted uses YAML to declare partition configs. Put multiple presets in one file and switch between them whenever you want.\nversion: v1 mig-configs: # Disable MIG all-disabled: - devices: all mig-enabled: false # 7-way split on all GPUs (1g.10gb × 7) all-1g.10gb: - devices: all mig-enabled: true mig-devices: \u0026#34;1g.10gb\u0026#34;: 7 # 3-way split on all GPUs (2g.20gb × 3) all-2g.20gb: - devices: all mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 # 2-way split on all GPUs (3g.40gb × 2) all-3g.40gb: - devices: all mig-enabled: true mig-devices: \u0026#34;3g.40gb\u0026#34;: 2 # Full GPU as single instance (7g.80gb × 1) all-7g.80gb: - devices: all mig-enabled: true mig-devices: \u0026#34;7g.80gb\u0026#34;: 1 # Per-GPU custom configuration custom-config: - devices: [0] mig-enabled: true mig-devices: \u0026#34;3g.40gb\u0026#34;: 2 - devices: [1] mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 \u0026#34;1g.10gb\u0026#34;: 1 - devices: [2] mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 \u0026#34;1g.10gb\u0026#34;: 1 - devices: [3] mig-enabled: true mig-devices: \u0026#34;1g.10gb\u0026#34;: 7 The custom-config at the bottom is what matters most. It mixes different profiles per GPU. More on why later.\nApplying the Configuration # # Apply uniform 7-way split sudo nvidia-mig-parted apply -f ./config.yaml -c all-1g.10gb # Or apply custom config sudo nvidia-mig-parted apply -f ./config.yaml -c custom-config Add -d for debug logs. Once it\u0026rsquo;s done, check the result with nvidia-smi -L:\nGPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-xxxx) MIG 3g.40gb Device 0: (UUID: MIG-xxxx) MIG 3g.40gb Device 1: (UUID: MIG-xxxx) GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-xxxx) MIG 2g.20gb Device 0: (UUID: MIG-xxxx) MIG 2g.20gb Device 1: (UUID: MIG-xxxx) MIG 2g.20gb Device 2: (UUID: MIG-xxxx) MIG 1g.10gb Device 3: (UUID: MIG-xxxx) ... Persisting Across Reboots # MIG config resets on reboot. Fix this with a systemd service that reapplies it at boot.\n# /etc/systemd/system/nvidia-mig-config.service [Unit] Description=Apply NVIDIA MIG Configuration After=nvidia-persistenced.service [Service] Type=oneshot ExecStart=/usr/bin/nvidia-mig-parted apply -f /home/user/config.yaml -c custom-config RemainAfterExit=yes [Install] WantedBy=multi-user.target sudo systemctl daemon-reload sudo systemctl enable nvidia-mig-config.service Lessons Learned: Why Uniform Partitioning Falls Short # We started simple. Split all 4 GPUs into 1g.10gb × 7, get 28 instances, let everyone share. What could go wrong?\nLarge Models Don\u0026rsquo;t Fit # A lot, it turns out. The recommendation team\u0026rsquo;s models needed 30GB+ of GPU memory. Each MIG instance only had 10GB. CUDA OOM. Models wouldn\u0026rsquo;t even load.\nLLMs made it worse. Llama 2 7B failed on a MIG device even at 4-bit quantization. 13B? Not a chance.\nYour partition strategy has to match your workloads. There\u0026rsquo;s no way around it.\nPer-GPU Custom Partitioning # We looked at what each team actually ran and landed on this:\nGPU Profile Purpose GPU 0 3g.40gb × 2 Mid-size model training (~40GB memory) GPU 1 2g.20gb × 3 + 1g.10gb × 1 Small-to-mid experiments GPU 2 2g.20gb × 3 + 1g.10gb × 1 Small-to-mid experiments GPU 3 1g.10gb × 7 Jupyter notebooks, light batch jobs If you need full GPU power for large model training, leave some GPUs with MIG off. We did exactly that as LLM experiments picked up.\nWatch out: Mixing MIG slices has physical constraints. Memory is allocated left-to-right inside the GPU and can\u0026rsquo;t be shared vertically across slices. Check the NVIDIA docs for which combos actually work.\nKubernetes Integration # To use MIG instances in Kubernetes, you need to reconfigure the NVIDIA device plugin.\nMIG Strategies # The device plugin has three ways to expose MIG instances:\nStrategy Resource Name Description none nvidia.com/gpu MIG-unaware. Allocates whole physical GPUs single nvidia.com/gpu Auto-assigns MIG instances. Existing workloads work as-is mixed nvidia.com/mig-{profile} Exposes each MIG profile as a separate resource single is convenient. Existing workloads requesting nvidia.com/gpu: 1 get a MIG instance without any code changes. The downside: you can\u0026rsquo;t pick the size.\nmixed gives you control. Request nvidia.com/mig-3g.40gb: 1 and you get exactly that. If you\u0026rsquo;re running different partitions per GPU, this is what you want.\nUpdating the Device Plugin # # values.yaml migStrategy: mixed helm upgrade -i nvdp nvdp/nvidia-device-plugin \\ --namespace nvidia-device-plugin \\ --create-namespace \\ --version 0.12.3 \\ -f ./values.yaml Restart the nvidia-device-plugin pods after this. Then check kubectl describe node to see the new resources:\nAllocatable: nvidia.com/mig-1g.10gb: 9 nvidia.com/mig-2g.20gb: 6 nvidia.com/mig-3g.40gb: 2 Using MIG in JupyterHub # Switch the JupyterHub profile to request MIG resources instead of whole GPUs:\n# Before (whole GPU allocation) extra_resource_limits: nvidia.com/gpu: \u0026#34;1\u0026#34; # After (MIG instance allocation) extra_resource_limits: nvidia.com/mig-1g.10gb: \u0026#34;1\u0026#34; Heads up: with mixed strategy, nvidia.com/gpu: 1 grabs an entire physical GPU. Always specify the MIG profile name.\nAirflow KubernetesPodOperator # Same idea for GPU tasks in Airflow. Swap the resource name:\nresources = { \u0026#34;cpu\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;16Gi\u0026#34;, \u0026#34;nvidia.com/mig-2g.20gb\u0026#34;: \u0026#34;1\u0026#34;, } KubernetesPodOperator( ... container_resources=k8s.V1ResourceRequirements( requests=resources, limits=resources, ), ... ) Operational Tips # When to Reconfigure # Changing MIG config kills all processes on those GPUs. Pick a quiet time. Our batch jobs kicked off at 4 AM, so we did MIG changes after 9 PM.\nAll-or-Nothing Per Server # You can\u0026rsquo;t turn MIG on for some GPUs and leave it off for others on the same machine. All GPUs must be MIG-enabled or all disabled. Need a full GPU for large model training? Either use a different server or set 7g.80gb: 1 in your config to expose the whole GPU as one instance.\nCapacity Numbers Will Jump # Your Kubernetes GPU capacity looks very different after MIG. We went from 4 GPUs to 20+ allocatable instances. Update your monitoring dashboards and alert thresholds or you\u0026rsquo;ll get false alarms.\nConclusion # MIG works. It won\u0026rsquo;t solve everything, but it makes expensive hardware go further. Here\u0026rsquo;s what stuck with us:\nCheck GPU support first. A100 and H100 work. A40 and V100 don\u0026rsquo;t. Save yourself the debugging. Skip VMs. GPU passthrough can\u0026rsquo;t do GPU reset. Bare metal makes life easier. Start uniform, go custom. Begin with 7-way splits to get things running. Then tune per-GPU as you learn what your teams actually need. Big models need big GPUs. LLMs and large recommendation models won\u0026rsquo;t fit in MIG slices. Keep some full GPUs around. GPUs are expensive. Slice them well and you get a lot more out of what you already have.\nReferences:\nNVIDIA MIG User Guide nvidia-mig-parted (GitHub) NVIDIA Device Plugin for Kubernetes Getting the Most Out of the A100 GPU with MIG ","date":"23 February 2026","externalUrl":null,"permalink":"/en/posts/nvidia-mig-setup-guide/","section":"Posts","summary":"Four A100 GPUs can yield up to 28 independent GPU instances. This guide covers MIG concepts, mig-parted installation, diverse slice configurations, and Kubernetes integration based on hands-on experience.","title":"NVIDIA MIG Setup Guide: Splitting One A100 GPU Into Many","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/olap/","section":"Tags","summary":"","title":"Olap","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/open-source/","section":"Tags","summary":"","title":"Open-Source","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/orchestration/","section":"Tags","summary":"","title":"Orchestration","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/performance-tuning/","section":"Tags","summary":"","title":"Performance-Tuning","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/real-time/","section":"Tags","summary":"","title":"Real-Time","type":"tags"},{"content":"When you think of Bali, certain images come to mind. Surfing, tropical nature, digital nomads with laptops open in coworking spaces. It\u0026rsquo;s a place often called the holy land of workations.\nI was drawn to that image too. I thought it would give my child a new experience at an international kindergarten, and give my family a sense of escape from the daily routine. In early 2023, I used my company\u0026rsquo;s overseas remote work policy to live and work in Bali for about a month.\nTo cut to the chase: remote work in Bali was more underwhelming than expected for a family with a young child. This post is an honest record of that experience. If you\u0026rsquo;re considering remote work in Bali, especially with a young child, I hope this helps you make a more informed decision.\nBefore Departure: What You Need to Prepare # Approval and Tax Issues # Before booking flights for overseas remote work, you need to get company approval first. The approval process can lead to schedule changes.\nIn my case, I initially applied for a three-month stay, but during the tax review I was told that only stays of two months or less were allowed under tax regulations. This is because tax treatment varies depending on the length of overseas stay. If I had already booked my flights, I would have had to pay change fees. Always purchase your tickets after approval is confirmed.\nI used Korean Air mileage for the flights, and it turned out to be a great deal since the mileage deducted was relatively low compared to the cost.\nAccommodation: The Villa Trap # There are generally two options for accommodation in Bali. You can hop between hotels and resorts, or rent a villa on a monthly basis.\nI stayed at a hotel for about three days, then signed a monthly contract for a Bali-style villa with a swimming pool. It looks stunning in photos. A private pool beneath palm trees, a spacious living room, a tropical garden. The problem is that actually living there is a different story. I ended up canceling the villa and going back to a hotel.\nThe reality of villas:\nBugs everywhere. Ants and mosquitoes are a given, and all sorts of unidentifiable insects show up. Hotels and resorts fumigate, but villas generally don\u0026rsquo;t. Inconsistent bedding and cleanliness. The level of maintenance varies wildly from place to place. Water supply issues. I\u0026rsquo;ll get into this later, but this was the biggest problem. Transportation is inconvenient. There\u0026rsquo;s a reason villas are cheap. Most are located far from the center of town, and walking anywhere in Bali is practically impossible. If you\u0026rsquo;re staying a month or longer, a villa might seem like the rational choice, but especially if you have a child, a hotel or serviced residence is better. In terms of maintenance, hygiene, and convenience, it wins on every front.\nHygiene: Bali\u0026rsquo;s Biggest Risk # There are a few topics that dominate Indonesia travel communities. \u0026ldquo;Which hospital should I go to?\u0026rdquo;, \u0026ldquo;Can someone recommend medicine?\u0026rdquo;, \u0026ldquo;Looking for a showerhead filter.\u0026rdquo;\nThis is not an exaggeration.\nThe Water Problem # Many places in Bali draw from groundwater. Contamination is not uncommon, and even after scrubbing with soap, your skin can feel perpetually slippery. It\u0026rsquo;s a different kind of unpleasantness from the limescale-heavy water in Europe.\nSwimming pools are everywhere in Bali, but many of them don\u0026rsquo;t smell of chlorine. No chlorine smell means they\u0026rsquo;re not being disinfected. In a tropical climate where bacteria thrive.\nThe beaches aren\u0026rsquo;t safe either. Many people get sick after swallowing contaminated seawater that flows in from the land while surfing. You can\u0026rsquo;t even be sure whether the ice in cold drinks at cafes is made from purified water.\nMy Child\u0026rsquo;s Hospitalization # I did my best to prepare before departure. We got typhoid vaccinations, and the day before our flight, I had blood tests done at a general hospital to check inflammation and white blood cell levels. I also started giving my child probiotics two weeks in advance.\nThree days after arrival, my child developed a high fever.\nA child who had never gone above 38°C spiked to 39.5°C. We went to the emergency room three times. At the third ER visit, when I said I wanted to go home, they told me to sign a form stating they wouldn\u0026rsquo;t be held responsible. We ended up being admitted. Four days in the hospital.\nLooking back, this experience was truly harrowing.\nA fundamental distrust of Indonesia\u0026rsquo;s outdated medical facilities. Communication was also difficult. While my child was in the ER, it started pouring rain and the ceiling collapsed. There was a thunderous crash, the ceiling light fixtures fell, and we grabbed our child and ran out moments before the ceiling caved in. This happened at what was supposedly the best hospital in the area. About 500,000 KRW per ER visit including tests. Total medical expenses: over 3 million KRW. Travel insurance is absolutely essential. Make sure to carefully check the coverage details and choose a plan with a sufficient medical expense limit. And honestly, after this experience, the thought \u0026ldquo;What are we even doing here?\u0026rdquo; wouldn\u0026rsquo;t leave my mind.\nDaily Life with a Child: Expectations vs. Reality # There\u0026rsquo;s Nothing to Do # Think about what you do with your child on a typical day in Korea. Reading books, taking walks, going to playgrounds, visiting kids\u0026rsquo; cafes, playing with toys, weekend camping. In Bali, you can do almost none of these things.\nNo books. Unless you pack a suitcase full of Korean picture books, you have nothing to read to your child. Walking is impossible. Bali has almost no sidewalks. The saying \u0026ldquo;if you want to go more than three steps, you need to call a motorbike\u0026rdquo; is barely an exaggeration. Walking under the blazing sun through exhaust fumes, you live in constant fear of getting your feet run over by a motorbike. Taking a walk with a child is unthinkable. Almost no playgrounds. It\u0026rsquo;s not like Korea where every apartment complex has a playground. It doesn\u0026rsquo;t seem to be a legally required facility either. You occasionally spot one here and there, but the quality is dramatically lower than what you\u0026rsquo;d find in Korea. No indoor play environment. At home you have familiar toys, but here there are none. In the end, you find yourself letting them watch YouTube, something you never allowed back home. That\u0026rsquo;s the reality.\nThe Kindergarten I Had Hoped For # Because Bali has a large Australian expat community, there are well-established English-language kindergartens. I had high expectations about giving my child an immersive English-language experience.\nBut I overlooked my child\u0026rsquo;s personality. Our child is introverted and doesn\u0026rsquo;t take the initiative to approach others. In an environment where most of the other kids speak English, I had to watch my child play alone for two weeks. We eventually decided to stop sending them. The bigger the expectations, the bigger the disappointment.\nThe experience can vary entirely depending on your child\u0026rsquo;s temperament. If your child is sociable and adapts quickly, it could be a great experience. But not every child is like that. You need to honestly assess your child\u0026rsquo;s personality.\nMy Daily Life: It\u0026rsquo;s a Workation, Not a Vacation # Extra Time Doesn\u0026rsquo;t Magically Appear # Let me describe my routine back in Korea. I\u0026rsquo;m not a particularly early riser. I wake up, feed my child, send them to kindergarten, work, and after work, play with my child. Once the child falls asleep, I have maybe one hour, two at most. I watch YouTube, catch up on work, or exercise.\nBeing in Bali doesn\u0026rsquo;t magically create time that didn\u0026rsquo;t exist. A person who has one hour of free time a day doesn\u0026rsquo;t suddenly get three hours just by changing locations. If anything, my commute got longer.\nInternet: The VPN Wall # Working from home was effectively impossible. The internet was slow, power outages were frequent, and the connection dropped constantly.\nCoworking spaces are a different story. Good ones have UPS systems and their own generators, and they contract with multiple ISPs so that if one goes down, another line keeps things running. Most of them met the minimum internet speed my company required.\nThe problem is VPN. Company security policy required me to be on VPN while working, and the moment I turned it on, internet speed dropped to about 10-20% of normal. Across all of Bali, only a handful of coworking spaces let you work comfortably with VPN on. Those places charge around 20,000 KRW a day, and they\u0026rsquo;re not close to where you\u0026rsquo;re staying either.\nThe Reality of Evenings # After work, nighttime. Surfing or outdoor activities are out of the question. It\u0026rsquo;s not like Europe where street musicians perform. Realistically, what you can do is go to a decent restaurant or bar, eat something good, and have a drink.\nBut even that isn\u0026rsquo;t as easy as it sounds.\nThe drinking culture is underdeveloped. Perhaps because of the Islamic background, craft beer is limited to two or three local breweries, and there\u0026rsquo;s no imported craft beer at all. Spirits are expensive, and wines are imported with limited selection and high prices. The decent places are as expensive as Korea. Drinks are charged separately, and taxes add 15-20%. Eating at decent places every day puts serious pressure on your bank account. If you stick to the cheap places, you start wondering why you came all the way to another country just to suffer like this. Two weeks after arriving, I watched my bank balance plummet.\nThe Things That Were Good # It feels like I\u0026rsquo;ve only been negative, so let me honestly share what was good too.\nCoworking Spaces # The coworking space culture in Bali is genuinely impressive. Part internet cafe, part library, part coffee shop. Just exploring them was a fun experience in itself, checking out the vibe of this one, discovering what that one had on the menu. Working alongside digital nomads from around the world in the same space wasn\u0026rsquo;t bad at all.\nBeach Clubs # Pool behind you, ocean in front. Music and delicious food. Bali\u0026rsquo;s beach clubs are truly a unique experience. Just watching young people enjoy themselves was energizing.\nWeekends # On weekends, I finally felt like I had actually come to Bali for a holiday. We went on tours, visited beach clubs, went surfing. The charm of Bali that was invisible during the week hit all at once on weekends. Paradoxically, this was the precise meaning of the word \u0026ldquo;workation.\u0026rdquo; Work and vacation don\u0026rsquo;t come simultaneously; they alternate.\nSummary: Who Is Bali Remote Work Right For? # Summing up a month of experience, satisfaction with remote work in Bali splits sharply depending on your lifestyle.\nCondition Satisfaction Reason Single or couple (no kids) High Activities, free time, flexible schedule Family with children Low Hygiene risks, nothing to do, no time Work that doesn\u0026rsquo;t require VPN High Freedom to use any coworking space Work that requires VPN Moderate Limited coworking space options Generous budget High Good accommodation + good food = good experience Tight budget Low Exhaustion from hopping between cheap places Checklist Before You Go # If you\u0026rsquo;re a family with children and still want to go:\nTravel insurance: Check the medical expense coverage limit. A single ER visit can cost 500,000 KRW. Typhoid vaccination: Get vaccinated at least two weeks before departure. Accommodation: Hotels or serviced residences over villas. The difference in hygiene and maintenance is real. VPN testing: Research coworking spaces in advance where you can work with your company VPN on. Supplies for kids: Korean books, toys, offline content downloaded to a tablet. Kindergarten: Honestly assess your child\u0026rsquo;s temperament. An English-language kindergarten can be stressful for an introverted child. Budget: Plan for your usual living expenses plus an extra 30-50%. The idea that Bali is cheap is based on local food and local accommodation. To maintain a quality of life that a Korean would find satisfactory, it can cost the same as or more than Korea. Closing Thoughts # Looking back after returning, the biggest takeaway from remote work in Bali is this:\nChanging your location doesn\u0026rsquo;t change your life. A person with one hour of free time a day still has one hour in Bali. A parent who needs to care for their child still needs to care for their child in Bali. On top of that, add hygiene risks, medical anxiety, and infrastructure inconveniences.\nThat said, I don\u0026rsquo;t regret this experience. If I hadn\u0026rsquo;t done it, I would have stayed curious, harboring a vague fantasy about Bali. There was value in simply confirming the fantasy against reality.\nHowever, if I were to do overseas remote work again under the same conditions (with a young child), it wouldn\u0026rsquo;t be Bali. A city with solid medical infrastructure, where you can take walks with your child. A place with sidewalks, playgrounds, and trustworthy tap water. A workation in a city where the basics of daily life are in place is a true workation.\n","date":"23 February 2026","externalUrl":null,"permalink":"/en/posts/bali-remote-work-with-family/","section":"Posts","summary":"Surfing, nature, the mecca of digital nomads. I took my child and worked remotely from Bali for a month. This is an honest record of the gap between expectations and reality.","title":"Remote Work in Bali with a Kid: An Honest Month-Long Review","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/remote-work/","section":"Tags","summary":"","title":"Remote-Work","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/ruff/","section":"Tags","summary":"","title":"Ruff","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/starburst/","section":"Tags","summary":"","title":"Starburst","type":"tags"},{"content":"If you run Trino as a production query engine, you\u0026rsquo;ve probably felt it throughout 2025. Releases have been getting sparse. This isn\u0026rsquo;t just a vague feeling — the numbers tell the story. Trino open-source releases dropped from 30 in 2024 to just 11 in 2025. A 63% decline.\nIn this post, I analyze why and how Starburst pivoted to become an AI-centric platform company, and lay out how Trino open-source users should think about this shift.\nWhat Happened to Trino Releases # A Sharp Drop in Release Frequency # Looking at Trino open-source releases by quarter, the downward trend is unmistakable.\nPeriod Releases Avg. Cadence Notes 2024 Q4 9 Weekly Stable pattern 2025 Q1 6 Biweekly Decline begins 2025 Q2 2 Monthly Sharp drop 2025 Q3 1 Quarterly All-time low 2025 Q4 2 Every 1.5 months Still sluggish Up through 2024 Q4, a new release came out every week. But starting in 2025 Q2, the cadence dropped to roughly once a month, and in Q3 there was only a single release for the entire quarter. Starburst Enterprise releases were similarly scarce.\nThe numbers look alarming, but this doesn\u0026rsquo;t signal the \u0026ldquo;decline\u0026rdquo; of Trino. It\u0026rsquo;s a strategic choice by Starburst.\nWhy the Drop # The answer becomes clear when you look at Starburst\u0026rsquo;s contribution to Trino open source. In 2024, the Starburst team accounted for 84% of all Trino commits. 138 contributors, 2,822 commits, and over 50 companies participated — but the real development engine was Starburst. And that engine started redirecting its engineering resources elsewhere.\nThat \u0026ldquo;elsewhere\u0026rdquo; is AI.\nStarburst\u0026rsquo;s AI Pivot # The Shift in Positioning # Tracking the evolution of Starburst\u0026rsquo;s official messaging reveals just how deliberate this pivot was.\nPeriod Positioning Core Message ~2023 Open Data Lakehouse Company Distributed query engine built on Trino 2024 Data Lake Analytics Platform Federated queries + Iceberg 2025 Data Platform for Apps and AI AI Agent + Agentic Workforce From \u0026ldquo;Open Data Lakehouse\u0026rdquo; to \u0026ldquo;Data Platform for Apps and AI.\u0026rdquo; This wasn\u0026rsquo;t just a marketing rebrand — the entire product roadmap was realigned in this direction.\n2025 Key Announcements Timeline # May 2025 — Launch Point\nStarburst officially announced AI Agent and AI Workflows.\nAI Agent: A natural-language interface for querying data. A question like \u0026ldquo;What were our sales in Europe last quarter?\u0026rdquo; is automatically converted to SQL. Air-gapped environments (finance, healthcare, government) are explicitly supported, along with Google\u0026rsquo;s Agent2Agent protocol and Anthropic\u0026rsquo;s Model Context Protocol (MCP). AI Workflows: A pipeline for storing vector embeddings in Iceberg tables and leveraging structured, semi-structured, and unstructured data for AI training. RAG (Retrieval-Augmented Generation) is natively supported. Other: Starburst Data Catalog (replacing Hive Metastore), Automated Table Maintenance (automated file cleanup and compaction), Native ODBC Driver, Role-based Query Routing October 2025 — AI \u0026amp; Datanova 2025\nHere, Starburst went a step further by announcing the Agentic Workforce platform and Lakeside AI Architecture.\nThe core concept is Model-to-Data architecture. Whereas the traditional approach collects data into a centralized warehouse and then runs AI models against it, Starburst proposes sending the AI model to where the data lives.\nTraditional: Data → Centralized Warehouse → AI Model Starburst: AI Model → Federated Data (where it lives) The logic is that by not moving data, you can maintain data sovereignty (GDPR, Schrems II) while still enabling unified analytics. Global financial institutions like Citi and HSBC are reportedly using this approach to unify data across 165 countries.\nThe Shift in the Tech Stack # Layering Starburst\u0026rsquo;s tech stack makes it clear what was added in 2025.\n┌─────────────────────────────────────┐ │ AI Agent \u0026amp; Agent2Agent Protocol │ ← 2025 NEW ├─────────────────────────────────────┤ │ AI Workflows (Vector Store) │ ← 2025 NEW ├─────────────────────────────────────┤ │ Starburst Data Catalog │ ← 2025 NEW ├─────────────────────────────────────┤ │ Lakehouse (Trino + Iceberg) │ ├─────────────────────────────────────┤ │ Federated Data Sources (50+) │ └─────────────────────────────────────┘ Trino still sits in the foundational layer, but all the innovation is happening above it. This is precisely why Trino open-source releases have slowed. Engineering resources have shifted to the upper layers.\nVector Store on Iceberg: A Technical Innovation Worth Watching # Among Starburst\u0026rsquo;s 2025 announcements, the most technically interesting is the approach of storing vector embeddings directly in Apache Iceberg tables.\nHere\u0026rsquo;s why this matters:\nNo separate vector DB required. You no longer need to operate dedicated vector databases like Pinecone, Weaviate, or Milvus. Leverages existing data engineering skills. You can manage vector data the same way you already manage Iceberg tables. Iceberg\u0026rsquo;s advantages extend to vector data. Time travel, ACID transactions, schema evolution, partitioning — all applicable to vector data. Governance policies apply consistently. The same access controls, audit logs, and data masking policies can be applied to both structured data and vector data. Open format means no vendor lock-in. Assuming a future where AI workloads become a core requirement of data platforms, this approach is remarkably pragmatic. There may be trade-offs in search performance compared to dedicated vector DBs, but the reduction in operational complexity and unified governance make this attractive for enterprise environments.\nBusiness Results: Is the Strategy Working in the Market? # The business metrics prove that the AI pivot isn\u0026rsquo;t just marketing.\nFY25 Results (announced February 2025):\nMetric Result New customers 20% YoY increase Galaxy (SaaS) customers 76% YoY increase Galaxy usage 94% YoY increase Largest deal Eight-figure multi-year contract with a global financial institution Partnerships Selected as the query engine for Dell Data Lakehouse The 76% growth in Galaxy (SaaS) customers is particularly noteworthy. It signals an accelerating shift toward cloud managed services — which, incidentally, is an alternative for teams currently self-managing open-source Trino.\nThe customer roster is equally impressive:\nHSBC: Data integration across 165 countries Citi: Unified analytics while maintaining global data sovereignty Vectra AI: Threat detection platform across 120 countries ZoomInfo: Multi-cloud data integration Competitive Positioning # Comparing Starburst\u0026rsquo;s position against competitors in the data platform market sharpens its points of differentiation.\nCapability Databricks Snowflake Dremio Starburst AI Agent O O X O Federated Query Limited Limited O Core strength Data Sovereignty Limited Limited Limited Core strength Open-source foundation Spark X Arrow Trino Vector Store O O X O (Iceberg) On-prem + Cloud O Limited O Core strength Starburst\u0026rsquo;s core differentiators boil down to three things:\nTrue federation: Real-time queries across 50+ data sources. Analyze data in place without moving it. Data sovereignty: Unified analytics while complying with regulations across 165 countries. A decisive advantage in GDPR and Schrems II environments. Hybrid deployment: Simultaneous support for on-premise and multi-cloud. Critical value for regulated industries where cloud migration is slow. On the other hand, Databricks is an AI/ML Lakehouse centered on Spark + Delta Lake, strengthening governance through Unity Catalog. Snowflake added Iceberg support in 2024 and is pushing AI/ML through Snowpark, but its model still assumes data centralization. Dremio emphasizes Arrow Flight-based performance and a semantic layer, but still lags in enterprise features.\nAn interesting remark from Starburst CEO Justin Borgman: \u0026ldquo;What they\u0026rsquo;ve done for Spark is what we aim to do for Presto (Trino).\u0026rdquo; Just as Databricks built a powerful commercial platform on top of the Spark open-source project, Starburst aims to build the same structure on top of Trino.\nIs Trino Open Source Going to Be Okay? # The Split Between Open-Source and Commercial Features # Here\u0026rsquo;s what currently remains in Trino open source versus what has moved to Starburst-only.\nTrino Open Source:\nCore query engine Standard connectors Fault-tolerant execution SQL MERGE Basic security features Starburst Only:\nWarp Speed (up to 7x performance improvement) AI Agent \u0026amp; AI Workflows Starburst Data Catalog Advanced governance (RBAC, data masking, audit logs) Automated Table Maintenance Smart Indexing Materialized Views (partial) The most notable item is Warp Speed. The fact that a proprietary indexing/caching layer delivering up to 7x performance improvement is commercial-only means that the performance gap between open source and the commercial product could widen for large-scale workloads.\nReasons for Optimism # The Trino core engine has reached maturity. As a distributed SQL query engine, it has most of the features it needs. Fewer releases doesn\u0026rsquo;t mean lower quality. The community is still active. Trino Summit 2024 was a success with participation from Netflix, LinkedIn, Wise, and others. The Trino Community Broadcast continues to run. Slack and GitHub activity remains healthy. Over 50 companies are contributing. Even if Starburst\u0026rsquo;s contributions decline, there\u0026rsquo;s room for other companies to pick up the slack. Reasons for Concern # The company responsible for 84% of commits has started focusing elsewhere. Whether other companies have sufficient incentive to fill this gap is an open question. The key to performance optimization is commercial-only. Teams running large-scale workloads without Warp Speed may find themselves at an increasing disadvantage. All AI-related innovation is concentrated in the commercial product. In a future where AI becomes essential to data platforms, competing on open source alone may become difficult. What Trino Operations Teams Should Consider # For teams running Trino in production, it helps to think about this situation across two time horizons.\nShort Term (1-2 years): No Major Concerns # Trino open source is still stable and battle-tested in production. Core features are sufficiently mature, and the baseline query performance and connector ecosystem are solid. There\u0026rsquo;s no reason to switch to an alternative right now.\nMid to Long Term (3-5 years): Strategic Preparation Is Needed # You need to account for the possibility that the feature gap between open source and the commercial product will widen. Preparation is needed in the following areas:\nPerformance optimization: How will you maintain performance for large-scale workloads without Warp Speed? Consider your own caching layers, indexing strategies, or expanding the role of complementary engines like StarRocks. AI integration: When AI integration becomes an organizational requirement for the data platform, evaluate whether open-source Trino alone is sufficient. Can you implement approaches like Vector Store on Iceberg on your own, or do you need to combine other tools? Governance: As the organization grows and regulations tighten, the need for advanced governance features (RBAC, data masking, audit logs) will increase. Can open source alone meet these requirements? Alternative evaluation: Periodically evaluate adopting Starburst Galaxy, switching to other query engines, or hybrid approaches (Trino for batch, StarRocks for real-time). Final Thoughts # Starburst\u0026rsquo;s AI pivot isn\u0026rsquo;t just a marketing play. The business metrics — 76% growth in Galaxy customers, the largest contract in company history — prove that this strategy is working in the market. The transition from a query engine company to an AI platform company is irreversible.\nTrino open source isn\u0026rsquo;t dying anytime soon. But it\u0026rsquo;s approaching a maintenance-mode state of being \u0026ldquo;sufficiently mature,\u0026rdquo; and the center of gravity for innovation has clearly shifted to the commercial product. This is the same pattern Databricks followed with Spark.\nIf you\u0026rsquo;re running Trino in production, you can rest easy for now, but preparation for three years from now should start today. The prudent approach is to lean on open source\u0026rsquo;s stability while securing strategic options along two axes: the performance gap and AI integration.\nTechnical debt always accumulates quietly. And the interest is always more expensive than you thought.\nReferences:\nTrino Release Notes Starburst Enterprise Release Notes TechTarget: Addition of new AI capabilities shows Starburst\u0026rsquo;s growth BigDataWire: Starburst\u0026rsquo;s New Platform Aims to Close AI\u0026rsquo;s Biggest Gap ","date":"23 February 2026","externalUrl":null,"permalink":"/en/posts/starburst-trino-ai-pivot/","section":"Posts","summary":"Trino open-source releases dropped 63% as Starburst pivoted from a query engine company to an AI platform. From the perspective of a team running Trino in production, here’s what this shift means and what to do about it.","title":"Starburst's AI Pivot: Is Trino Open Source Going to Be Okay?","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/starrocks/","section":"Tags","summary":"","title":"Starrocks","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/categories/starrocks/","section":"Categories","summary":"","title":"StarRocks","type":"categories"},{"content":" Background # If you run data pipelines long enough, you inevitably face one question: How do you build real-time dashboards?\nOur team was no different. Our existing pipeline looked like this:\nService → Kafka → Iceberg → S3 → Trino → Airflow(5min) → Dashboard On the surface it worked fine, but the pain points in practice were clear:\nAt least 5 minutes of latency: The Airflow schedule interval was the bottleneck Pipeline complexity: Managing 5+ components chained as Kafka → Flink → Redis → API → Dashboard Redundant I/O: Trino full-scanned S3 on every query High development cost: Each new real-time dashboard took roughly 2 weeks to build After adopting StarRocks, the architecture simplified to this:\nService → Kafka → StarRocks → Dashboard (sub-second latency) Eliminating the intermediate components dramatically simplified the pipeline, and ingesting data directly from Kafka into StarRocks gave us the real-time capability we needed.\nResults # After approximately 3 months of PoC and 6 months of phased rollout, we achieved the following improvements:\nMetric Before After Improvement Dashboard latency 5 min \u0026lt; 1 sec ~300x Dashboard dev time ~2 weeks ~1 week 50% reduction Pipeline components 5+ 2 60% reduction Query response time 30~50 sec 5~10 sec 5~10x Hardware cost 128 GB x 18 nodes 64 GB x 3 nodes ~75% savings Trino is fast in terms of raw query time, but when factoring in Airflow schedule delays and end-to-end latency, along with hardware cost efficiency, StarRocks proved to be a better fit for real-time workloads.\nTable Model Selection Guide # Choosing the right table model is the most important decision when first adopting StarRocks. A wrong choice means you will have to recreate the table later.\nDecision Flow # ┌─────────────────────────────┐ │ What data are you storing? │ └──────────────┬──────────────┘ │ ┌───────▼────────┐ │ Need UPDATE? │ └───────┬────────┘ │ ┌────────┴────────┐ │ │ [No] [Yes] │ │ ┌─────▼─────┐ ┌─────▼──────┐ │ Need │ │ Primary Key │ │aggregation?│ │ (frequent │ └─────┬─────┘ │ UPDATE) │ │ └────────────┘ [No] [Yes] │ │ ┌─────▼───┐ ┌▼──────────┐ │Duplicate│ │Aggregate │ │(raw data)│ │(auto-agg) ★│ └─────────┘ └────────────┘ Model Comparison # Model Duplicates Allowed UPDATE Auto-aggregation Best For Duplicate Key O X X Logs, raw events Aggregate Key X Auto O Real-time statistics ★ Primary Key X O (fast) X Frequent UPDATEs Duplicate Key: Storing Raw Data # Use this when you need to preserve original data as-is, such as click logs, API events, or sensor data.\nCREATE TABLE order_events ( event_id BIGINT, event_time DATETIME, order_id VARCHAR(50), user_id BIGINT, event_type VARCHAR(20), amount DECIMAL(10, 2) ) DUPLICATE KEY(event_id, event_time) PARTITION BY date_trunc(\u0026#39;day\u0026#39;, event_time) DISTRIBUTED BY HASH(event_id) BUCKETS 10; Aggregate Key: Real-time Statistics ★ # Data is automatically aggregated at ingestion time. This model was the key reason for adopting StarRocks.\nCREATE TABLE order_stats ( stat_time DATETIME NOT NULL COMMENT \u0026#39;5-minute intervals\u0026#39;, region VARCHAR(20) NOT NULL, delivery_type VARCHAR(20) NOT NULL, -- Aggregate columns: aggregate functions applied automatically at ingestion order_count BIGINT SUM DEFAULT \u0026#34;0\u0026#34;, total_amount DECIMAL(15, 2) SUM DEFAULT \u0026#34;0\u0026#34;, user_bitmap BITMAP BITMAP_UNION, max_amount DECIMAL(10, 2) MAX ) AGGREGATE KEY(stat_time, region, delivery_type) PARTITION BY date_trunc(\u0026#39;day\u0026#39;, stat_time) DISTRIBUTED BY HASH(stat_time) BUCKETS 10; Available aggregate functions:\nFunction Purpose Example SUM Summation Order count, total revenue MAX / MIN Maximum / minimum value Highest price, lowest price REPLACE Overwrite with latest value Latest status BITMAP_UNION Exact unique count Unique visitors HLL_UNION Approximate unique count High-cardinality sets Unlike HyperLogLog, BITMAP_UNION provides exact unique counts. For business KPI dashboards where accuracy matters, always use this approach.\nPrimary Key: Frequent UPDATEs # Best suited for scenarios where the same key is frequently updated, such as order status tracking or inventory management.\nCREATE TABLE orders ( order_id VARCHAR(50) NOT NULL, status VARCHAR(20), amount DECIMAL(10, 2), updated_at DATETIME ) PRIMARY KEY(order_id) DISTRIBUTED BY HASH(order_id) BUCKETS 10 PROPERTIES ( \u0026#34;enable_persistent_index\u0026#34; = \u0026#34;true\u0026#34; ); Enabling enable_persistent_index significantly improves UPDATE performance.\nData Ingestion # Routine Load: Real-time Kafka Integration # This approach continuously ingests data from a Kafka topic. It is the method used in most real-time pipelines.\nCREATE ROUTINE LOAD order_load ON orders COLUMNS( order_id, user_id, timestamp_ms, amount, order_date = FROM_UNIXTIME(timestamp_ms / 1000) ) PROPERTIES ( \u0026#34;format\u0026#34; = \u0026#34;json\u0026#34;, \u0026#34;jsonpaths\u0026#34; = \u0026#34;[\\\u0026#34;$.orderId\\\u0026#34;,\\\u0026#34;$.userId\\\u0026#34;,\\\u0026#34;$.timestamp\\\u0026#34;,\\\u0026#34;$.amount\\\u0026#34;]\u0026#34; ) FROM KAFKA ( \u0026#34;kafka_broker_list\u0026#34; = \u0026#34;kafka-broker:9092\u0026#34;, \u0026#34;kafka_topic\u0026#34; = \u0026#34;orders\u0026#34; ); When combined with an Aggregate Key table, transformation and aggregation happen simultaneously at ingestion time.\nCREATE ROUTINE LOAD order_stats_load ON order_stats COLUMNS( timestamp_ms, region, amount, user_id, -- Round to 5-minute intervals stat_time = FROM_UNIXTIME(FLOOR(timestamp_ms / 1000 / 300) * 300), order_count = 1, total_amount = amount, user_bitmap = BITMAP_HASH(user_id) ) WHERE amount \u0026gt; 0 PROPERTIES (\u0026#34;format\u0026#34; = \u0026#34;json\u0026#34;) FROM KAFKA ( \u0026#34;kafka_broker_list\u0026#34; = \u0026#34;kafka-broker:9092\u0026#34;, \u0026#34;kafka_topic\u0026#34; = \u0026#34;orders\u0026#34; ); This single pattern replaced the aggregation logic that previously required Flink \u0026ndash; using nothing but SQL.\nStream Load: Bulk Data Loading # Ideal for one-time bulk loads via files or APIs.\n# CSV file loading curl --location-trusted \\ -u user:password \\ -H \u0026#34;label:load_$(date +%Y%m%d%H%M%S)\u0026#34; \\ -H \u0026#34;column_separator:,\u0026#34; \\ -T data.csv \\ http://starrocks-fe:8030/api/mydb/mytable/_stream_load Performance Tuning Tips # Thread Pool Configuration # In high-load environments with 500+ RPS of concurrent connections, the default thread pool size is insufficient.\n# be.conf pipeline_scan_thread_pool_thread_num = 32 # default: 24 pipeline_exec_thread_pool_thread_num = 32 # default: 24 Bucket Count Guidelines # Data Size Recommended Buckets \u0026lt; 10 GB 10 10~50 GB 20 50~100 GB 30 \u0026gt; 100 GB 50+ Formula: buckets = max(1, data_size_GB / 10)\nPartitioning Strategy # Applying functions to partition columns prevents partition pruning. This is a more common mistake than you might think.\n-- ✅ Correct: partition pruning works WHERE event_time \u0026gt;= NOW() - INTERVAL 3 DAY -- ❌ Incorrect: partition pruning disabled WHERE DATE(event_time) \u0026gt;= CURRENT_DATE - 3 TTL Configuration # To automatically drop old partitions, configure TTL.\nPROPERTIES ( \u0026#34;partition_live_number\u0026#34; = \u0026#34;3\u0026#34; -- Keep only the 3 most recent partitions ) Operational Know-how # Materialized View Management # ASYNC refresh can stop without warning. Periodically check the status and manually recover when issues arise.\n-- Check status SHOW MATERIALIZED VIEWS; -- Force synchronous refresh REFRESH MATERIALIZED VIEW db.mv_name WITH SYNC MODE; -- Reactivate a deactivated MV ALTER MATERIALIZED VIEW db.mv_name ACTIVE; Routine Load Monitoring # The status frequently transitions to PAUSED. Common causes include Kafka offset issues or malformed messages.\n-- Check status SHOW ROUTINE LOAD FOR db.load_job; -- Resume RESUME ROUTINE LOAD FOR db.load_job; Scale-in Precautions # When scaling down nodes, you must perform a Decommission first. Removing nodes without this procedure will result in data loss.\n-- 1. Check current nodes SHOW PROC \u0026#39;/backends\u0026#39;; -- 2. Start decommission ALTER SYSTEM DECOMMISSION BACKEND \u0026#34;\u0026lt;BE_IP\u0026gt;:\u0026lt;HEARTBEAT_PORT\u0026gt;\u0026#34;; -- 3. Wait until TabletNum reaches 0, then remove ALTER SYSTEM DROP BACKEND \u0026#34;\u0026lt;BE_IP\u0026gt;:\u0026lt;HEARTBEAT_PORT\u0026gt;\u0026#34;; Things to Know Before Adopting # Known Limitations # Issue Description Workaround Routine Load Limited handling of malformed messages Pre-validate on the Kafka side datetime partitions Compatibility issues with Iceberg datetime partitions Use an alternative partitioning strategy Version upgrades Encountered bugs in 4.x releases Always test in a staging environment Always thoroughly validate version upgrades in a staging environment before applying them to production. We went through several rounds of upgrades and rollbacks ourselves. Have a rollback plan ready at all times.\nAdoption Checklist # Pre-deployment\nDefine use cases and requirements Estimate data volume and growth rate Choose table models Design partitioning strategy Post-deployment\nCreate and verify Routine Load jobs Configure user permissions Set data retention policies (TTL) Document scale-in/out procedures Build monitoring dashboards Conclusion # Here are the key lessons we learned from adopting StarRocks:\nThe Aggregate Key model is the centerpiece \u0026ndash; Automatic aggregation at ingestion time optimizes both storage and query performance Use BITMAP_UNION for exact unique counts \u0026ndash; Business KPIs demand precise numbers, not approximations Routine Load + Aggregate Key replaces Flink \u0026ndash; You can build a real-time aggregation pipeline with SQL alone Invest in operational automation \u0026ndash; Monitoring Materialized Views and Routine Load is essential For real-time analytics workloads, StarRocks is a powerful option that dramatically reduces pipeline complexity. That said, it is still maturing in terms of version upgrade stability and operational robustness, so we recommend conducting thorough PoC testing and staging validation before adopting it.\nReference: StarRocks Official Docs\n","date":"23 February 2026","externalUrl":null,"permalink":"/en/posts/starrocks-adoption-guide/","section":"Posts","summary":"How we reduced pipeline latency from 5 minutes to sub-second by adopting StarRocks. Covers table model selection, data ingestion, performance tuning, and operational lessons from production.","title":"StarRocks Adoption Story: Revolutionizing Data Pipelines with Real-time OLAP","type":"posts"},{"content":" Why Compression Settings Matter # When you operate StarRocks long enough, there will inevitably come a point where your data grows to tens of terabytes. I have seen, time and again, cases where a single compression setting made a 30 to 50 percent difference in storage costs. But this is not just about disk space. Higher compression ratios reduce disk I/O and improve scan performance, while overly aggressive compression burns CPU and increases latency. Ultimately, choosing the right compression algorithm for your workload is one of the most important tuning decisions in StarRocks operations.\nComparing Supported Compression Algorithms # StarRocks supports several compression algorithms. Here is a comparison of the three most commonly used in practice.\nAlgorithm Compression Ratio Compression Speed Decompression Speed Best-fit Workload LZ4 Moderate (2-3x) Very fast Very fast Real-time analytics, low-latency queries ZSTD High (4-6x) Moderate Fast Batch analytics, cold data Snappy Low (1.5-2x) Fast Fast General purpose, legacy compatibility ZLIB High (4-5x) Slow Moderate Archiving, infrequently accessed data Personally, the combination I use most often is LZ4 for hot data and ZSTD for cold data. I occasionally use Snappy when dealing with data migrated from the Hadoop ecosystem, but I do not recommend it for new tables.\nSetting Compression When Creating a Table # You can specify the compression algorithm via the compression property in PROPERTIES when creating a table. If no value is set, StarRocks defaults to LZ4.\nReal-time Analytics Table (LZ4) # CREATE TABLE analytics.realtime_events ( event_id BIGINT, user_id BIGINT, event_type VARCHAR(64), event_time DATETIME, properties JSON ) ENGINE = OLAP DUPLICATE KEY(event_id) DISTRIBUTED BY HASH(user_id) BUCKETS 32 PROPERTIES ( \u0026#34;replication_num\u0026#34; = \u0026#34;3\u0026#34;, \u0026#34;compression\u0026#34; = \u0026#34;LZ4\u0026#34; ); LZ4 has overwhelmingly fast decompression speed, making it ideal for tables that need to serve dashboard queries with sub-second response times.\nBatch Analytics Table (ZSTD) # CREATE TABLE warehouse.order_history ( order_id BIGINT, customer_id BIGINT, order_date DATE, total_amount DECIMAL(18, 2), status VARCHAR(32), items ARRAY\u0026lt;STRUCT\u0026lt;sku STRING, qty INT, price DECIMAL(10,2)\u0026gt;\u0026gt; ) ENGINE = OLAP DUPLICATE KEY(order_id) PARTITION BY RANGE(order_date) ( PARTITION p2025 VALUES LESS THAN (\u0026#39;2026-01-01\u0026#39;), PARTITION p2026 VALUES LESS THAN (\u0026#39;2027-01-01\u0026#39;) ) DISTRIBUTED BY HASH(customer_id) BUCKETS 16 PROPERTIES ( \u0026#34;replication_num\u0026#34; = \u0026#34;2\u0026#34;, \u0026#34;compression\u0026#34; = \u0026#34;ZSTD\u0026#34; ); ZSTD achieves 1.5 to 2 times higher compression ratios than LZ4, which translates into significant storage savings on history tables where hundreds of millions of rows accumulate per partition.\nChanging Compression on an Existing Table # If you want to change the compression algorithm on a table that is already in production, you can use ALTER TABLE. Keep in mind, however, that the new setting only applies to data loaded after the change. Existing segments will not be recompressed until a compaction cycle runs against them.\nALTER TABLE warehouse.order_history SET (\u0026#34;compression\u0026#34; = \u0026#34;ZSTD\u0026#34;); Recommended Compression Settings by Workload # The following recommendations are based on patterns I have validated repeatedly in production environments.\nReal-time dashboards / Ad-hoc queries: Use LZ4. Its CPU overhead is negligible, minimizing the impact on P99 latency. Nightly batch reports / ETL result tables: Use ZSTD. When query frequency is low and data volume is high, the storage savings translate directly into cost reductions. High-volume log ingestion: Use ZSTD, but lower zstd_compression_level to 3 or below. This strikes a good balance between compression speed and compression ratio. Benchmark Results: Compression Ratio vs. Performance Tradeoffs # The following benchmark was run against an event log table containing approximately 5 billion rows (roughly 800 GB uncompressed).\nMetric LZ4 ZSTD (level 3) ZSTD (level 9) Compressed size 320 GB 195 GB 170 GB Compression ratio 2.5x 4.1x 4.7x Simple scan query (Avg) 1.2 s 1.5 s 1.8 s Aggregation query (Avg) 3.4 s 3.8 s 4.5 s Data ingestion throughput 120 MB/s 95 MB/s 60 MB/s Compared to LZ4, ZSTD level 3 reduced storage by approximately 39% while only increasing query latency by about 10 to 15 percent. ZSTD level 9, on the other hand, offered diminishing compression gains at the cost of a steep drop in ingestion throughput, making level 3 the optimal choice in most environments.\nOperational Tips and Monitoring # Here are three tips from production experience:\nAlways monitor compaction. Keep an eye on the compaction_score metric. When compression settings change or data volumes spike, compaction can fall behind, leading to degraded query performance from an excessive number of small segments. Separate compression strategies at the table level. Do not apply a single compression algorithm across your entire cluster. Match the algorithm to each table\u0026rsquo;s access pattern \u0026ndash; LZ4 for frequently queried tables, ZSTD for archival ones. Track disk usage trends over time. Use SHOW DATA to monitor how each table\u0026rsquo;s storage footprint evolves after compression changes. SHOW DATA FROM warehouse.order_history; Compression is not a set-it-and-forget-it decision. As data volumes grow and query patterns shift, it is worth revisiting your compression settings periodically. Even a small adjustment can yield meaningful improvements in both performance and infrastructure costs over time.\n","date":"23 February 2026","externalUrl":null,"permalink":"/en/posts/starrocks-compression-guide/","section":"Posts","summary":"A practical guide to optimizing compression settings in StarRocks. Covers algorithm comparison, per-workload recommendations, benchmark results, and operational tips from production experience.","title":"StarRocks Compression Guide: Optimizing Performance and Storage","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/trends/","section":"Tags","summary":"","title":"Trends","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/vector-store/","section":"Tags","summary":"","title":"Vector-Store","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/en/tags/workation/","section":"Tags","summary":"","title":"Workation","type":"tags"},{"content":" Hi, I\u0026rsquo;m nanta # I\u0026rsquo;m a data engineer focused on building reliable, scalable data infrastructure and pipelines. I enjoy solving the challenges that come with moving, transforming, and serving data at scale.\nAbout This Blog # This blog is a place where I share practical data engineering knowledge \u0026ndash; lessons learned from real-world projects, tutorials, architecture decisions, and tips that I hope will be useful to fellow engineers working in the data space.\nTech Stack # Here are the core technologies I work with on a daily basis:\nStream Processing: Apache Kafka Workflow Orchestration: Apache Airflow Query Engines: Trino, StarRocks Container Orchestration: Kubernetes Languages: Python, SQL Infrastructure: Docker Contact # Email: nanta0032@naver.com Feel free to reach out if you have questions, suggestions, or just want to connect!\n","externalUrl":null,"permalink":"/en/about/","section":"nanta - Data Engineering","summary":"About me and this blog","title":"About","type":"page"},{"content":" Who We Are # This blog is operated by nanta. The site address is https://nanta-data.dev/.\nWhat Data We Collect # Google Analytics # This site uses Google Analytics 4 (GA4) to understand how visitors use the site. GA4 collects:\nPages you visit and how long you stay Your approximate location (country/city level) Device type, browser, and operating system Referral source (how you found this site) Google Analytics uses cookies to distinguish unique visitors. No personally identifiable information is intentionally collected.\nFor more information, see Google\u0026rsquo;s Privacy Policy.\nGoogle AdSense # This site may display advertisements through Google AdSense. AdSense may use cookies and web beacons to serve ads based on your prior visits to this and other websites.\nGoogle uses the DART cookie to serve ads based on your browsing history You may opt out of personalized advertising by visiting Google Ads Settings For more information, see Google AdSense Privacy Policy.\nComments # This site does not currently have a comment system. If one is added in the future, this policy will be updated.\nCookies # Cookies are small text files stored on your device. This site uses cookies for:\nAnalytics: To measure site traffic (Google Analytics) Advertising: To serve relevant ads (Google AdSense) Preferences: To remember your theme preference (dark/light mode) You can control cookies through your browser settings. Disabling cookies may affect site functionality.\nThird-Party Links # Blog posts may contain links to external websites. We are not responsible for the privacy practices of other sites.\nYour Rights # You have the right to:\nKnow what data is being collected Opt out of tracking (via browser settings or Google Ads Settings) Request information about your data Changes to This Policy # This privacy policy may be updated from time to time. Changes will be posted on this page.\nContact # If you have questions about this privacy policy, please contact us via the information on the About page.\nLast updated: February 23, 2026\n","externalUrl":null,"permalink":"/en/privacy-policy/","section":"nanta - Data Engineering","summary":"Privacy Policy","title":"Privacy Policy","type":"page"}]