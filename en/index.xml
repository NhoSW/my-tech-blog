<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>nanta - Data Engineering</title><link>https://nanta-data.dev/en/</link><description>Recent content on nanta - Data Engineering</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2026 nanta</copyright><lastBuildDate>Fri, 27 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://nanta-data.dev/en/index.xml" rel="self" type="application/rss+xml"/><item><title>Adopting Trino Gateway: Zero-Downtime Deployments and Multi-Cluster Routing</title><link>https://nanta-data.dev/en/posts/trino-gateway-zero-downtime/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/trino-gateway-zero-downtime/</guid><description>Trino doesn&amp;rsquo;t support coordinator HA. Redeploying the coordinator means downtime. We adopted Trino Gateway to enable Blue/Green deployments with zero downtime and route BI/OLAP queries to separate clusters based on HTTP headers.</description></item><item><title>BigQuery Data Transfer + Airflow: Why We Create and Delete Transfers Every Batch</title><link>https://nanta-data.dev/en/posts/bigquery-data-transfer-airflow/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/bigquery-data-transfer-airflow/</guid><description>We built a pipeline to load S3 mart tables into BigQuery using Data Transfer Service. During PoC, DTS scheduling was managed by GCP. For production, we moved it into Airflow — creating a transfer object each batch tick and deleting it after completion. User feedback drove improvements: multi-day lookback windows, concurrent execution quota management via slot pools, and empty source path detection through GCP logging API.</description></item><item><title>Building Iceberg Table Monitoring with Trino Meta-Tables and Prometheus Pushgateway</title><link>https://nanta-data.dev/en/posts/iceberg-table-monitoring/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/iceberg-table-monitoring/</guid><description>As our Iceberg table count grew, we needed systematic monitoring for file health. We built dashboards using Trino meta-tables, Prometheus Pushgateway, and Grafana to track file counts, sizes, and partition distribution. Along the way we hit Trino bugs and performance issues worth documenting.</description></item><item><title>EKS Topology Aware Hints: Why They Had No Effect on Our Cluster</title><link>https://nanta-data.dev/en/posts/eks-topology-aware-hints/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/eks-topology-aware-hints/</guid><description>We evaluated Kubernetes Topology Aware Hints to reduce cross-AZ network costs on EKS. Hints were correctly applied to EndpointSlices, but had no actual effect. AWS Load Balancer Controller&amp;rsquo;s IP target mode bypasses kube-proxy entirely, and our primary internal workloads — Spark, Trino, Airflow — are all single-zone or stateful, meaning the traffic paths where hints get referenced simply don&amp;rsquo;t exist in our environment.</description></item><item><title>EMR on EKS VPA Review: When an Official AWS Feature Doesn't Work</title><link>https://nanta-data.dev/en/posts/emr-on-eks-vpa-review/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/emr-on-eks-vpa-review/</guid><description>We tried using AWS&amp;rsquo;s built-in VPA integration for EMR on EKS to auto-optimize Spark executor resources. After about a month of intensive PoC work, multiple AWS support cases, and a custom manifest bundle rebuild, the operator still didn&amp;rsquo;t work. We abandoned it.</description></item><item><title>Kafka Rack Awareness and Spark: Not Supported Yet</title><link>https://nanta-data.dev/en/posts/kafka-rack-awareness-spark/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/kafka-rack-awareness-spark/</guid><description>We tried to apply Kafka rack awareness to Spark jobs to reduce cross-AZ network costs. Getting the AZ information was solved easily via AWS IMDS, but Spark itself doesn&amp;rsquo;t support rack-aware Kafka partition assignment. The related Jira ticket is open but the PR was closed.</description></item><item><title>S3 Table Buckets PoC: Evaluating Managed Iceberg for CDC Workloads</title><link>https://nanta-data.dev/en/posts/s3-table-buckets-poc/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/s3-table-buckets-poc/</guid><description>AWS S3 Table Buckets offer managed Iceberg tables with automatic compaction. We ran a PoC to see if they could solve our CDC table compaction problem. We validated Trino, Spark, and Kafka Connect integration, examined auto-compaction behavior, and assessed costs. The conclusion: not a fit for every table, but valuable specifically for CDC workloads with unpredictable partition-level updates.</description></item><item><title>Superset: Resolving Six User-Reported Issues</title><link>https://nanta-data.dev/en/posts/superset-user-reported-issues/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/superset-user-reported-issues/</guid><description>Our data governance team reported six issues they found while using Superset. From a missing .png extension on Slack screenshots to a CSV download 500 error caused by PostgreSQL&amp;rsquo;s idle_in_transaction_session_timeout killing the metadata DB connection mid-download. The trickiest one required tracing through both Superset server logs and RDS error logs to discover that the error connection and the causal connection were different — the SSL error was on the metadata DB, not Redshift.</description></item><item><title>Trino Alluxio Cache PoC: EBS Throughput Was the Bottleneck</title><link>https://nanta-data.dev/en/posts/trino-alluxio-cache-poc/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/trino-alluxio-cache-poc/</guid><description>We ran a PoC of Trino&amp;rsquo;s Alluxio-based file system cache in a production OLAP environment. Adding the cache alone didn&amp;rsquo;t help much. The default EBS throughput of 125 MiB/s was the bottleneck. After bumping it to 1000 MiB/s, query performance improved visibly and S3 API costs dropped by ~$3,200/month.</description></item><item><title>Trino v451 to v476: Upgrading a Production Cluster Across 25 Versions</title><link>https://nanta-data.dev/en/posts/trino-v476-upgrade/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/trino-v476-upgrade/</guid><description>We upgraded Trino from v451 to v476 — 25 minor versions in one jump. Blue/Green deployment on our OLAP clusters caught two regressions: a Materialized View refresh bug and a Parquet read failure. We binary-searched the root cause down to a single v469 commit and got a community fix within a day.</description></item><item><title>Why BigQuery Hangs in Superset: The gevent and gRPC Incompatibility</title><link>https://nanta-data.dev/en/posts/bigquery-bi-integration/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/bigquery-bi-integration/</guid><description>We integrated BigQuery with both Redash and Superset for per-user BI access. Redash had a known schema browsing limitation. Superset hit a much worse problem — queries executed successfully on BigQuery but results never came back, blocking indefinitely. The root cause: gunicorn&amp;rsquo;s gevent worker monkey-patches Python&amp;rsquo;s threading primitives, which deadlocks gRPC calls inside the BigQuery SDK. Switching the worker type to gthread resolved it. We contributed a docs fix upstream.</description></item><item><title>Building an ALB Log Pipeline with Filebeat, Flink, and Iceberg</title><link>https://nanta-data.dev/en/posts/alb-log-collection-system/</link><pubDate>Wed, 25 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/alb-log-collection-system/</guid><description>We needed to turn CSV ALB logs sitting in S3 into a queryable Iceberg table. The first attempt with Flink&amp;rsquo;s filesystem connector ran out of memory. This post covers the redesign with Filebeat + SQS + Kafka, autoscaling, checkpoint tuning, and the operational surprises we hit along the way.</description></item><item><title>Building a Proxy Server with Apache APISIX: De-identifying 1.3 Billion Daily Logs</title><link>https://nanta-data.dev/en/posts/apisix-proxy-server-guide/</link><pubDate>Tue, 24 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/apisix-proxy-server-guide/</guid><description>We needed to strip PII from app logs before sending them to an overseas tracking server. This post covers our journey from Fluent-bit to Kong to APISIX, custom Lua plugin development, 55K TPS load testing, and Kubernetes deployment.</description></item><item><title>Iceberg - Why CDC Table Compaction Is So Tricky</title><link>https://nanta-data.dev/en/posts/iceberg-cdc-compaction-challenge/</link><pubDate>Tue, 24 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/iceberg-cdc-compaction-challenge/</guid><description>Iceberg v2&amp;rsquo;s row-level delete implementation, the difference between position and equality deletes, and why compaction commits keep failing when a real-time CDC sink is running. Covers how v3 Deletion Vectors change the picture and the workaround we settled on for v2 in production.</description></item><item><title>2026 Data Engineering Trends and Where Large-Scale Platforms Stand</title><link>https://nanta-data.dev/en/posts/2026-data-engineering-trends/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/2026-data-engineering-trends/</guid><description>Analyzing 2026 data engineering trends from Joe Reis&amp;rsquo;s 1,101-respondent survey, contrasted with our team&amp;rsquo;s current architecture at a large-scale platform. An honest look at what we&amp;rsquo;re doing well and what we need to work on.</description></item><item><title>Airflow 3.0 Migration Guide: Lessons from a Large-Scale DAG Environment</title><link>https://nanta-data.dev/en/posts/airflow-3-migration-guide/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/airflow-3-migration-guide/</guid><description>Practical lessons from migrating to Airflow 3.x ahead of the 2.x EOL. Covers major breaking changes, a phased upgrade strategy, DAG compatibility approaches, and hard-won lessons from operating hundreds of DAGs in production.</description></item><item><title>NVIDIA MIG Setup Guide: Splitting One A100 GPU Into Many</title><link>https://nanta-data.dev/en/posts/nvidia-mig-setup-guide/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/nvidia-mig-setup-guide/</guid><description>Four A100 GPUs can yield up to 28 independent GPU instances. This guide covers MIG concepts, mig-parted installation, diverse slice configurations, and Kubernetes integration based on hands-on experience.</description></item><item><title>Remote Work in Bali with a Kid: An Honest Month-Long Review</title><link>https://nanta-data.dev/en/posts/bali-remote-work-with-family/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/bali-remote-work-with-family/</guid><description>Surfing, nature, the mecca of digital nomads. I took my child and worked remotely from Bali for a month. This is an honest record of the gap between expectations and reality.</description></item><item><title>Starburst's AI Pivot: Is Trino Open Source Going to Be Okay?</title><link>https://nanta-data.dev/en/posts/starburst-trino-ai-pivot/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/starburst-trino-ai-pivot/</guid><description>Trino open-source releases dropped 63% as Starburst pivoted from a query engine company to an AI platform. From the perspective of a team running Trino in production, here&amp;rsquo;s what this shift means and what to do about it.</description></item><item><title>StarRocks Adoption Story: Revolutionizing Data Pipelines with Real-time OLAP</title><link>https://nanta-data.dev/en/posts/starrocks-adoption-guide/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/starrocks-adoption-guide/</guid><description>How we reduced pipeline latency from 5 minutes to sub-second by adopting StarRocks. Covers table model selection, data ingestion, performance tuning, and operational lessons from production.</description></item><item><title>StarRocks Compression Guide: Optimizing Performance and Storage</title><link>https://nanta-data.dev/en/posts/starrocks-compression-guide/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/posts/starrocks-compression-guide/</guid><description>A practical guide to optimizing compression settings in StarRocks. Covers algorithm comparison, per-workload recommendations, benchmark results, and operational tips from production experience.</description></item><item><title>About</title><link>https://nanta-data.dev/en/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/about/</guid><description>About me and this blog</description></item><item><title>Privacy Policy</title><link>https://nanta-data.dev/en/privacy-policy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nanta-data.dev/en/privacy-policy/</guid><description>Privacy Policy</description></item></channel></rss>