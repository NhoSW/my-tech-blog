[{"content":"Introduction Running Apache Airflow in a development environment is straightforward. Running it in production, where dozens of DAGs process terabytes of data on tight schedules and failures page you at 3 AM, is an entirely different challenge. Over the past few years I have built and maintained Airflow deployments that orchestrate everything from simple ETL jobs to multi-stage machine learning pipelines. This post distills the lessons that cost me the most sleep.\nDAG Design Best Practices Make Every Task Idempotent The single most important rule is that every task must produce the same result regardless of how many times it runs. Airflow will retry your tasks. The scheduler will occasionally execute a task twice. If a task appends rows without checking whether they already exist, you end up with duplicates that silently corrupt downstream reports.\nIn practice this means using INSERT ... ON CONFLICT or MERGE statements instead of plain INSERT, partitioning output by execution date so that reruns overwrite the same partition, and never relying on wall-clock time inside a task when {{ ds }} or {{ data_interval_start }} is available.\nKeep Tasks Atomic and Focused A task should do one thing. When a single operator extracts data from an API, transforms it, and loads it into a warehouse, any failure forces a full re-run of the entire sequence. Splitting this into three discrete tasks \u0026ndash; extract, transform, load \u0026ndash; means that a transient warehouse timeout only retries the load step.\nModel Dependencies Explicitly Resist the temptation to chain tasks with \u0026gt;\u0026gt; in one long line. Group related tasks with TaskGroup, and use ExternalTaskSensor or the Dataset API to express cross-DAG dependencies. Implicit ordering through schedule alignment is fragile and breaks the moment execution times drift.\nError Handling Strategies Retries with exponential backoff. Most transient errors \u0026ndash; network timeouts, rate limits, brief service outages \u0026ndash; resolve themselves within minutes. Configure retries=3 and retry_delay=timedelta(minutes=5) as a baseline, and increase the delay exponentially with retry_exponential_backoff=True.\nSLA monitoring. Define sla=timedelta(hours=2) on critical tasks. When a task exceeds its SLA, Airflow fires an sla_miss_callback that can page your on-call engineer before downstream consumers even notice the delay.\nAlerting on failure. Use on_failure_callback at the DAG level to send a Slack or PagerDuty notification the instant any task fails. Do not rely solely on the Airflow UI \u0026ndash; no one is watching it at 3 AM.\nA Concrete Example Below is a simplified but production-style DAG that ingests order data from an API, stages it in cloud storage, and loads it into a data warehouse.\nfrom datetime import datetime, timedelta from airflow import DAG from airflow.decorators import task from airflow.providers.amazon.aws.transfers.local_to_s3 import ( LocalFilesystemToS3Operator, ) from airflow.providers.postgres.operators.postgres import PostgresOperator default_args = { \u0026#34;owner\u0026#34;: \u0026#34;data-engineering\u0026#34;, \u0026#34;retries\u0026#34;: 3, \u0026#34;retry_delay\u0026#34;: timedelta(minutes=5), \u0026#34;retry_exponential_backoff\u0026#34;: True, \u0026#34;on_failure_callback\u0026#34;: notify_slack, # defined elsewhere \u0026#34;sla\u0026#34;: timedelta(hours=2), } with DAG( dag_id=\u0026#34;orders_etl\u0026#34;, default_args=default_args, schedule=\u0026#34;@daily\u0026#34;, start_date=datetime(2026, 1, 1), catchup=False, tags=[\u0026#34;etl\u0026#34;, \u0026#34;orders\u0026#34;], ) as dag: @task() def extract_orders(**context): \u0026#34;\u0026#34;\u0026#34;Pull orders for the logical date from the API.\u0026#34;\u0026#34;\u0026#34; logical_date = context[\u0026#34;ds\u0026#34;] raw = fetch_orders_api(date=logical_date) # idempotent fetch path = f\u0026#34;/tmp/orders_{logical_date}.parquet\u0026#34; raw.to_parquet(path) return path upload_to_s3 = LocalFilesystemToS3Operator( task_id=\u0026#34;upload_to_s3\u0026#34;, filename=\u0026#34;{{ ti.xcom_pull(task_ids=\u0026#39;extract_orders\u0026#39;) }}\u0026#34;, dest_key=\u0026#34;raw/orders/{{ ds }}/orders.parquet\u0026#34;, dest_bucket=\u0026#34;data-lake-prod\u0026#34;, aws_conn_id=\u0026#34;aws_default\u0026#34;, replace=True, # overwrite ensures idempotency ) load_to_warehouse = PostgresOperator( task_id=\u0026#34;load_to_warehouse\u0026#34;, postgres_conn_id=\u0026#34;warehouse\u0026#34;, sql=\u0026#34;\u0026#34;\u0026#34; COPY orders_staging FROM \u0026#39;s3://data-lake-prod/raw/orders/{{ ds }}/orders.parquet\u0026#39; IAM_ROLE \u0026#39;arn:aws:iam::123456789012:role/redshift-copy\u0026#39; FORMAT AS PARQUET; -- Upsert into the final table for idempotency DELETE FROM orders WHERE order_date = \u0026#39;{{ ds }}\u0026#39;; INSERT INTO orders SELECT * FROM orders_staging; TRUNCATE orders_staging; \u0026#34;\u0026#34;\u0026#34;, ) extract_orders() \u0026gt;\u0026gt; upload_to_s3 \u0026gt;\u0026gt; load_to_warehouse Key things to notice: the replace=True flag on the S3 upload ensures reruns overwrite the same object, the warehouse load deletes before inserting to avoid duplicates, and retry configuration lives in default_args so it applies uniformly.\nOperational Tips Test DAGs before deploying. Run python your_dag.py to catch import errors, and use dag.test() (available since Airflow 2.5) to execute tasks locally against a real environment. Integrate these checks into CI so broken DAGs never reach production.\nManage connections through environment variables or a secrets backend. Storing credentials in the Airflow metadata database works for small teams, but it does not scale. HashiCorp Vault or AWS Secrets Manager as a backend keeps secrets centralized and auditable.\nMonitor resource consumption. Watch scheduler loop duration, DAG parse time, and worker slot utilization. A DAG that takes 30 seconds to parse slows down the entire scheduler. Move heavy imports inside task callables and keep the top-level DAG file as lean as possible.\nVersion your DAGs like application code. Use Git, require code review, and tag releases. When a DAG produces incorrect data, you need to know exactly which version was running and be able to roll back quickly.\nConclusion Reliable Airflow pipelines are not built by accident. They emerge from deliberate design choices \u0026ndash; idempotent tasks, explicit dependencies, aggressive retry policies, and proactive monitoring. None of these practices are revolutionary on their own, but applying them consistently is what separates a pipeline that quietly does its job from one that wakes you up at night. Start with idempotency, add proper alerting, and iterate from there. Your future on-call self will thank you.\n","permalink":"https://NhoSW.github.io/my-tech-blog/posts/building-data-pipeline-with-airflow/","summary":"Practical lessons from building and maintaining production Airflow DAGs. Covers DAG design patterns, error handling strategies, and operational best practices.","title":"Building Reliable Data Pipelines with Apache Airflow: Lessons from Production"},{"content":"Introduction Kafka Connect is one of the most powerful components in the Apache Kafka ecosystem, providing a scalable and reliable way to stream data between Kafka and external systems. However, running Kafka Connect in production is far from a set-and-forget experience. Connector tasks silently fail, serialization mismatches corrupt pipelines, and rebalancing storms can bring an entire Connect cluster to its knees.\nThis post distills the issues I have encountered most frequently while operating Kafka Connect across dozens of production pipelines, along with the concrete steps I use to diagnose and resolve them.\nCommon Connector Failures Tasks Stuck in FAILED State The single most common issue is a connector whose tasks transition to FAILED without an obvious reason. Your first move should always be the Connect REST API:\n# Check the status of a specific connector curl -s http://localhost:8083/connectors/my-jdbc-sink/status | jq . # Restart a single failed task (task 0) curl -s -X POST http://localhost:8083/connectors/my-jdbc-sink/tasks/0/restart The status response includes a trace field on failed tasks that contains the full Java stack trace. Read it carefully before restarting blindly \u0026ndash; a restart will not fix a misconfigured connection string or an expired credential.\nSerialization and Deserialization Errors Serialization problems account for a disproportionate share of connector failures. The root cause is almost always a mismatch between the converter configured on the connector and the actual format of the data on the topic.\nA typical mistake is pointing an Avro converter at a topic that contains JSON data, or vice versa. Make sure your connector configuration explicitly declares the correct converters:\n{ \u0026#34;name\u0026#34;: \u0026#34;orders-sink\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.jdbc.JdbcSinkConnector\u0026#34;, \u0026#34;topics\u0026#34;: \u0026#34;orders\u0026#34;, \u0026#34;connection.url\u0026#34;: \u0026#34;jdbc:postgresql://db-host:5432/analytics\u0026#34;, \u0026#34;connection.user\u0026#34;: \u0026#34;connect_user\u0026#34;, \u0026#34;connection.password\u0026#34;: \u0026#34;${file:/opt/connect/secrets/db.properties:db.password}\u0026#34;, \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, \u0026#34;value.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, \u0026#34;value.converter.schema.registry.url\u0026#34;: \u0026#34;http://schema-registry:8081\u0026#34;, \u0026#34;insert.mode\u0026#34;: \u0026#34;upsert\u0026#34;, \u0026#34;pk.mode\u0026#34;: \u0026#34;record_value\u0026#34;, \u0026#34;pk.fields\u0026#34;: \u0026#34;order_id\u0026#34;, \u0026#34;auto.create\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;batch.size\u0026#34;: 3000 } } Notice that key.converter and value.converter are set at the connector level, overriding the worker defaults. This is a best practice \u0026ndash; it makes each connector self-documenting and immune to changes in the worker-level configuration.\nSchema Registry Problems When using Avro or Protobuf converters, the Schema Registry becomes a critical dependency. If the registry is unreachable, every serialization call will fail. Common issues include:\nNetwork partitions or DNS failures between the Connect worker and the Schema Registry. Subject-level compatibility violations when a producer evolves a schema in a backward-incompatible way. Authentication misconfiguration when the registry sits behind basic auth or mTLS. You can verify registry connectivity directly from the Connect host:\n# List all registered subjects curl -s http://schema-registry:8081/subjects | jq . # Check the latest schema for a specific subject curl -s http://schema-registry:8081/subjects/orders-value/versions/latest | jq . Performance Troubleshooting Slow Sink Consumers and Growing Lag If your sink connector cannot keep up with the production rate, consumer lag will grow unboundedly. Start by checking the current lag:\nkafka-consumer-groups.sh \\ --bootstrap-server kafka:9092 \\ --describe \\ --group connect-orders-sink Common remedies include:\nIncrease tasks.max to parallelize consumption across more threads. The upper bound is the number of partitions on the source topic. Tune batch.size and consumer.override.max.poll.records to allow larger micro-batches, reducing per-record overhead. Check the downstream system \u0026ndash; a slow database, an overloaded HTTP endpoint, or a saturated network link is often the real bottleneck, not Kafka Connect itself. Rebalancing Storms In distributed mode, every time a connector or task is added, removed, or fails, the Connect cluster triggers a rebalance. Frequent rebalances (sometimes called \u0026ldquo;rebalancing storms\u0026rdquo;) cause all tasks to pause, leading to spikes in consumer lag.\nMitigation strategies:\nUse incremental cooperative rebalancing by setting connect.protocol=cooperative in the worker configuration (available since Kafka 2.3). Increase scheduled.rebalance.max.delay.ms to give workers a grace period to rejoin before their tasks are redistributed. Avoid deploying connectors during peak traffic windows. Debugging Techniques Structured Log Analysis Connect workers log extensively at the INFO level. For deeper investigation, temporarily raise the log level for a specific connector\u0026rsquo;s logger:\n# Set DEBUG logging for the JDBC sink connector curl -s -X PUT \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;level\u0026#34;: \u0026#34;DEBUG\u0026#34;}\u0026#39; \\ http://localhost:8083/admin/loggers/io.confluent.connect.jdbc This is a live change that does not require a worker restart, and it is invaluable for tracing exactly where a task fails in the put/flush cycle.\nJMX Metrics Kafka Connect exposes JMX metrics under the kafka.connect MBean domain. The most useful metrics for troubleshooting include:\nconnector-total-task-count and connector-failed-task-count \u0026ndash; instant visibility into task health. sink-record-read-rate and sink-record-send-rate \u0026ndash; throughput at the task level. offset-commit-completion-rate and offset-commit-skip-rate \u0026ndash; offset commit failures often precede data duplication or loss. Export these to Prometheus via JMX Exporter, and build Grafana dashboards that alert on failed task counts and abnormal lag growth.\nBest Practices for Production Stability Pin converter settings per connector. Never rely on worker-level converter defaults for production pipelines. Use externalized secrets. The ${file:...} config provider keeps credentials out of the Connect REST API responses and connector status endpoints. Deploy with infrastructure-as-code. Store connector JSON configurations in version control and apply them via CI/CD. This ensures reproducibility and enables rollback. Implement a dead-letter queue. For sink connectors, configure errors.tolerance=all and errors.deadletterqueue.topic.name to prevent a single poison message from halting the entire pipeline. Monitor offsets, not just lag. A connector can appear healthy (no failed tasks) while silently stalling on offset commits. Track offset-commit-completion-rate in your alerting stack. Test schema changes in staging first. Run your full Connect topology in a staging environment with realistic data volumes before promoting schema changes to production. Kafka Connect is a reliable workhorse when configured and monitored correctly. Most production incidents trace back to a handful of recurring causes \u0026ndash; serialization mismatches, untuned batching, and missing observability. Addressing these systematically will save you from many late-night pages.\n","permalink":"https://NhoSW.github.io/my-tech-blog/posts/kafka-connect-troubleshooting-guide/","summary":"A practical guide to diagnosing and fixing common Kafka Connect issues. Covers connector failures, serialization errors, performance bottlenecks, and operational tips.","title":"Kafka Connect Troubleshooting: Common Issues and Solutions"},{"content":"Trino is a powerful distributed SQL engine, but throwing queries at it without thinking about how it executes them is a fast track to slow dashboards, failed queries, and frustrated stakeholders. After running Trino in production against multi-terabyte data lakes for several years, I have collected a set of optimization patterns that consistently make a real difference. This post covers the ones I reach for most often.\n1. Partition Pruning and Predicate Pushdown The single highest-impact optimization is making sure Trino reads as little data as possible. If your tables are partitioned \u0026ndash; and they should be \u0026ndash; always filter on the partition column explicitly.\nBad \u0026ndash; full table scan:\nSELECT user_id, event_type, created_at FROM hive.analytics.events WHERE created_at \u0026gt;= TIMESTAMP \u0026#39;2026-01-01 00:00:00\u0026#39;; If dt is the partition column (a date string like 2026-01-15) and created_at is a timestamp column inside the data files, Trino cannot use the created_at filter to skip partitions. It has to open every partition and scan.\nOptimized \u0026ndash; partition column in the WHERE clause:\nSELECT user_id, event_type, created_at FROM hive.analytics.events WHERE dt \u0026gt;= \u0026#39;2026-01-01\u0026#39; AND created_at \u0026gt;= TIMESTAMP \u0026#39;2026-01-01 00:00:00\u0026#39;; Adding the dt predicate lets Trino prune partitions at the metastore level before reading a single byte of data. I have seen this alone reduce query times from 10+ minutes to under 30 seconds.\nThe same principle applies to predicate pushdown into connectors. Filters on columns that exist in file-level metadata (Parquet min/max statistics, for example) allow Trino to skip entire row groups. Keep your predicates simple and on the columns the storage layer knows about.\n2. Join Optimization Trino supports two join distribution strategies: broadcast and partitioned (distributed). The choice matters enormously.\nBroadcast join: The smaller table is copied to every worker node. Great when one side is small (\u0026lt; a few hundred MB). Distributed join: Both sides are hash-partitioned across workers. Required when both tables are large. Trino\u0026rsquo;s cost-based optimizer usually picks well, but when statistics are stale or absent, it guesses wrong. You can guide it:\n-- Force a broadcast join when you know the dimension table is small SELECT /*+ BROADCAST(d) */ f.order_id, f.amount, d.region_name FROM hive.warehouse.fact_orders f JOIN hive.warehouse.dim_regions d ON f.region_id = d.region_id; Join ordering also matters. Filter early, join late. Push filters and aggregations as close to the base tables as possible so that intermediate result sets are small before they hit the join.\nBad \u0026ndash; join first, filter later:\nSELECT o.order_id, o.amount, c.segment FROM orders o JOIN customers c ON o.customer_id = c.customer_id WHERE o.dt = \u0026#39;2026-02-01\u0026#39; AND c.segment = \u0026#39;enterprise\u0026#39;; Better \u0026ndash; pre-filter with CTEs for clarity and to help the optimizer:\nWITH filtered_orders AS ( SELECT order_id, amount, customer_id FROM orders WHERE dt = \u0026#39;2026-02-01\u0026#39; ), enterprise_customers AS ( SELECT customer_id, segment FROM customers WHERE segment = \u0026#39;enterprise\u0026#39; ) SELECT o.order_id, o.amount, c.segment FROM filtered_orders o JOIN enterprise_customers c ON o.customer_id = c.customer_id; In practice, Trino\u0026rsquo;s optimizer can often push predicates down through joins on its own. But making the intent explicit with CTEs both helps readability and gives the planner a clearer picture, especially in complex multi-join queries.\n3. Data Type and Format Considerations Your file format and schema design decisions are made long before query time, but they have a direct impact on performance.\nUse columnar formats. Parquet and ORC both support column projection and predicate pushdown via min/max statistics. Avoid JSON and CSV for analytical tables. Select only the columns you need. With columnar formats, every column you add to your SELECT is an additional I/O cost. Bad:\nSELECT * FROM hive.analytics.events WHERE dt = \u0026#39;2026-02-01\u0026#39;; Optimized:\nSELECT user_id, event_type, created_at FROM hive.analytics.events WHERE dt = \u0026#39;2026-02-01\u0026#39;; This is basic, but I still see SELECT * in production dashboards that read 200-column tables when only 4 columns are used. On a wide Parquet table the difference can be 10-20x in I/O.\nAlso watch your data types. Joining on a VARCHAR column when both sides could be BIGINT adds unnecessary overhead from hashing and comparison. If you control the pipeline, cast to the most efficient type upstream.\n4. Query Profiling with EXPLAIN and Query Stats Before optimizing blindly, measure. Trino gives you two essential tools:\nEXPLAIN shows the logical and distributed plan:\nEXPLAIN SELECT user_id, COUNT(*) AS cnt FROM hive.analytics.events WHERE dt = \u0026#39;2026-02-01\u0026#39; GROUP BY user_id; Look for ScanFilterProject to verify predicate pushdown is happening. Check whether the join strategy is REPLICATED (broadcast) or PARTITIONED (distributed). If the plan shows a full TableScan with no filter, your predicates are not being pushed down.\nEXPLAIN ANALYZE actually runs the query and gives you runtime statistics:\nEXPLAIN ANALYZE SELECT user_id, COUNT(*) AS cnt FROM hive.analytics.events WHERE dt = \u0026#39;2026-02-01\u0026#39; GROUP BY user_id; This shows wall time, rows processed, data read per stage, and memory usage. I make it a habit to run EXPLAIN ANALYZE on any new query before it goes into a scheduled pipeline. The Trino Web UI also exposes per-query stats, stage-level timing, and memory breakdowns \u0026ndash; all valuable for diagnosing bottlenecks.\n5. Resource Management Tips A single expensive query can starve an entire cluster. A few configuration and usage practices help:\nUse resource groups to isolate workloads. Give ETL pipelines, ad-hoc analysts, and dashboards separate concurrency and memory limits so they do not interfere with each other. Limit concurrent queries per user or group. Even well-optimized queries add up when 50 of them run simultaneously. Set session-level memory limits for exploratory work: SET SESSION query_max_memory = \u0026#39;2GB\u0026#39;; This prevents an accidental cross join from consuming the entire cluster\u0026rsquo;s memory before it gets killed.\nAvoid ORDER BY without LIMIT on large result sets. Sorting requires materializing the full result in memory. If you just need the top N rows, always add LIMIT. Final Thoughts Most Trino performance wins come from reading less data: prune partitions, select fewer columns, filter early. After that, getting join strategies right and profiling with EXPLAIN ANALYZE covers the majority of remaining issues. None of these tips are exotic \u0026ndash; they are fundamentals that compound. Getting them right consistently is what separates a query that finishes in 3 seconds from one that times out.\n","permalink":"https://NhoSW.github.io/my-tech-blog/posts/trino-query-optimization-tips/","summary":"Practical tips for optimizing Trino queries in production. Covers partition pruning, join strategies, data types, and query profiling techniques.","title":"Trino Query Optimization: Practical Tips for Better Performance"},{"content":"Hi, I\u0026rsquo;m Seungwoo Noh I\u0026rsquo;m a data engineer focused on building reliable, scalable data infrastructure and pipelines. I enjoy solving the challenges that come with moving, transforming, and serving data at scale.\nAbout This Blog This blog is a place where I share practical data engineering knowledge \u0026ndash; lessons learned from real-world projects, tutorials, architecture decisions, and tips that I hope will be useful to fellow engineers working in the data space.\nTech Stack Here are the core technologies I work with on a daily basis:\nStream Processing: Apache Kafka Workflow Orchestration: Apache Airflow Query Engines: Trino, StarRocks Container Orchestration: Kubernetes Languages: Python, SQL Infrastructure: Docker Contact GitHub: github.com/your-username LinkedIn: linkedin.com/in/your-profile Email: your-email@example.com Feel free to reach out if you have questions, suggestions, or just want to connect!\n","permalink":"https://NhoSW.github.io/my-tech-blog/about/","summary":"About me and this blog","title":"About"}]