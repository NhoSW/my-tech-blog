[{"content":"Introduction Running Apache Airflow in a development environment is straightforward. Running it in production, where dozens of DAGs process terabytes of data on tight schedules and failures page you at 3 AM, is an entirely different challenge. Over the past few years I have built and maintained Airflow deployments that orchestrate everything from simple ETL jobs to multi-stage machine learning pipelines. This post distills the lessons that cost me the most sleep.\nDAG Design Best Practices Make Every Task Idempotent The single most important rule is that every task must produce the same result regardless of how many times it runs. Airflow will retry your tasks. The scheduler will occasionally execute a task twice. If a task appends rows without checking whether they already exist, you end up with duplicates that silently corrupt downstream reports.\nIn practice this means using INSERT ... ON CONFLICT or MERGE statements instead of plain INSERT, partitioning output by execution date so that reruns overwrite the same partition, and never relying on wall-clock time inside a task when {{ ds }} or {{ data_interval_start }} is available.\nKeep Tasks Atomic and Focused A task should do one thing. When a single operator extracts data from an API, transforms it, and loads it into a warehouse, any failure forces a full re-run of the entire sequence. Splitting this into three discrete tasks \u0026ndash; extract, transform, load \u0026ndash; means that a transient warehouse timeout only retries the load step.\nModel Dependencies Explicitly Resist the temptation to chain tasks with \u0026gt;\u0026gt; in one long line. Group related tasks with TaskGroup, and use ExternalTaskSensor or the Dataset API to express cross-DAG dependencies. Implicit ordering through schedule alignment is fragile and breaks the moment execution times drift.\nError Handling Strategies Retries with exponential backoff. Most transient errors \u0026ndash; network timeouts, rate limits, brief service outages \u0026ndash; resolve themselves within minutes. Configure retries=3 and retry_delay=timedelta(minutes=5) as a baseline, and increase the delay exponentially with retry_exponential_backoff=True.\nSLA monitoring. Define sla=timedelta(hours=2) on critical tasks. When a task exceeds its SLA, Airflow fires an sla_miss_callback that can page your on-call engineer before downstream consumers even notice the delay.\nAlerting on failure. Use on_failure_callback at the DAG level to send a Slack or PagerDuty notification the instant any task fails. Do not rely solely on the Airflow UI \u0026ndash; no one is watching it at 3 AM.\nA Concrete Example Below is a simplified but production-style DAG that ingests order data from an API, stages it in cloud storage, and loads it into a data warehouse.\nfrom datetime import datetime, timedelta from airflow import DAG from airflow.decorators import task from airflow.providers.amazon.aws.transfers.local_to_s3 import ( LocalFilesystemToS3Operator, ) from airflow.providers.postgres.operators.postgres import PostgresOperator default_args = { \u0026#34;owner\u0026#34;: \u0026#34;data-engineering\u0026#34;, \u0026#34;retries\u0026#34;: 3, \u0026#34;retry_delay\u0026#34;: timedelta(minutes=5), \u0026#34;retry_exponential_backoff\u0026#34;: True, \u0026#34;on_failure_callback\u0026#34;: notify_slack, # defined elsewhere \u0026#34;sla\u0026#34;: timedelta(hours=2), } with DAG( dag_id=\u0026#34;orders_etl\u0026#34;, default_args=default_args, schedule=\u0026#34;@daily\u0026#34;, start_date=datetime(2026, 1, 1), catchup=False, tags=[\u0026#34;etl\u0026#34;, \u0026#34;orders\u0026#34;], ) as dag: @task() def extract_orders(**context): \u0026#34;\u0026#34;\u0026#34;Pull orders for the logical date from the API.\u0026#34;\u0026#34;\u0026#34; logical_date = context[\u0026#34;ds\u0026#34;] raw = fetch_orders_api(date=logical_date) # idempotent fetch path = f\u0026#34;/tmp/orders_{logical_date}.parquet\u0026#34; raw.to_parquet(path) return path upload_to_s3 = LocalFilesystemToS3Operator( task_id=\u0026#34;upload_to_s3\u0026#34;, filename=\u0026#34;{{ ti.xcom_pull(task_ids=\u0026#39;extract_orders\u0026#39;) }}\u0026#34;, dest_key=\u0026#34;raw/orders/{{ ds }}/orders.parquet\u0026#34;, dest_bucket=\u0026#34;data-lake-prod\u0026#34;, aws_conn_id=\u0026#34;aws_default\u0026#34;, replace=True, # overwrite ensures idempotency ) load_to_warehouse = PostgresOperator( task_id=\u0026#34;load_to_warehouse\u0026#34;, postgres_conn_id=\u0026#34;warehouse\u0026#34;, sql=\u0026#34;\u0026#34;\u0026#34; COPY orders_staging FROM \u0026#39;s3://data-lake-prod/raw/orders/{{ ds }}/orders.parquet\u0026#39; IAM_ROLE \u0026#39;arn:aws:iam::123456789012:role/redshift-copy\u0026#39; FORMAT AS PARQUET; -- Upsert into the final table for idempotency DELETE FROM orders WHERE order_date = \u0026#39;{{ ds }}\u0026#39;; INSERT INTO orders SELECT * FROM orders_staging; TRUNCATE orders_staging; \u0026#34;\u0026#34;\u0026#34;, ) extract_orders() \u0026gt;\u0026gt; upload_to_s3 \u0026gt;\u0026gt; load_to_warehouse Key things to notice: the replace=True flag on the S3 upload ensures reruns overwrite the same object, the warehouse load deletes before inserting to avoid duplicates, and retry configuration lives in default_args so it applies uniformly.\nOperational Tips Test DAGs before deploying. Run python your_dag.py to catch import errors, and use dag.test() (available since Airflow 2.5) to execute tasks locally against a real environment. Integrate these checks into CI so broken DAGs never reach production.\nManage connections through environment variables or a secrets backend. Storing credentials in the Airflow metadata database works for small teams, but it does not scale. HashiCorp Vault or AWS Secrets Manager as a backend keeps secrets centralized and auditable.\nMonitor resource consumption. Watch scheduler loop duration, DAG parse time, and worker slot utilization. A DAG that takes 30 seconds to parse slows down the entire scheduler. Move heavy imports inside task callables and keep the top-level DAG file as lean as possible.\nVersion your DAGs like application code. Use Git, require code review, and tag releases. When a DAG produces incorrect data, you need to know exactly which version was running and be able to roll back quickly.\nConclusion Reliable Airflow pipelines are not built by accident. They emerge from deliberate design choices \u0026ndash; idempotent tasks, explicit dependencies, aggressive retry policies, and proactive monitoring. None of these practices are revolutionary on their own, but applying them consistently is what separates a pipeline that quietly does its job from one that wakes you up at night. Start with idempotency, add proper alerting, and iterate from there. Your future on-call self will thank you.\n","permalink":"https://NhoSW.github.io/my-tech-blog/posts/building-data-pipeline-with-airflow/","summary":"Practical lessons from building and maintaining production Airflow DAGs. Covers DAG design patterns, error handling strategies, and operational best practices.","title":"Building Reliable Data Pipelines with Apache Airflow: Lessons from Production"},{"content":"Introduction Kafka Connect is one of the most powerful components in the Apache Kafka ecosystem, providing a scalable and reliable way to stream data between Kafka and external systems. However, running Kafka Connect in production is far from a set-and-forget experience. Connector tasks silently fail, serialization mismatches corrupt pipelines, and rebalancing storms can bring an entire Connect cluster to its knees.\nThis post distills the issues I have encountered most frequently while operating Kafka Connect across dozens of production pipelines, along with the concrete steps I use to diagnose and resolve them.\nCommon Connector Failures Tasks Stuck in FAILED State The single most common issue is a connector whose tasks transition to FAILED without an obvious reason. Your first move should always be the Connect REST API:\n# Check the status of a specific connector curl -s http://localhost:8083/connectors/my-jdbc-sink/status | jq . # Restart a single failed task (task 0) curl -s -X POST http://localhost:8083/connectors/my-jdbc-sink/tasks/0/restart The status response includes a trace field on failed tasks that contains the full Java stack trace. Read it carefully before restarting blindly \u0026ndash; a restart will not fix a misconfigured connection string or an expired credential.\nSerialization and Deserialization Errors Serialization problems account for a disproportionate share of connector failures. The root cause is almost always a mismatch between the converter configured on the connector and the actual format of the data on the topic.\nA typical mistake is pointing an Avro converter at a topic that contains JSON data, or vice versa. Make sure your connector configuration explicitly declares the correct converters:\n{ \u0026#34;name\u0026#34;: \u0026#34;orders-sink\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.jdbc.JdbcSinkConnector\u0026#34;, \u0026#34;topics\u0026#34;: \u0026#34;orders\u0026#34;, \u0026#34;connection.url\u0026#34;: \u0026#34;jdbc:postgresql://db-host:5432/analytics\u0026#34;, \u0026#34;connection.user\u0026#34;: \u0026#34;connect_user\u0026#34;, \u0026#34;connection.password\u0026#34;: \u0026#34;${file:/opt/connect/secrets/db.properties:db.password}\u0026#34;, \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.storage.StringConverter\u0026#34;, \u0026#34;value.converter\u0026#34;: \u0026#34;io.confluent.connect.avro.AvroConverter\u0026#34;, \u0026#34;value.converter.schema.registry.url\u0026#34;: \u0026#34;http://schema-registry:8081\u0026#34;, \u0026#34;insert.mode\u0026#34;: \u0026#34;upsert\u0026#34;, \u0026#34;pk.mode\u0026#34;: \u0026#34;record_value\u0026#34;, \u0026#34;pk.fields\u0026#34;: \u0026#34;order_id\u0026#34;, \u0026#34;auto.create\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;batch.size\u0026#34;: 3000 } } Notice that key.converter and value.converter are set at the connector level, overriding the worker defaults. This is a best practice \u0026ndash; it makes each connector self-documenting and immune to changes in the worker-level configuration.\nSchema Registry Problems When using Avro or Protobuf converters, the Schema Registry becomes a critical dependency. If the registry is unreachable, every serialization call will fail. Common issues include:\nNetwork partitions or DNS failures between the Connect worker and the Schema Registry. Subject-level compatibility violations when a producer evolves a schema in a backward-incompatible way. Authentication misconfiguration when the registry sits behind basic auth or mTLS. You can verify registry connectivity directly from the Connect host:\n# List all registered subjects curl -s http://schema-registry:8081/subjects | jq . # Check the latest schema for a specific subject curl -s http://schema-registry:8081/subjects/orders-value/versions/latest | jq . Performance Troubleshooting Slow Sink Consumers and Growing Lag If your sink connector cannot keep up with the production rate, consumer lag will grow unboundedly. Start by checking the current lag:\nkafka-consumer-groups.sh \\ --bootstrap-server kafka:9092 \\ --describe \\ --group connect-orders-sink Common remedies include:\nIncrease tasks.max to parallelize consumption across more threads. The upper bound is the number of partitions on the source topic. Tune batch.size and consumer.override.max.poll.records to allow larger micro-batches, reducing per-record overhead. Check the downstream system \u0026ndash; a slow database, an overloaded HTTP endpoint, or a saturated network link is often the real bottleneck, not Kafka Connect itself. Rebalancing Storms In distributed mode, every time a connector or task is added, removed, or fails, the Connect cluster triggers a rebalance. Frequent rebalances (sometimes called \u0026ldquo;rebalancing storms\u0026rdquo;) cause all tasks to pause, leading to spikes in consumer lag.\nMitigation strategies:\nUse incremental cooperative rebalancing by setting connect.protocol=cooperative in the worker configuration (available since Kafka 2.3). Increase scheduled.rebalance.max.delay.ms to give workers a grace period to rejoin before their tasks are redistributed. Avoid deploying connectors during peak traffic windows. Debugging Techniques Structured Log Analysis Connect workers log extensively at the INFO level. For deeper investigation, temporarily raise the log level for a specific connector\u0026rsquo;s logger:\n# Set DEBUG logging for the JDBC sink connector curl -s -X PUT \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;level\u0026#34;: \u0026#34;DEBUG\u0026#34;}\u0026#39; \\ http://localhost:8083/admin/loggers/io.confluent.connect.jdbc This is a live change that does not require a worker restart, and it is invaluable for tracing exactly where a task fails in the put/flush cycle.\nJMX Metrics Kafka Connect exposes JMX metrics under the kafka.connect MBean domain. The most useful metrics for troubleshooting include:\nconnector-total-task-count and connector-failed-task-count \u0026ndash; instant visibility into task health. sink-record-read-rate and sink-record-send-rate \u0026ndash; throughput at the task level. offset-commit-completion-rate and offset-commit-skip-rate \u0026ndash; offset commit failures often precede data duplication or loss. Export these to Prometheus via JMX Exporter, and build Grafana dashboards that alert on failed task counts and abnormal lag growth.\nBest Practices for Production Stability Pin converter settings per connector. Never rely on worker-level converter defaults for production pipelines. Use externalized secrets. The ${file:...} config provider keeps credentials out of the Connect REST API responses and connector status endpoints. Deploy with infrastructure-as-code. Store connector JSON configurations in version control and apply them via CI/CD. This ensures reproducibility and enables rollback. Implement a dead-letter queue. For sink connectors, configure errors.tolerance=all and errors.deadletterqueue.topic.name to prevent a single poison message from halting the entire pipeline. Monitor offsets, not just lag. A connector can appear healthy (no failed tasks) while silently stalling on offset commits. Track offset-commit-completion-rate in your alerting stack. Test schema changes in staging first. Run your full Connect topology in a staging environment with realistic data volumes before promoting schema changes to production. Kafka Connect is a reliable workhorse when configured and monitored correctly. Most production incidents trace back to a handful of recurring causes \u0026ndash; serialization mismatches, untuned batching, and missing observability. Addressing these systematically will save you from many late-night pages.\n","permalink":"https://NhoSW.github.io/my-tech-blog/posts/kafka-connect-troubleshooting-guide/","summary":"A practical guide to diagnosing and fixing common Kafka Connect issues. Covers connector failures, serialization errors, performance bottlenecks, and operational tips.","title":"Kafka Connect Troubleshooting: Common Issues and Solutions"},{"content":"왜 압축 설정이 중요한가 StarRocks를 운영하다 보면 데이터가 수십 TB 규모로 늘어나는 시점이 반드시 온다. 이때 압축 설정 하나로 스토리지 비용이 30~50% 차이 나는 경우를 여러 번 경험했다. 단순히 저장 공간만의 문제가 아니다. 압축률이 높으면 디스크 I/O가 줄어들어 스캔 성능이 좋아지고, 반대로 압축/해제에 CPU를 많이 쓰면 지연 시간이 늘어난다. 결국 워크로드 특성에 맞는 압축 알고리즘을 선택하는 것이 StarRocks 운영의 핵심 튜닝 포인트 중 하나다.\n지원되는 압축 알고리즘 비교 StarRocks는 여러 압축 알고리즘을 지원한다. 실무에서 주로 사용하는 세 가지를 비교해 보겠다.\n알고리즘 압축률 압축 속도 해제 속도 적합한 워크로드 LZ4 보통 (2~3x) 매우 빠름 매우 빠름 실시간 분석, 저지연 쿼리 ZSTD 높음 (4~6x) 보통 빠름 배치 분석, 콜드 데이터 Snappy 낮음 (1.5~2x) 빠름 빠름 범용, 레거시 호환 ZLIB 높음 (4~5x) 느림 보통 아카이빙, 저빈도 접근 데이터 개인적으로 가장 많이 쓰는 조합은 핫 데이터에 LZ4, 콜드 데이터에 ZSTD다. Snappy는 Hadoop 에코시스템에서 넘어온 데이터를 다룰 때 간혹 사용하지만, 신규 테이블에는 권장하지 않는다.\n테이블 생성 시 압축 설정 방법 테이블을 생성할 때 PROPERTIES에서 compression 속성을 지정하면 된다. 별도로 설정하지 않으면 StarRocks 기본값인 LZ4가 적용된다.\n실시간 분석용 테이블 (LZ4) CREATE TABLE analytics.realtime_events ( event_id BIGINT, user_id BIGINT, event_type VARCHAR(64), event_time DATETIME, properties JSON ) ENGINE = OLAP DUPLICATE KEY(event_id) DISTRIBUTED BY HASH(user_id) BUCKETS 32 PROPERTIES ( \u0026#34;replication_num\u0026#34; = \u0026#34;3\u0026#34;, \u0026#34;compression\u0026#34; = \u0026#34;LZ4\u0026#34; ); LZ4는 해제 속도가 압도적으로 빠르기 때문에, 대시보드 쿼리처럼 수백 밀리초 이내 응답이 필요한 테이블에 적합하다.\n배치 분석용 테이블 (ZSTD) CREATE TABLE warehouse.order_history ( order_id BIGINT, customer_id BIGINT, order_date DATE, total_amount DECIMAL(18, 2), status VARCHAR(32), items ARRAY\u0026lt;STRUCT\u0026lt;sku STRING, qty INT, price DECIMAL(10,2)\u0026gt;\u0026gt; ) ENGINE = OLAP DUPLICATE KEY(order_id) PARTITION BY RANGE(order_date) ( PARTITION p2025 VALUES LESS THAN (\u0026#39;2026-01-01\u0026#39;), PARTITION p2026 VALUES LESS THAN (\u0026#39;2027-01-01\u0026#39;) ) DISTRIBUTED BY HASH(customer_id) BUCKETS 16 PROPERTIES ( \u0026#34;replication_num\u0026#34; = \u0026#34;2\u0026#34;, \u0026#34;compression\u0026#34; = \u0026#34;ZSTD\u0026#34; ); ZSTD는 압축률이 LZ4 대비 1.5~2배 높아서, 파티션 단위로 수억 건 이상 적재되는 히스토리 테이블에서 스토리지 절감 효과가 크다.\n기존 테이블 압축 변경 이미 운영 중인 테이블의 압축 알고리즘을 변경하고 싶다면 ALTER TABLE을 사용할 수 있다. 단, 변경 이후 새로 적재되는 데이터부터 적용되며 기존 세그먼트는 Compaction이 수행되어야 반영된다는 점에 유의하자.\nALTER TABLE warehouse.order_history SET (\u0026#34;compression\u0026#34; = \u0026#34;ZSTD\u0026#34;); 워크로드별 권장 압축 설정 실무에서 반복적으로 검증한 결과를 기반으로 정리하면 다음과 같다.\n실시간 대시보드 / Ad-hoc 쿼리: LZ4를 권장한다. CPU 오버헤드가 거의 없어 P99 지연 시간에 미치는 영향이 최소화된다. 야간 배치 리포트 / ETL 결과 테이블: ZSTD를 권장한다. 쿼리 빈도가 낮고 데이터 양이 많은 경우 스토리지 절감 효과가 비용에 직접 반영된다. 로그성 대용량 적재: ZSTD를 사용하되 zstd_compression_level을 3 이하로 낮추면 압축 속도와 압축률 사이의 균형을 잡을 수 있다. 압축률과 성능 트레이드오프 실측 결과 약 50억 건(원본 약 800GB)의 이벤트 로그 테이블을 대상으로 압축 알고리즘별 벤치마크를 수행한 결과다.\n지표 LZ4 ZSTD (level 3) ZSTD (level 9) 압축 후 크기 320 GB 195 GB 170 GB 압축률 2.5x 4.1x 4.7x 단순 스캔 쿼리 (Avg) 1.2초 1.5초 1.8초 집계 쿼리 (Avg) 3.4초 3.8초 4.5초 데이터 적재 속도 120 MB/s 95 MB/s 60 MB/s LZ4 대비 ZSTD level 3은 스토리지를 약 39% 절감하면서도 쿼리 지연은 약 10~15%만 증가했다. 반면 ZSTD level 9는 추가 압축 이득 대비 적재 속도 저하가 커서 대부분의 환경에서는 level 3이 최적의 선택이었다.\n운영 팁과 모니터링 마지막으로 실무에서 압축 관련 운영 시 놓치기 쉬운 포인트를 정리한다.\nCompaction 모니터링을 반드시 하자. 압축 알고리즘을 변경한 뒤 Compaction이 완료되기 전까지 혼합 세그먼트가 존재하면 쿼리 성능이 일시적으로 불안정해질 수 있다. BE의 compaction_score 메트릭을 모니터링하여 Compaction 적체 여부를 확인해야 한다.\n테이블 단위로 압축 전략을 분리하라. 하나의 클러스터에서 모든 테이블에 동일한 압축을 적용하는 것은 비효율적이다. 접근 빈도, 데이터 크기, SLA에 따라 테이블별로 다르게 설정하는 것이 올바른 접근이다.\n디스크 사용량 추이를 추적하라. 압축 변경 후 SHOW DATA 명령으로 테이블별 실제 디스크 사용량을 주기적으로 확인하고, 기대한 압축률이 나오지 않는다면 데이터 특성(카디널리티, NULL 비율 등)을 재점검해야 한다.\nSHOW DATA FROM warehouse.order_history; 압축 설정은 한 번 정하고 끝나는 것이 아니라, 데이터 특성과 워크로드가 변함에 따라 지속적으로 재검토해야 하는 영역이다. 이 글이 StarRocks 운영에서 압축 전략을 수립하는 데 실질적인 참고가 되길 바란다.\n","permalink":"https://NhoSW.github.io/my-tech-blog/ko/posts/starracks-compression-guide/","summary":"StarRocks에서 압축 설정을 최적화하여 스토리지 비용을 절감하고 쿼리 성능을 향상시키는 방법을 실무 경험을 바탕으로 정리했습니다.","title":"StarRocks 압축 설정 가이드: 성능과 스토리지 최적화"},{"content":"Trino is a powerful distributed SQL engine, but throwing queries at it without thinking about how it executes them is a fast track to slow dashboards, failed queries, and frustrated stakeholders. After running Trino in production against multi-terabyte data lakes for several years, I have collected a set of optimization patterns that consistently make a real difference. This post covers the ones I reach for most often.\n1. Partition Pruning and Predicate Pushdown The single highest-impact optimization is making sure Trino reads as little data as possible. If your tables are partitioned \u0026ndash; and they should be \u0026ndash; always filter on the partition column explicitly.\nBad \u0026ndash; full table scan:\nSELECT user_id, event_type, created_at FROM hive.analytics.events WHERE created_at \u0026gt;= TIMESTAMP \u0026#39;2026-01-01 00:00:00\u0026#39;; If dt is the partition column (a date string like 2026-01-15) and created_at is a timestamp column inside the data files, Trino cannot use the created_at filter to skip partitions. It has to open every partition and scan.\nOptimized \u0026ndash; partition column in the WHERE clause:\nSELECT user_id, event_type, created_at FROM hive.analytics.events WHERE dt \u0026gt;= \u0026#39;2026-01-01\u0026#39; AND created_at \u0026gt;= TIMESTAMP \u0026#39;2026-01-01 00:00:00\u0026#39;; Adding the dt predicate lets Trino prune partitions at the metastore level before reading a single byte of data. I have seen this alone reduce query times from 10+ minutes to under 30 seconds.\nThe same principle applies to predicate pushdown into connectors. Filters on columns that exist in file-level metadata (Parquet min/max statistics, for example) allow Trino to skip entire row groups. Keep your predicates simple and on the columns the storage layer knows about.\n2. Join Optimization Trino supports two join distribution strategies: broadcast and partitioned (distributed). The choice matters enormously.\nBroadcast join: The smaller table is copied to every worker node. Great when one side is small (\u0026lt; a few hundred MB). Distributed join: Both sides are hash-partitioned across workers. Required when both tables are large. Trino\u0026rsquo;s cost-based optimizer usually picks well, but when statistics are stale or absent, it guesses wrong. You can guide it:\n-- Force a broadcast join when you know the dimension table is small SELECT /*+ BROADCAST(d) */ f.order_id, f.amount, d.region_name FROM hive.warehouse.fact_orders f JOIN hive.warehouse.dim_regions d ON f.region_id = d.region_id; Join ordering also matters. Filter early, join late. Push filters and aggregations as close to the base tables as possible so that intermediate result sets are small before they hit the join.\nBad \u0026ndash; join first, filter later:\nSELECT o.order_id, o.amount, c.segment FROM orders o JOIN customers c ON o.customer_id = c.customer_id WHERE o.dt = \u0026#39;2026-02-01\u0026#39; AND c.segment = \u0026#39;enterprise\u0026#39;; Better \u0026ndash; pre-filter with CTEs for clarity and to help the optimizer:\nWITH filtered_orders AS ( SELECT order_id, amount, customer_id FROM orders WHERE dt = \u0026#39;2026-02-01\u0026#39; ), enterprise_customers AS ( SELECT customer_id, segment FROM customers WHERE segment = \u0026#39;enterprise\u0026#39; ) SELECT o.order_id, o.amount, c.segment FROM filtered_orders o JOIN enterprise_customers c ON o.customer_id = c.customer_id; In practice, Trino\u0026rsquo;s optimizer can often push predicates down through joins on its own. But making the intent explicit with CTEs both helps readability and gives the planner a clearer picture, especially in complex multi-join queries.\n3. Data Type and Format Considerations Your file format and schema design decisions are made long before query time, but they have a direct impact on performance.\nUse columnar formats. Parquet and ORC both support column projection and predicate pushdown via min/max statistics. Avoid JSON and CSV for analytical tables. Select only the columns you need. With columnar formats, every column you add to your SELECT is an additional I/O cost. Bad:\nSELECT * FROM hive.analytics.events WHERE dt = \u0026#39;2026-02-01\u0026#39;; Optimized:\nSELECT user_id, event_type, created_at FROM hive.analytics.events WHERE dt = \u0026#39;2026-02-01\u0026#39;; This is basic, but I still see SELECT * in production dashboards that read 200-column tables when only 4 columns are used. On a wide Parquet table the difference can be 10-20x in I/O.\nAlso watch your data types. Joining on a VARCHAR column when both sides could be BIGINT adds unnecessary overhead from hashing and comparison. If you control the pipeline, cast to the most efficient type upstream.\n4. Query Profiling with EXPLAIN and Query Stats Before optimizing blindly, measure. Trino gives you two essential tools:\nEXPLAIN shows the logical and distributed plan:\nEXPLAIN SELECT user_id, COUNT(*) AS cnt FROM hive.analytics.events WHERE dt = \u0026#39;2026-02-01\u0026#39; GROUP BY user_id; Look for ScanFilterProject to verify predicate pushdown is happening. Check whether the join strategy is REPLICATED (broadcast) or PARTITIONED (distributed). If the plan shows a full TableScan with no filter, your predicates are not being pushed down.\nEXPLAIN ANALYZE actually runs the query and gives you runtime statistics:\nEXPLAIN ANALYZE SELECT user_id, COUNT(*) AS cnt FROM hive.analytics.events WHERE dt = \u0026#39;2026-02-01\u0026#39; GROUP BY user_id; This shows wall time, rows processed, data read per stage, and memory usage. I make it a habit to run EXPLAIN ANALYZE on any new query before it goes into a scheduled pipeline. The Trino Web UI also exposes per-query stats, stage-level timing, and memory breakdowns \u0026ndash; all valuable for diagnosing bottlenecks.\n5. Resource Management Tips A single expensive query can starve an entire cluster. A few configuration and usage practices help:\nUse resource groups to isolate workloads. Give ETL pipelines, ad-hoc analysts, and dashboards separate concurrency and memory limits so they do not interfere with each other. Limit concurrent queries per user or group. Even well-optimized queries add up when 50 of them run simultaneously. Set session-level memory limits for exploratory work: SET SESSION query_max_memory = \u0026#39;2GB\u0026#39;; This prevents an accidental cross join from consuming the entire cluster\u0026rsquo;s memory before it gets killed.\nAvoid ORDER BY without LIMIT on large result sets. Sorting requires materializing the full result in memory. If you just need the top N rows, always add LIMIT. Final Thoughts Most Trino performance wins come from reading less data: prune partitions, select fewer columns, filter early. After that, getting join strategies right and profiling with EXPLAIN ANALYZE covers the majority of remaining issues. None of these tips are exotic \u0026ndash; they are fundamentals that compound. Getting them right consistently is what separates a query that finishes in 3 seconds from one that times out.\n","permalink":"https://NhoSW.github.io/my-tech-blog/posts/trino-query-optimization-tips/","summary":"Practical tips for optimizing Trino queries in production. Covers partition pruning, join strategies, data types, and query profiling techniques.","title":"Trino Query Optimization: Practical Tips for Better Performance"},{"content":"Hi, I\u0026rsquo;m Seungwoo Noh I\u0026rsquo;m a data engineer focused on building reliable, scalable data infrastructure and pipelines. I enjoy solving the challenges that come with moving, transforming, and serving data at scale.\nAbout This Blog This blog is a place where I share practical data engineering knowledge \u0026ndash; lessons learned from real-world projects, tutorials, architecture decisions, and tips that I hope will be useful to fellow engineers working in the data space.\nTech Stack Here are the core technologies I work with on a daily basis:\nStream Processing: Apache Kafka Workflow Orchestration: Apache Airflow Query Engines: Trino, StarRocks Container Orchestration: Kubernetes Languages: Python, SQL Infrastructure: Docker Contact GitHub: github.com/your-username LinkedIn: linkedin.com/in/your-profile Email: your-email@example.com Feel free to reach out if you have questions, suggestions, or just want to connect!\n","permalink":"https://NhoSW.github.io/my-tech-blog/about/","summary":"About me and this blog","title":"About"},{"content":"안녕하세요, 노승우입니다 저는 안정적이고 확장 가능한 데이터 인프라와 파이프라인을 구축하는 데이터 엔지니어입니다. 대규모 데이터의 수집, 변환, 서빙 과정에서 발생하는 다양한 문제를 해결하는 일에 보람을 느끼고 있습니다.\n블로그 소개 이 블로그는 실무에서 얻은 데이터 엔지니어링 지식을 공유하는 공간입니다. 실제 프로젝트에서 배운 교훈, 튜토리얼, 아키텍처 관련 의사결정, 그리고 데이터 분야에서 일하는 엔지니어분들께 도움이 될 만한 팁들을 다루고 있습니다.\n기술 스택 현재 주로 사용하고 있는 기술들입니다:\n스트림 처리: Apache Kafka 워크플로 오케스트레이션: Apache Airflow 쿼리 엔진: Trino, StarRocks 컨테이너 오케스트레이션: Kubernetes 언어: Python, SQL 인프라: Docker 연락처 GitHub: github.com/your-username LinkedIn: linkedin.com/in/your-profile 이메일: your-email@example.com 궁금한 점이나 제안 사항이 있으시면 편하게 연락 주세요!\n","permalink":"https://NhoSW.github.io/my-tech-blog/ko/about/","summary":"블로그 소개","title":"소개"}]