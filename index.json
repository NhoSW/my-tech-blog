[{"content":"Joe Reis published his 2026 data engineering trends based on a survey of 1,101 data practitioners. As someone leading a data engineering team at a large-scale platform, I wanted to contrast these trends against our team\u0026rsquo;s current architecture — taking an honest look at what we\u0026rsquo;re already doing well and what we still need to tackle.\nOur Architecture in a Nutshell # We run a hybrid architecture centered on S3 as the data lake, with Apache Iceberg as the table format, Trino for batch/ad-hoc analytics, and StarRocks for real-time OLAP. Data ingestion relies on Kafka + Debezium CDC and Flink streaming, while orchestration runs on a heavily customized Airflow setup.\n[Services] → Kafka + Debezium CDC → Flink → S3 (Iceberg) ↓ ┌─────┴─────┐ │ │ Trino StarRocks (Batch/Ad-hoc) (Real-time OLAP) │ │ └─────┬─────┘ ↓ Dashboard 1. AI Adoption — What We\u0026rsquo;re Already Doing and the Walls We Need to Climb # Trend Summary # 82% of survey respondents use AI daily, yet 64% are still stuck at the experimentation stage or using it only for simple tasks. Joe Reis predicts that by the end of 2026, the qualifier \u0026ldquo;AI-assisted\u0026rdquo; will disappear from job descriptions entirely.\nWhat We\u0026rsquo;re Doing # AI coding tools are already part of our daily pipeline development workflow. We actively use LLMs for SQL optimization, code review, and troubleshooting, and we\u0026rsquo;re experimenting with natural-language-based data exploration linked to our data catalog.\nWhat We Need to Do # The challenge is moving beyond individual AI usage to embedding AI into the team\u0026rsquo;s entire workflow.\nData pipeline anomaly detection Automated schema evolution handling Auto-generation of data quality rules To be counted among what Joe Reis calls the \u0026ldquo;10% of AI-mature teams,\u0026rdquo; we need a strategy that integrates AI not as a simple helper tool but as a core component of the platform.\n2. The Data Modeling Crisis and the Semantic Layer — Our Biggest Challenge # Trend Summary # 89% of respondents reported struggling with data modeling, and only 5% of teams are using semantic models. Joe Reis expects the semantic layer to become mainstream first, followed by an evolution toward LLMs interpreting schemas on the fly.\nWhat We\u0026rsquo;re Doing # We manage lineage and metadata through a data catalog and have defined a table layer hierarchy (L1/L2/L3) to manage data quality at multiple levels. We also run automated data validation through custom Airflow operators.\nWhat We Need to Do # We evaluated adopting dbt, but that effort stalled because it got caught up in our next-generation data platform migration strategy. Rather than migrating current pipelines to dbt, we\u0026rsquo;re considering moving them directly to the new platform. In the meantime, however, the standardization and modularization of data transformations remains a gap. This maps exactly to the \u0026ldquo;89% pain\u0026rdquo; that Joe Reis described.\nThe semantic layer is also uncharted territory for us. Business metric definitions vary across teams, and different teams write different SQL for the same KPI. Just as the survey showed 19% demand for semantic model training, the work of raising data literacy across the entire organization is urgent.\nEspecially if we\u0026rsquo;re preparing for a future where AI agents autonomously leverage data, a well-defined semantic layer isn\u0026rsquo;t optional — it\u0026rsquo;s essential. Even if the platform migration is delayed, modeling standards and semantic definitions can — and should — proceed independently.\n3. Orchestration Consolidation — The Future of Airflow # Trend Summary # Airflow remains dominant, but Dagster has grown from the bottom up, capturing 12% share among small companies. It\u0026rsquo;s also surprising that 20% of teams across all company sizes have no orchestration at all.\nWhat We\u0026rsquo;re Doing # We run a deeply customized Airflow setup. We\u0026rsquo;ve developed our own Provider packages and built platform-specific capabilities including automated data validation operators and custom transfer operators. We\u0026rsquo;re currently working on the Airflow 3.x major version upgrade, with a systematic plan covering the Python version upgrade and breaking change migration.\nWhat We Need to Do # Our deep investment in Airflow is a strength, but it\u0026rsquo;s also technical debt.\nMaintenance burden of custom Providers Compatibility issues during version upgrades Preparing for the new paradigm of AI agent orchestration As Joe Reis predicted, we should also keep an eye on the trend of orchestration being absorbed into platforms. Considering alignment with our next-generation data platform, establishing a mid-to-long-term orchestration strategy roadmap is urgent.\n4. Lakehouse vs. Warehouse — An Area Where We\u0026rsquo;ve Already Found Our Answer # Trend Summary # In the survey, 44% use a Warehouse, 27% use a Lakehouse, and 12% use a Hybrid approach. As Snowflake and Databricks converge in functionality, the debate itself is becoming moot. Joe Reis predicts that by the end of 2026, the \u0026ldquo;warehouse vs. lakehouse\u0026rdquo; debate will feel outdated.\nWhat We\u0026rsquo;re Doing # On this trend, our team is already close to the right answer. We adopted Iceberg as the standard open table format on S3 and selectively use Trino and StarRocks depending on the use case. This architecture is neither a Warehouse nor a Lakehouse — it takes the best of both worlds. Through CDC pipelines, we ingest real-time data into Iceberg tables and run both batch and real-time analytics on the same underlying data.\nWhat We Need to Do # To take full advantage of new Iceberg v3 features like Deletion Vectors and Row Lineage, we need to ensure compatibility across our query engines. Since Trino and StarRocks currently have limited Iceberg v3 support, our engine upgrade roadmap and Iceberg version strategy need to be aligned. We also need to strengthen the governance framework for our open table format architecture — catalog integration, access control, and data quality assurance.\n5. Leadership as a Bottleneck — The Hardest and Most Important Challenge # Trend Summary # 22% of data engineers cited \u0026ldquo;lack of leadership direction\u0026rdquo; as a major issue — nearly on par with legacy technical debt at 26%. Joe Reis warns that in 2026, more data teams will be dissolved or merged into engineering organizations.\nWhat We\u0026rsquo;re Doing # Our data platform team exists as an independent organization, owning end-to-end responsibility from infrastructure to ingestion, transformation, and analytics environments. We maintain direct communication channels with business teams and gather data requirements firsthand.\nWhat We Need to Do # Technical capability alone can\u0026rsquo;t prove a team\u0026rsquo;s value. As Joe Reis emphasized, \u0026ldquo;Only teams that prove business value will survive.\u0026rdquo;\nA framework for quantitatively measuring and communicating the ROI of the data platform A vision for what role the data platform should play in the AI era Reducing data downtime through data observability Presenting concrete business impact such as pipeline development productivity metrics Summary: What We\u0026rsquo;re Doing Well vs. What We Need to Do # Area Doing Well Need to Do AI Adoption Active use of AI coding tools at the individual level Embed AI into team workflows, operational automation Data Modeling Catalog-based metadata management, layer hierarchy defined Adopt semantic layer, standardize data transformations Orchestration Deep Airflow customization, 3.x upgrade in progress Long-term orchestration strategy, AI agent readiness Lakehouse/Warehouse Iceberg-based hybrid architecture in place Iceberg v3 compatibility, strengthen governance Leadership End-to-end platform team operation Quantify business impact, data observability Final Thoughts # The most striking sentence from Joe Reis\u0026rsquo;s survey was this:\n\u0026ldquo;In 2026, data engineering is less about picking the right tools and more about building the organizational muscle to use them well.\u0026rdquo;\nOur team is ahead of the curve when it comes to the tech stack. An Iceberg-based open data lake, a hybrid architecture spanning real-time and batch, deep Airflow customization — these are levels that many organizations haven\u0026rsquo;t reached yet.\nBut technical advantage alone isn\u0026rsquo;t enough. Standardized data transformations, a semantic layer, data observability, AI-native workflows, and above all, leadership that proves business value — this is where we need to focus in 2026.\nThe debts of the past are accruing interest, and payday is approaching.\nOriginal source: Where Data Engineering Is Heading in 2026 — Joe Reis\n","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/posts/2026-data-engineering-trends/","section":"Posts","summary":"Analyzing 2026 data engineering trends from Joe Reis’s 1,101-respondent survey, contrasted with our team’s current architecture at a large-scale platform. An honest look at what we’re doing well and what we need to work on.","title":"2026 Data Engineering Trends and Where Large-Scale Platforms Stand","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/ai/","section":"Tags","summary":"","title":"Ai","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/airflow/","section":"Tags","summary":"","title":"Airflow","type":"tags"},{"content":"The End of Life for Airflow 2.x is approaching on April 22, 2026. Our team carried out an Airflow 3.x migration in a production environment running hundreds of DAGs. This post is a record of the breaking changes we encountered, the phased upgrade strategy we used, and the practical lessons we learned from migrating at scale.\nWhy Migrate Now? # Key Improvements in Airflow 3.x # Airflow 3.x is not just a major version bump. Fundamental changes have occurred at the architectural level.\nDAG versioning: No more hacking version suffixes onto dag_id or dealing with scheduling confusion when the schedule changes. Native backfill: Backfills that used to depend on the CLI or custom plugins are now supported directly from the web UI. Event/asset-based triggers: Scheduling options now go far beyond simple cron expressions. React-based web UI: The UI has been completely rebuilt from Flask App Builder to React, dramatically improving usability. Architectural Change: The Arrival of the API Server # The most significant architectural change in 3.x is that the API Server has become the sole gateway to the metadata database.\nAirflow 2.x: Webserver ─── MetaDB Worker ────── MetaDB Scheduler ─── MetaDB DAG Code ──── MetaDB (direct access possible) Airflow 3.x: API Server ── MetaDB (sole access path) Webserver ─── API Server Worker ────── API Server Scheduler ─── API Server DAG Code ──── API Server (no direct access) As a result, any pattern where DAG top-level code directly accessed the metadata database will break. This is the single most impactful change in the entire migration.\nPhased Upgrade Strategy # Jumping straight to the latest version is risky. We devised a four-phase strategy.\nPhase 1: Update to the Latest 2.x Version (2.11) (Optional) # This serves as a safety net in case issues arise during the jump to 3.x. Version 2.11 displays deprecation warnings for features that will be removed in 3.x, so you can identify which code needs to be modified ahead of time.\nPhase 2: Update to 3.0.x # If you\u0026rsquo;re on Python 3.9, only 3.0.x is supported, not the latest 3.1.x. Upgrade the Airflow major version first, before upgrading Python.\nPhase 3: Upgrade Python (3.9 → 3.12+) # Airflow 3.1.x does not support Python 3.9. Aim for Python 3.12 or later, but compromise with 3.10 or 3.11 if dependency compatibility issues arise.\nPhase 4: Update to 3.1.x # Finally, upgrade to the latest stable release.\nSequential Rollout Across Environments # DEV → BETA \u0026amp; Personal Environments → STAGE → PROD Proceed to the next environment only after thorough validation in each one. We spent about two weeks validating in DEV and one week in BETA.\nKey Breaking Changes and How to Handle Them # 1. schedule_interval → schedule # This is the most commonly encountered change. Simply pass the same cron expression you used for schedule_interval to schedule instead.\n# Before (Airflow 2.x) DAG( dag_id=\u0026#34;my_dag\u0026#34;, schedule_interval=\u0026#34;5 2 * * *\u0026#34;, ) # After (Airflow 3.x) DAG( dag_id=\u0026#34;my_dag\u0026#34;, schedule=\u0026#34;5 2 * * *\u0026#34;, ) It\u0026rsquo;s a straightforward substitution, but when you have hundreds of DAGs, every single one must be updated without exception. We\u0026rsquo;ll cover how to automate verification in CI later.\n2. Passing Non-Existent Operator Arguments Is No Longer Allowed # In Airflow 3.x, individual tasks now receive serialized DAGs from the metadata database for execution. As a result, the allow_illegal_arguments setting has been removed, and passing an argument not defined on the operator will cause the DAG import itself to fail.\n# Code like this worked silently in 2.x, but raises an error in 3.x MyOperator( task_id=\u0026#34;my_task\u0026#34;, num_partition=10, # Actual argument name is num_partitions (plural) ) TypeError: Invalid arguments were passed to MyOperator (task_id: my_task). Invalid arguments were: **kwargs: {\u0026#39;num_partition\u0026#39;: 10} This change actually serves as an opportunity to catch latent bugs. If a misspelled argument had been silently ignored for a long time, this migration is the moment to fix it.\n3. Deprecated Context/Template Variables Removed # Variables that only showed deprecation warnings in 2.x have been completely removed in 3.x. The most impactful one is execution_date.\nDeprecated Variable Replacement {{ execution_date }} {{ logical_date }} or {{ data_interval_start }} {{ next_execution_date }} {{ data_interval_end }} {{ prev_execution_date_success }} {{ prev_data_interval_start_success }} Both Jinja templates and Python code need to be updated.\n# Jinja templates # Before \u0026#34;SELECT * FROM table WHERE dt = \u0026#39;{{ execution_date }}\u0026#39;\u0026#34; # After \u0026#34;SELECT * FROM table WHERE dt = \u0026#39;{{ logical_date }}\u0026#39;\u0026#34; # Python context # Before execution_date = context[\u0026#34;execution_date\u0026#34;] # After logical_date = context[\u0026#34;logical_date\u0026#34;] 4. Per-Database Operators Unified → SQLExecuteQueryOperator # Individual operators for MySQL, PostgreSQL, Trino, etc. have been consolidated into a single SQLExecuteQueryOperator. Internally, it automatically selects the appropriate Hook based on the connection type.\n# Before (Airflow 2.x) from airflow.providers.mysql.operators.mysql import MySqlOperator MySqlOperator( task_id=\u0026#34;task\u0026#34;, mysql_conn_id=\u0026#34;my_conn\u0026#34;, sql=\u0026#34;SELECT 1\u0026#34; ) # After (Airflow 3.x) from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator SQLExecuteQueryOperator( task_id=\u0026#34;task\u0026#34;, conn_id=\u0026#34;my_conn\u0026#34;, # DB-specific conn_id → unified conn_id sql=\u0026#34;SELECT 1\u0026#34; ) 5. DummyOperator → EmptyOperator # You need to use an import path that works in both 2.x and 3.x.\n# Works only in v2 (errors in 3.x) from airflow.operators.dummy import DummyOperator # Works only in v3 from airflow.providers.standard.operators.empty import EmptyOperator # Compatible with both v2 \u0026amp; v3 (recommended) from airflow.operators.empty import EmptyOperator 6. SimpleHttpOperator → HttpOperator # # Before from airflow.providers.http.operators.http import SimpleHttpOperator # After from airflow.providers.http.operators.http import HttpOperator 7. Connection Getter Methods → Direct Property Access # The Connection class interface has been changed to be more Pythonic.\n# Before conn = BaseHook.get_connection(\u0026#34;my_conn\u0026#34;) password = conn.get_password() host = conn.get_host() # After conn = BaseHook.get_connection(\u0026#34;my_conn\u0026#34;) password = conn.password host = conn.host 8. Other Package Path Changes # # cached_property # Before: from airflow.compat.functools import cached_property # After: from functools import cached_property (Python built-in) # KubernetesPodOperator # Before: from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import ... # After: from airflow.providers.cncf.kubernetes.operators.pod import ... Migration Strategies for Large-Scale DAG Environments # Add v3 Compatibility Checks to Your CI Pipeline # It\u0026rsquo;s impossible to manually verify hundreds of DAGs. We added a CI job that automatically checks v3 compatibility at the MR (Merge Request) stage.\n# .gitlab-ci.yml example airflow-v3-compat-check: stage: test image: apache/airflow:3.0.6-python3.12 script: - pip install -r requirements.txt - python -m py_compile dags/**/*.py - airflow dags list --output table allow_failure: true # Start as warning-only, then switch to required Start with allow_failure: true to get a picture of the current state, then switch to mandatory checks as the migration deadline approaches.\nIntentionally Keep the Migration Hurdle High # This was the most important lesson we learned.\nWe could have applied a blanket compatibility patch to all DAG code. But we deliberately chose to maintain the migration difficulty. The reason was clear:\nA significant number of DAGs were being operated out of inertia but weren\u0026rsquo;t actually in use.\nBy treating the migration as an opportunity, we nudged DAG owners into asking themselves \u0026ldquo;Do we really need this DAG?\u0026rdquo; As a result, a substantial number of unnecessary DAGs were cleaned up, which directly reduced operational overhead.\nThe specific process:\nCompile a list of inactive DAGs and organize them in a shared spreadsheet Notify DAG owners/departments and ask them to confirm whether each DAG is still needed by a deadline Deactivate DAGs with no response by the deadline DAG owners perform v3 compatibility patches themselves Proactively Update Custom Provider Packages # If you maintain in-house custom operators or utilities as a Provider package, the key is to prepare a compatibility layer that absorbs Airflow core\u0026rsquo;s breaking changes ahead of time.\nWe incrementally updated our custom Provider package across four releases:\nv3.0.0: Basic compatibility v3.0.1: Operator argument validation support v3.0.2: Compatibility layer for deprecated context variables v3.0.3: Documentation and minor bug fixes The approach was to minimize changes to user-facing code while handling v2/v3 branching logic internally within the Provider package.\nHelm Chart Updates # If you run Airflow on Kubernetes, the Helm chart needs to be updated as well. This is because the DAG Processor component and the API Server separation introduced in 3.x must be reflected.\nA safe two-step approach is to first verify compatibility with the existing chart version, then update to the latest stable version once things have stabilized.\nFAB Auth Manager Issue # With the full web UI rebuild to React in 3.x, the existing Flask App Builder (FAB)-based Auth Manager was removed from the default package. If you\u0026rsquo;re using a custom Security Manager, you\u0026rsquo;ll need to install the package separately and update your code.\nFailed to import WoowaSecurityManager, using default security manager If you see this error, you need to explicitly install the FAB Auth Manager package and update the import paths.\nClosing Thoughts # Migrating to Airflow 3.x is not a simple version upgrade. It requires extensive work spanning architectural changes (API Server-centric design), code compatibility updates, and infrastructure modifications.\nHere are the key lessons summarized:\nUpgrade in phases \u0026ndash; Don\u0026rsquo;t jump straight to the latest version. Follow the path: 2.11 → 3.0.x → Python upgrade → 3.1.x. Automate verification in CI \u0026ndash; It\u0026rsquo;s impossible for humans to manually check the compatibility of hundreds of DAGs. Treat the migration as a cleanup opportunity \u0026ndash; Intentionally maintain the migration hurdle to naturally filter out unnecessary DAGs. Proactively update custom Providers \u0026ndash; A compatibility layer that minimizes changes to user code is the key. Don\u0026rsquo;t be complacent just because there\u0026rsquo;s still time until the Airflow 2.x EOL. Migrations in large-scale environments take longer than expected. It\u0026rsquo;s not too late to start now.\nReferences:\nUpgrading to Airflow 3 - Apache Airflow Documentation Apache Airflow 3 is Generally Available! Airflow 3.x Release Notes ","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/posts/airflow-3-migration-guide/","section":"Posts","summary":"Practical lessons from migrating to Airflow 3.x ahead of the 2.x EOL. Covers major breaking changes, a phased upgrade strategy, DAG compatibility approaches, and hard-won lessons from operating hundreds of DAGs in production.","title":"Airflow 3.0 Migration Guide: Lessons from a Large-Scale DAG Environment","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/bali/","section":"Tags","summary":"","title":"Bali","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/compression/","section":"Tags","summary":"","title":"Compression","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/categories/data-engineering/","section":"Categories","summary":"","title":"Data Engineering","type":"categories"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/data-engineering/","section":"Tags","summary":"","title":"Data-Engineering","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/data-pipeline/","section":"Tags","summary":"","title":"Data-Pipeline","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/digital-nomad/","section":"Tags","summary":"","title":"Digital-Nomad","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/family/","section":"Tags","summary":"","title":"Family","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/gpu/","section":"Tags","summary":"","title":"Gpu","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/iceberg/","section":"Tags","summary":"","title":"Iceberg","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/infrastructure/","section":"Tags","summary":"","title":"Infrastructure","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/kafka/","section":"Tags","summary":"","title":"Kafka","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/lakehouse/","section":"Tags","summary":"","title":"Lakehouse","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/categories/life/","section":"Categories","summary":"","title":"Life","type":"categories"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/mig/","section":"Tags","summary":"","title":"Mig","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/migration/","section":"Tags","summary":"","title":"Migration","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/","section":"nanta - Data Engineering","summary":"","title":"nanta - Data Engineering","type":"page"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/nvidia/","section":"Tags","summary":"","title":"Nvidia","type":"tags"},{"content":"GPUs are expensive. A single A100 costs tens of thousands of dollars, yet most workloads don\u0026rsquo;t use all 80GB of memory. Allocating an entire GPU for a simple Jupyter notebook experiment means 70GB sits idle.\nNVIDIA MIG (Multi-Instance GPU) solves this problem. It partitions a single physical GPU into up to 7 independent instances, allowing multiple workloads to run simultaneously. This post documents the process of setting up MIG on a 4× A100 environment and integrating it with Kubernetes.\nWhat Is MIG # MIG splits a single physical GPU into multiple isolated GPU instances. Each instance has its own dedicated memory and compute units, so they don\u0026rsquo;t interfere with each other. An OOM crash in one instance doesn\u0026rsquo;t affect the others.\nHere are the partition options for the A100 80GB:\nProfile Memory SMs Max Instances per GPU 1g.10gb ~10GB 14 7 2g.20gb ~20GB 28 3 3g.40gb ~40GB 42 2 7g.80gb ~80GB 98 1 With 7-way partitioning (1g.10gb), four GPUs yield 28 instances. That\u0026rsquo;s a 4x+ improvement in GPU utilization by simple math.\nChecking GPU Support # MIG is only supported on specific GPUs like the A100 and H100. The A40 and V100 do not support MIG. We initially tried on an A40 server, confirmed it wasn\u0026rsquo;t supported, and switched to A100.\nCheck the MIG M. column in the nvidia-smi output:\n# GPU without MIG support (A40) | MIG M. | | N/A | ← Not supported # GPU with MIG support (A100) | MIG M. | | Disabled | ← Supported but inactive | Enabled | ← Active See the NVIDIA documentation for the full list of supported GPUs.\nSetting Up MIG # Prerequisite: Disable nvidia-mig-manager # Before enabling MIG, you must disable nvidia-mig-manager.service. This daemon resets MIG to disabled on every boot.\nsudo systemctl disable nvidia-mig-manager.service Install mig-parted # nvidia-mig-parted is NVIDIA\u0026rsquo;s MIG configuration tool. You write a declarative config file and it applies the desired partition layout in one shot.\n# Install deb package (Ubuntu/Debian) curl -LO https://github.com/NVIDIA/mig-parted/releases/download/v0.5.2/nvidia-mig-manager_0.5.2-1_amd64.deb sudo dpkg -i nvidia-mig-manager_0.5.2-1_amd64.deb Enable MIG Mode # sudo nvidia-smi -mig 1 This puts MIG mode in a pending state. A reboot is required to apply it.\nsudo reboot VM caveat: Virtual machines using GPU passthrough do not support nvidia-smi --gpu-reset. You cannot enable MIG without a reboot, so bare-metal servers are strongly recommended. We had to migrate from VMs to bare metal because of this issue.\nWriting config.yaml # mig-parted uses a YAML file to declare partition configurations. You can define multiple presets in a single file and switch between them as needed.\nversion: v1 mig-configs: # Disable MIG all-disabled: - devices: all mig-enabled: false # 7-way split on all GPUs (1g.10gb × 7) all-1g.10gb: - devices: all mig-enabled: true mig-devices: \u0026#34;1g.10gb\u0026#34;: 7 # 3-way split on all GPUs (2g.20gb × 3) all-2g.20gb: - devices: all mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 # 2-way split on all GPUs (3g.40gb × 2) all-3g.40gb: - devices: all mig-enabled: true mig-devices: \u0026#34;3g.40gb\u0026#34;: 2 # Full GPU as single instance (7g.80gb × 1) all-7g.80gb: - devices: all mig-enabled: true mig-devices: \u0026#34;7g.80gb\u0026#34;: 1 # Per-GPU custom configuration custom-config: - devices: [0] mig-enabled: true mig-devices: \u0026#34;3g.40gb\u0026#34;: 2 - devices: [1] mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 \u0026#34;1g.10gb\u0026#34;: 1 - devices: [2] mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 \u0026#34;1g.10gb\u0026#34;: 1 - devices: [3] mig-enabled: true mig-devices: \u0026#34;1g.10gb\u0026#34;: 7 The custom-config at the bottom is the key part — it mixes different profiles per GPU. Why this matters is explained later.\nApplying the Configuration # # Apply uniform 7-way split sudo nvidia-mig-parted apply -f ./config.yaml -c all-1g.10gb # Or apply custom config sudo nvidia-mig-parted apply -f ./config.yaml -c custom-config Add the -d flag for debug logs. After applying, verify the instances with nvidia-smi -L:\nGPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-xxxx) MIG 3g.40gb Device 0: (UUID: MIG-xxxx) MIG 3g.40gb Device 1: (UUID: MIG-xxxx) GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-xxxx) MIG 2g.20gb Device 0: (UUID: MIG-xxxx) MIG 2g.20gb Device 1: (UUID: MIG-xxxx) MIG 2g.20gb Device 2: (UUID: MIG-xxxx) MIG 1g.10gb Device 3: (UUID: MIG-xxxx) ... Persisting Across Reboots # MIG configuration resets on reboot. Register a systemd service to reapply it automatically.\n# /etc/systemd/system/nvidia-mig-config.service [Unit] Description=Apply NVIDIA MIG Configuration After=nvidia-persistenced.service [Service] Type=oneshot ExecStart=/usr/bin/nvidia-mig-parted apply -f /home/user/config.yaml -c custom-config RemainAfterExit=yes [Install] WantedBy=multi-user.target sudo systemctl daemon-reload sudo systemctl enable nvidia-mig-config.service Lessons Learned: Why Uniform Partitioning Falls Short # Our first approach was simple: split all 4 GPUs into 1g.10gb × 7 for 28 total instances. Maximum concurrency — what could go wrong?\nLarge Models Don\u0026rsquo;t Fit in MIG Instances # Reality hit quickly. The recommendation team\u0026rsquo;s models needed 30GB+ of GPU memory. A single MIG instance only had 10GB, so models failed to load with CUDA OOM errors.\nLLMs were even worse. Llama 2 7B couldn\u0026rsquo;t load on a MIG device even with 4-bit quantization. The 13B model was out of the question.\nThe conclusion was clear: partition strategy must match the workload.\nPer-GPU Custom Partitioning # After analyzing team workloads, we settled on this configuration:\nGPU Profile Purpose GPU 0 3g.40gb × 2 Mid-size model training (~40GB memory) GPU 1 2g.20gb × 3 + 1g.10gb × 1 Small-to-mid experiments GPU 2 2g.20gb × 3 + 1g.10gb × 1 Small-to-mid experiments GPU 3 1g.10gb × 7 Jupyter notebooks, light batch jobs For large model training, you can also keep some GPUs with MIG disabled entirely. We ended up disabling MIG on some GPUs as LLM experimentation grew.\nNote: There are physical constraints when mixing MIG slices. Memory slices within a GPU are allocated left-to-right and cannot be shared vertically. Always check the supported combinations in the NVIDIA documentation.\nKubernetes Integration # Using MIG instances in Kubernetes requires changes to the NVIDIA device plugin.\nUnderstanding MIG Strategies # The device plugin supports three MIG strategies:\nStrategy Resource Name Description none nvidia.com/gpu MIG-unaware. Allocates whole physical GPUs single nvidia.com/gpu Auto-assigns MIG instances. Existing workloads work as-is mixed nvidia.com/mig-{profile} Exposes each MIG profile as a separate resource The single strategy maps existing nvidia.com/gpu: 1 requests to MIG instances without code changes. Convenient, but you can\u0026rsquo;t control which instance size you get.\nThe mixed strategy lets you request specific profiles like nvidia.com/mig-3g.40gb: 1. This is the right choice when using different partitions per GPU.\nUpdating the Device Plugin # # values.yaml migStrategy: mixed helm upgrade -i nvdp nvdp/nvidia-device-plugin \\ --namespace nvidia-device-plugin \\ --create-namespace \\ --version 0.12.3 \\ -f ./values.yaml After applying, restart the nvidia-device-plugin pods to refresh node resources. Verify with kubectl describe node:\nAllocatable: nvidia.com/mig-1g.10gb: 9 nvidia.com/mig-2g.20gb: 6 nvidia.com/mig-3g.40gb: 2 Using MIG in JupyterHub # Update the JupyterHub profile to request MIG-typed resources:\n# Before (whole GPU allocation) extra_resource_limits: nvidia.com/gpu: \u0026#34;1\u0026#34; # After (MIG instance allocation) extra_resource_limits: nvidia.com/mig-1g.10gb: \u0026#34;1\u0026#34; Be careful: with the mixed strategy, requesting nvidia.com/gpu: 1 allocates an entire physical GPU. You must specify the MIG profile name to get an instance.\nUsing MIG in Airflow KubernetesPodOperator # For GPU tasks in Airflow, update the resource requests to MIG types as well:\nresources = { \u0026#34;cpu\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;16Gi\u0026#34;, \u0026#34;nvidia.com/mig-2g.20gb\u0026#34;: \u0026#34;1\u0026#34;, } KubernetesPodOperator( ... container_resources=k8s.V1ResourceRequirements( requests=resources, limits=resources, ), ... ) Operational Tips # Timing MIG Reconfiguration # Changing MIG configuration requires terminating all processes on the affected GPUs. In production, schedule changes during idle windows. Our batch jobs started at 4 AM, so we ran MIG reconfiguration after 9 PM.\nMIG Must Be Enabled on All GPUs in a Server # You cannot enable MIG on some GPUs while leaving others disabled on the same server. All GPUs must be either MIG-enabled or all MIG-disabled. If you need a non-MIG GPU for large model training, either use a separate server or configure 7g.80gb: 1 in your MIG config to expose the full GPU as a single instance.\nGPU Capacity Changes # MIG dramatically changes your Kubernetes cluster\u0026rsquo;s GPU capacity numbers. In our environment, 4 physical GPUs (capacity: 4) became 20+ instances after MIG. Adjust your monitoring and alerting thresholds accordingly.\nConclusion # MIG is a great way to improve GPU utilization, but it\u0026rsquo;s not a silver bullet. Here\u0026rsquo;s what we learned:\nCheck GPU support first. Only specific models like A100 and H100 support MIG. A40 and V100 do not. Avoid VM environments. GPU passthrough doesn\u0026rsquo;t support GPU reset, making MIG activation painful. Bare metal is the way to go. Start uniform, evolve to custom. Analyze workloads and mix different profiles per GPU for practical results. Large models don\u0026rsquo;t fit in MIG. LLMs and large recommendation models need full GPU memory. Keep some non-MIG GPUs available. GPUs are expensive, but slice them right and you can do a lot more with the hardware you already have.\nReferences:\nNVIDIA MIG User Guide nvidia-mig-parted (GitHub) NVIDIA Device Plugin for Kubernetes Getting the Most Out of the A100 GPU with MIG ","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/posts/nvidia-mig-setup-guide/","section":"Posts","summary":"Four A100 GPUs can yield up to 28 independent GPU instances. This guide covers MIG concepts, mig-parted installation, diverse slice configurations, and Kubernetes integration based on hands-on experience.","title":"NVIDIA MIG Setup Guide: Splitting One A100 GPU Into Many","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/olap/","section":"Tags","summary":"","title":"Olap","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/open-source/","section":"Tags","summary":"","title":"Open-Source","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/orchestration/","section":"Tags","summary":"","title":"Orchestration","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/performance-tuning/","section":"Tags","summary":"","title":"Performance-Tuning","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/real-time/","section":"Tags","summary":"","title":"Real-Time","type":"tags"},{"content":"When you think of Bali, certain images come to mind. Surfing, tropical nature, digital nomads with laptops open in coworking spaces. It\u0026rsquo;s a place often called the holy land of workations.\nI was drawn to that image too. I thought it would give my child a new experience at an international kindergarten, and give my family a sense of escape from the daily routine. In early 2023, I used my company\u0026rsquo;s overseas remote work policy to live and work in Bali for about a month.\nTo cut to the chase: remote work in Bali was more underwhelming than expected for a family with a young child. This post is an honest record of that experience. If you\u0026rsquo;re considering remote work in Bali, especially with a young child, I hope this helps you make a more informed decision.\nBefore Departure: What You Need to Prepare # Approval and Tax Issues # Before booking flights for overseas remote work, you need to get company approval first. The approval process can lead to schedule changes.\nIn my case, I initially applied for a three-month stay, but during the tax review I was told that only stays of two months or less were allowed under tax regulations. This is because tax treatment varies depending on the length of overseas stay. If I had already booked my flights, I would have had to pay change fees. Always purchase your tickets after approval is confirmed.\nI used Korean Air mileage for the flights, and it turned out to be a great deal since the mileage deducted was relatively low compared to the cost.\nAccommodation: The Villa Trap # There are generally two options for accommodation in Bali. You can hop between hotels and resorts, or rent a villa on a monthly basis.\nI stayed at a hotel for about three days, then signed a monthly contract for a Bali-style villa with a swimming pool. It looks stunning in photos. A private pool beneath palm trees, a spacious living room, a tropical garden. The problem is that actually living there is a different story. I ended up canceling the villa and going back to a hotel.\nThe reality of villas:\nBugs everywhere. Ants and mosquitoes are a given, and all sorts of unidentifiable insects show up. Hotels and resorts fumigate, but villas generally don\u0026rsquo;t. Inconsistent bedding and cleanliness. The level of maintenance varies wildly from place to place. Water supply issues. I\u0026rsquo;ll get into this later, but this was the biggest problem. Transportation is inconvenient. There\u0026rsquo;s a reason villas are cheap. Most are located far from the center of town, and walking anywhere in Bali is practically impossible. If you\u0026rsquo;re staying a month or longer, a villa might seem like the rational choice, but especially if you have a child, a hotel or serviced residence is better. In terms of maintenance, hygiene, and convenience, it wins on every front.\nHygiene: Bali\u0026rsquo;s Biggest Risk # There are a few topics that dominate Indonesia travel communities. \u0026ldquo;Which hospital should I go to?\u0026rdquo;, \u0026ldquo;Can someone recommend medicine?\u0026rdquo;, \u0026ldquo;Looking for a showerhead filter.\u0026rdquo;\nThis is not an exaggeration.\nThe Water Problem # Many places in Bali draw from groundwater. Contamination is not uncommon, and even after scrubbing with soap, your skin can feel perpetually slippery. It\u0026rsquo;s a different kind of unpleasantness from the limescale-heavy water in Europe.\nSwimming pools are everywhere in Bali, but many of them don\u0026rsquo;t smell of chlorine. No chlorine smell means they\u0026rsquo;re not being disinfected. In a tropical climate where bacteria thrive.\nThe beaches aren\u0026rsquo;t safe either. Many people get sick after swallowing contaminated seawater that flows in from the land while surfing. You can\u0026rsquo;t even be sure whether the ice in cold drinks at cafes is made from purified water.\nMy Child\u0026rsquo;s Hospitalization # I did my best to prepare before departure. We got typhoid vaccinations, and the day before our flight, I had blood tests done at a general hospital to check inflammation and white blood cell levels. I also started giving my child probiotics two weeks in advance.\nThree days after arrival, my child developed a high fever.\nA child who had never gone above 38°C spiked to 39.5°C. We went to the emergency room three times. At the third ER visit, when I said I wanted to go home, they told me to sign a form stating they wouldn\u0026rsquo;t be held responsible. We ended up being admitted. Four days in the hospital.\nLooking back, this experience was truly harrowing.\nA fundamental distrust of Indonesia\u0026rsquo;s outdated medical facilities. Communication was also difficult. While my child was in the ER, it started pouring rain and the ceiling collapsed. There was a thunderous crash, the ceiling light fixtures fell, and we grabbed our child and ran out moments before the ceiling caved in. This happened at what was supposedly the best hospital in the area. About 500,000 KRW per ER visit including tests. Total medical expenses: over 3 million KRW. Travel insurance is absolutely essential. Make sure to carefully check the coverage details and choose a plan with a sufficient medical expense limit. And honestly, after this experience, the thought \u0026ldquo;What are we even doing here?\u0026rdquo; wouldn\u0026rsquo;t leave my mind.\nDaily Life with a Child: Expectations vs. Reality # There\u0026rsquo;s Nothing to Do # Think about what you do with your child on a typical day in Korea. Reading books, taking walks, going to playgrounds, visiting kids\u0026rsquo; cafes, playing with toys, weekend camping. In Bali, you can do almost none of these things.\nNo books. Unless you pack a suitcase full of Korean picture books, you have nothing to read to your child. Walking is impossible. Bali has almost no sidewalks. The saying \u0026ldquo;if you want to go more than three steps, you need to call a motorbike\u0026rdquo; is barely an exaggeration. Walking under the blazing sun through exhaust fumes, you live in constant fear of getting your feet run over by a motorbike. Taking a walk with a child is unthinkable. Almost no playgrounds. It\u0026rsquo;s not like Korea where every apartment complex has a playground. It doesn\u0026rsquo;t seem to be a legally required facility either. You occasionally spot one here and there, but the quality is dramatically lower than what you\u0026rsquo;d find in Korea. No indoor play environment. At home you have familiar toys, but here there are none. In the end, you find yourself letting them watch YouTube, something you never allowed back home. That\u0026rsquo;s the reality.\nThe Kindergarten I Had Hoped For # Because Bali has a large Australian expat community, there are well-established English-language kindergartens. I had high expectations about giving my child an immersive English-language experience.\nBut I overlooked my child\u0026rsquo;s personality. Our child is introverted and doesn\u0026rsquo;t take the initiative to approach others. In an environment where most of the other kids speak English, I had to watch my child play alone for two weeks. We eventually decided to stop sending them. The bigger the expectations, the bigger the disappointment.\nThe experience can vary entirely depending on your child\u0026rsquo;s temperament. If your child is sociable and adapts quickly, it could be a great experience. But not every child is like that. You need to honestly assess your child\u0026rsquo;s personality.\nMy Daily Life: It\u0026rsquo;s a Workation, Not a Vacation # Extra Time Doesn\u0026rsquo;t Magically Appear # Let me describe my routine back in Korea. I\u0026rsquo;m not a particularly early riser. I wake up, feed my child, send them to kindergarten, work, and after work, play with my child. Once the child falls asleep, I have maybe one hour, two at most. I watch YouTube, catch up on work, or exercise.\nBeing in Bali doesn\u0026rsquo;t magically create time that didn\u0026rsquo;t exist. A person who has one hour of free time a day doesn\u0026rsquo;t suddenly get three hours just by changing locations. If anything, my commute got longer.\nInternet: The VPN Wall # Working from home was effectively impossible. The internet was slow, power outages were frequent, and the connection dropped constantly.\nCoworking spaces are a different story. Good ones have UPS systems and their own generators, and they contract with multiple ISPs so that if one goes down, another line keeps things running. Most of them met the minimum internet speed my company required.\nThe problem is VPN. Company security policy required me to be on VPN while working, and the moment I turned it on, internet speed dropped to about 10-20% of normal. Across all of Bali, only a handful of coworking spaces let you work comfortably with VPN on. Those places charge around 20,000 KRW a day, and they\u0026rsquo;re not close to where you\u0026rsquo;re staying either.\nThe Reality of Evenings # After work, nighttime. Surfing or outdoor activities are out of the question. It\u0026rsquo;s not like Europe where street musicians perform. Realistically, what you can do is go to a decent restaurant or bar, eat something good, and have a drink.\nBut even that isn\u0026rsquo;t as easy as it sounds.\nThe drinking culture is underdeveloped. Perhaps because of the Islamic background, craft beer is limited to two or three local breweries, and there\u0026rsquo;s no imported craft beer at all. Spirits are expensive, and wines are imported with limited selection and high prices. The decent places are as expensive as Korea. Drinks are charged separately, and taxes add 15-20%. Eating at decent places every day puts serious pressure on your bank account. If you stick to the cheap places, you start wondering why you came all the way to another country just to suffer like this. Two weeks after arriving, I watched my bank balance plummet.\nThe Things That Were Good # It feels like I\u0026rsquo;ve only been negative, so let me honestly share what was good too.\nCoworking Spaces # The coworking space culture in Bali is genuinely impressive. Part internet cafe, part library, part coffee shop. Just exploring them was a fun experience in itself, checking out the vibe of this one, discovering what that one had on the menu. Working alongside digital nomads from around the world in the same space wasn\u0026rsquo;t bad at all.\nBeach Clubs # Pool behind you, ocean in front. Music and delicious food. Bali\u0026rsquo;s beach clubs are truly a unique experience. Just watching young people enjoy themselves was energizing.\nWeekends # On weekends, I finally felt like I had actually come to Bali for a holiday. We went on tours, visited beach clubs, went surfing. The charm of Bali that was invisible during the week hit all at once on weekends. Paradoxically, this was the precise meaning of the word \u0026ldquo;workation.\u0026rdquo; Work and vacation don\u0026rsquo;t come simultaneously; they alternate.\nSummary: Who Is Bali Remote Work Right For? # Summing up a month of experience, satisfaction with remote work in Bali splits sharply depending on your lifestyle.\nCondition Satisfaction Reason Single or couple (no kids) High Activities, free time, flexible schedule Family with children Low Hygiene risks, nothing to do, no time Work that doesn\u0026rsquo;t require VPN High Freedom to use any coworking space Work that requires VPN Moderate Limited coworking space options Generous budget High Good accommodation + good food = good experience Tight budget Low Exhaustion from hopping between cheap places Checklist Before You Go # If you\u0026rsquo;re a family with children and still want to go:\nTravel insurance: Check the medical expense coverage limit. A single ER visit can cost 500,000 KRW. Typhoid vaccination: Get vaccinated at least two weeks before departure. Accommodation: Hotels or serviced residences over villas. The difference in hygiene and maintenance is real. VPN testing: Research coworking spaces in advance where you can work with your company VPN on. Supplies for kids: Korean books, toys, offline content downloaded to a tablet. Kindergarten: Honestly assess your child\u0026rsquo;s temperament. An English-language kindergarten can be stressful for an introverted child. Budget: Plan for your usual living expenses plus an extra 30-50%. The idea that Bali is cheap is based on local food and local accommodation. To maintain a quality of life that a Korean would find satisfactory, it can cost the same as or more than Korea. Closing Thoughts # Looking back after returning, the biggest takeaway from remote work in Bali is this:\nChanging your location doesn\u0026rsquo;t change your life. A person with one hour of free time a day still has one hour in Bali. A parent who needs to care for their child still needs to care for their child in Bali. On top of that, add hygiene risks, medical anxiety, and infrastructure inconveniences.\nThat said, I don\u0026rsquo;t regret this experience. If I hadn\u0026rsquo;t done it, I would have stayed curious, harboring a vague fantasy about Bali. There was value in simply confirming the fantasy against reality.\nHowever, if I were to do overseas remote work again under the same conditions (with a young child), it wouldn\u0026rsquo;t be Bali. A city with solid medical infrastructure, where you can take walks with your child. A place with sidewalks, playgrounds, and trustworthy tap water. A workation in a city where the basics of daily life are in place is a true workation.\n","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/posts/bali-remote-work-with-family/","section":"Posts","summary":"Surfing, nature, the mecca of digital nomads. I took my child and worked remotely from Bali for a month. This is an honest record of the gap between expectations and reality.","title":"Remote Work in Bali with a Kid: An Honest Month-Long Review","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/remote-work/","section":"Tags","summary":"","title":"Remote-Work","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/starburst/","section":"Tags","summary":"","title":"Starburst","type":"tags"},{"content":"If you run Trino as a production query engine, you\u0026rsquo;ve probably felt it throughout 2025. Releases have been getting sparse. This isn\u0026rsquo;t just a vague feeling — the numbers tell the story. Trino open-source releases dropped from 30 in 2024 to just 11 in 2025. A 63% decline.\nIn this post, I analyze why and how Starburst pivoted to become an AI-centric platform company, and lay out how Trino open-source users should think about this shift.\nWhat Happened to Trino Releases # A Sharp Drop in Release Frequency # Looking at Trino open-source releases by quarter, the downward trend is unmistakable.\nPeriod Releases Avg. Cadence Notes 2024 Q4 9 Weekly Stable pattern 2025 Q1 6 Biweekly Decline begins 2025 Q2 2 Monthly Sharp drop 2025 Q3 1 Quarterly All-time low 2025 Q4 2 Every 1.5 months Still sluggish Up through 2024 Q4, a new release came out every week. But starting in 2025 Q2, the cadence dropped to roughly once a month, and in Q3 there was only a single release for the entire quarter. Starburst Enterprise releases were similarly scarce.\nThe numbers look alarming, but this doesn\u0026rsquo;t signal the \u0026ldquo;decline\u0026rdquo; of Trino. It\u0026rsquo;s a strategic choice by Starburst.\nWhy the Drop # The answer becomes clear when you look at Starburst\u0026rsquo;s contribution to Trino open source. In 2024, the Starburst team accounted for 84% of all Trino commits. 138 contributors, 2,822 commits, and over 50 companies participated — but the real development engine was Starburst. And that engine started redirecting its engineering resources elsewhere.\nThat \u0026ldquo;elsewhere\u0026rdquo; is AI.\nStarburst\u0026rsquo;s AI Pivot # The Shift in Positioning # Tracking the evolution of Starburst\u0026rsquo;s official messaging reveals just how deliberate this pivot was.\nPeriod Positioning Core Message ~2023 Open Data Lakehouse Company Distributed query engine built on Trino 2024 Data Lake Analytics Platform Federated queries + Iceberg 2025 Data Platform for Apps and AI AI Agent + Agentic Workforce From \u0026ldquo;Open Data Lakehouse\u0026rdquo; to \u0026ldquo;Data Platform for Apps and AI.\u0026rdquo; This wasn\u0026rsquo;t just a marketing rebrand — the entire product roadmap was realigned in this direction.\n2025 Key Announcements Timeline # May 2025 — Launch Point\nStarburst officially announced AI Agent and AI Workflows.\nAI Agent: A natural-language interface for querying data. A question like \u0026ldquo;What were our sales in Europe last quarter?\u0026rdquo; is automatically converted to SQL. Air-gapped environments (finance, healthcare, government) are explicitly supported, along with Google\u0026rsquo;s Agent2Agent protocol and Anthropic\u0026rsquo;s Model Context Protocol (MCP). AI Workflows: A pipeline for storing vector embeddings in Iceberg tables and leveraging structured, semi-structured, and unstructured data for AI training. RAG (Retrieval-Augmented Generation) is natively supported. Other: Starburst Data Catalog (replacing Hive Metastore), Automated Table Maintenance (automated file cleanup and compaction), Native ODBC Driver, Role-based Query Routing October 2025 — AI \u0026amp; Datanova 2025\nHere, Starburst went a step further by announcing the Agentic Workforce platform and Lakeside AI Architecture.\nThe core concept is Model-to-Data architecture. Whereas the traditional approach collects data into a centralized warehouse and then runs AI models against it, Starburst proposes sending the AI model to where the data lives.\nTraditional: Data → Centralized Warehouse → AI Model Starburst: AI Model → Federated Data (where it lives) The logic is that by not moving data, you can maintain data sovereignty (GDPR, Schrems II) while still enabling unified analytics. Global financial institutions like Citi and HSBC are reportedly using this approach to unify data across 165 countries.\nThe Shift in the Tech Stack # Layering Starburst\u0026rsquo;s tech stack makes it clear what was added in 2025.\n┌─────────────────────────────────────┐ │ AI Agent \u0026amp; Agent2Agent Protocol │ ← 2025 NEW ├─────────────────────────────────────┤ │ AI Workflows (Vector Store) │ ← 2025 NEW ├─────────────────────────────────────┤ │ Starburst Data Catalog │ ← 2025 NEW ├─────────────────────────────────────┤ │ Lakehouse (Trino + Iceberg) │ ├─────────────────────────────────────┤ │ Federated Data Sources (50+) │ └─────────────────────────────────────┘ Trino still sits in the foundational layer, but all the innovation is happening above it. This is precisely why Trino open-source releases have slowed. Engineering resources have shifted to the upper layers.\nVector Store on Iceberg: A Technical Innovation Worth Watching # Among Starburst\u0026rsquo;s 2025 announcements, the most technically interesting is the approach of storing vector embeddings directly in Apache Iceberg tables.\nHere\u0026rsquo;s why this matters:\nNo separate vector DB required. You no longer need to operate dedicated vector databases like Pinecone, Weaviate, or Milvus. Leverages existing data engineering skills. You can manage vector data the same way you already manage Iceberg tables. Iceberg\u0026rsquo;s advantages extend to vector data. Time travel, ACID transactions, schema evolution, partitioning — all applicable to vector data. Governance policies apply consistently. The same access controls, audit logs, and data masking policies can be applied to both structured data and vector data. Open format means no vendor lock-in. Assuming a future where AI workloads become a core requirement of data platforms, this approach is remarkably pragmatic. There may be trade-offs in search performance compared to dedicated vector DBs, but the reduction in operational complexity and unified governance make this attractive for enterprise environments.\nBusiness Results: Is the Strategy Working in the Market? # The business metrics prove that the AI pivot isn\u0026rsquo;t just marketing.\nFY25 Results (announced February 2025):\nMetric Result New customers 20% YoY increase Galaxy (SaaS) customers 76% YoY increase Galaxy usage 94% YoY increase Largest deal Eight-figure multi-year contract with a global financial institution Partnerships Selected as the query engine for Dell Data Lakehouse The 76% growth in Galaxy (SaaS) customers is particularly noteworthy. It signals an accelerating shift toward cloud managed services — which, incidentally, is an alternative for teams currently self-managing open-source Trino.\nThe customer roster is equally impressive:\nHSBC: Data integration across 165 countries Citi: Unified analytics while maintaining global data sovereignty Vectra AI: Threat detection platform across 120 countries ZoomInfo: Multi-cloud data integration Competitive Positioning # Comparing Starburst\u0026rsquo;s position against competitors in the data platform market sharpens its points of differentiation.\nCapability Databricks Snowflake Dremio Starburst AI Agent O O X O Federated Query Limited Limited O Core strength Data Sovereignty Limited Limited Limited Core strength Open-source foundation Spark X Arrow Trino Vector Store O O X O (Iceberg) On-prem + Cloud O Limited O Core strength Starburst\u0026rsquo;s core differentiators boil down to three things:\nTrue federation: Real-time queries across 50+ data sources. Analyze data in place without moving it. Data sovereignty: Unified analytics while complying with regulations across 165 countries. A decisive advantage in GDPR and Schrems II environments. Hybrid deployment: Simultaneous support for on-premise and multi-cloud. Critical value for regulated industries where cloud migration is slow. On the other hand, Databricks is an AI/ML Lakehouse centered on Spark + Delta Lake, strengthening governance through Unity Catalog. Snowflake added Iceberg support in 2024 and is pushing AI/ML through Snowpark, but its model still assumes data centralization. Dremio emphasizes Arrow Flight-based performance and a semantic layer, but still lags in enterprise features.\nAn interesting remark from Starburst CEO Justin Borgman: \u0026ldquo;What they\u0026rsquo;ve done for Spark is what we aim to do for Presto (Trino).\u0026rdquo; Just as Databricks built a powerful commercial platform on top of the Spark open-source project, Starburst aims to build the same structure on top of Trino.\nIs Trino Open Source Going to Be Okay? # The Split Between Open-Source and Commercial Features # Here\u0026rsquo;s what currently remains in Trino open source versus what has moved to Starburst-only.\nTrino Open Source:\nCore query engine Standard connectors Fault-tolerant execution SQL MERGE Basic security features Starburst Only:\nWarp Speed (up to 7x performance improvement) AI Agent \u0026amp; AI Workflows Starburst Data Catalog Advanced governance (RBAC, data masking, audit logs) Automated Table Maintenance Smart Indexing Materialized Views (partial) The most notable item is Warp Speed. The fact that a proprietary indexing/caching layer delivering up to 7x performance improvement is commercial-only means that the performance gap between open source and the commercial product could widen for large-scale workloads.\nReasons for Optimism # The Trino core engine has reached maturity. As a distributed SQL query engine, it has most of the features it needs. Fewer releases doesn\u0026rsquo;t mean lower quality. The community is still active. Trino Summit 2024 was a success with participation from Netflix, LinkedIn, Wise, and others. The Trino Community Broadcast continues to run. Slack and GitHub activity remains healthy. Over 50 companies are contributing. Even if Starburst\u0026rsquo;s contributions decline, there\u0026rsquo;s room for other companies to pick up the slack. Reasons for Concern # The company responsible for 84% of commits has started focusing elsewhere. Whether other companies have sufficient incentive to fill this gap is an open question. The key to performance optimization is commercial-only. Teams running large-scale workloads without Warp Speed may find themselves at an increasing disadvantage. All AI-related innovation is concentrated in the commercial product. In a future where AI becomes essential to data platforms, competing on open source alone may become difficult. What Trino Operations Teams Should Consider # For teams running Trino in production, it helps to think about this situation across two time horizons.\nShort Term (1-2 years): No Major Concerns # Trino open source is still stable and battle-tested in production. Core features are sufficiently mature, and the baseline query performance and connector ecosystem are solid. There\u0026rsquo;s no reason to switch to an alternative right now.\nMid to Long Term (3-5 years): Strategic Preparation Is Needed # You need to account for the possibility that the feature gap between open source and the commercial product will widen. Preparation is needed in the following areas:\nPerformance optimization: How will you maintain performance for large-scale workloads without Warp Speed? Consider your own caching layers, indexing strategies, or expanding the role of complementary engines like StarRocks. AI integration: When AI integration becomes an organizational requirement for the data platform, evaluate whether open-source Trino alone is sufficient. Can you implement approaches like Vector Store on Iceberg on your own, or do you need to combine other tools? Governance: As the organization grows and regulations tighten, the need for advanced governance features (RBAC, data masking, audit logs) will increase. Can open source alone meet these requirements? Alternative evaluation: Periodically evaluate adopting Starburst Galaxy, switching to other query engines, or hybrid approaches (Trino for batch, StarRocks for real-time). Final Thoughts # Starburst\u0026rsquo;s AI pivot isn\u0026rsquo;t just a marketing play. The business metrics — 76% growth in Galaxy customers, the largest contract in company history — prove that this strategy is working in the market. The transition from a query engine company to an AI platform company is irreversible.\nTrino open source isn\u0026rsquo;t dying anytime soon. But it\u0026rsquo;s approaching a maintenance-mode state of being \u0026ldquo;sufficiently mature,\u0026rdquo; and the center of gravity for innovation has clearly shifted to the commercial product. This is the same pattern Databricks followed with Spark.\nIf you\u0026rsquo;re running Trino in production, you can rest easy for now, but preparation for three years from now should start today. The prudent approach is to lean on open source\u0026rsquo;s stability while securing strategic options along two axes: the performance gap and AI integration.\nTechnical debt always accumulates quietly. And the interest is always more expensive than you thought.\nReferences:\nTrino Release Notes Starburst Enterprise Release Notes TechTarget: Addition of new AI capabilities shows Starburst\u0026rsquo;s growth BigDataWire: Starburst\u0026rsquo;s New Platform Aims to Close AI\u0026rsquo;s Biggest Gap ","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/posts/starburst-trino-ai-pivot/","section":"Posts","summary":"Trino open-source releases dropped 63% as Starburst pivoted from a query engine company to an AI platform. From the perspective of a team running Trino in production, here’s what this shift means and what to do about it.","title":"Starburst's AI Pivot: Is Trino Open Source Going to Be Okay?","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/starrocks/","section":"Tags","summary":"","title":"Starrocks","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/categories/starrocks/","section":"Categories","summary":"","title":"StarRocks","type":"categories"},{"content":" Background # If you run data pipelines long enough, you inevitably face one question: How do you build real-time dashboards?\nOur team was no different. Our existing pipeline looked like this:\nService → Kafka → Iceberg → S3 → Trino → Airflow(5min) → Dashboard On the surface it worked fine, but the pain points in practice were clear:\nAt least 5 minutes of latency: The Airflow schedule interval was the bottleneck Pipeline complexity: Managing 5+ components chained as Kafka → Flink → Redis → API → Dashboard Redundant I/O: Trino full-scanned S3 on every query High development cost: Each new real-time dashboard took roughly 2 weeks to build After adopting StarRocks, the architecture simplified to this:\nService → Kafka → StarRocks → Dashboard (sub-second latency) Eliminating the intermediate components dramatically simplified the pipeline, and ingesting data directly from Kafka into StarRocks gave us the real-time capability we needed.\nResults # After approximately 3 months of PoC and 6 months of phased rollout, we achieved the following improvements:\nMetric Before After Improvement Dashboard latency 5 min \u0026lt; 1 sec ~300x Dashboard dev time ~2 weeks ~1 week 50% reduction Pipeline components 5+ 2 60% reduction Query response time 30~50 sec 5~10 sec 5~10x Hardware cost 128 GB x 18 nodes 64 GB x 3 nodes ~75% savings Trino is fast in terms of raw query time, but when factoring in Airflow schedule delays and end-to-end latency, along with hardware cost efficiency, StarRocks proved to be a better fit for real-time workloads.\nTable Model Selection Guide # Choosing the right table model is the most important decision when first adopting StarRocks. A wrong choice means you will have to recreate the table later.\nDecision Flow # ┌─────────────────────────────┐ │ What data are you storing? │ └──────────────┬──────────────┘ │ ┌───────▼────────┐ │ Need UPDATE? │ └───────┬────────┘ │ ┌────────┴────────┐ │ │ [No] [Yes] │ │ ┌─────▼─────┐ ┌─────▼──────┐ │ Need │ │ Primary Key │ │aggregation?│ │ (frequent │ └─────┬─────┘ │ UPDATE) │ │ └────────────┘ [No] [Yes] │ │ ┌─────▼───┐ ┌▼──────────┐ │Duplicate│ │Aggregate │ │(raw data)│ │(auto-agg) ★│ └─────────┘ └────────────┘ Model Comparison # Model Duplicates Allowed UPDATE Auto-aggregation Best For Duplicate Key O X X Logs, raw events Aggregate Key X Auto O Real-time statistics ★ Primary Key X O (fast) X Frequent UPDATEs Duplicate Key: Storing Raw Data # Use this when you need to preserve original data as-is, such as click logs, API events, or sensor data.\nCREATE TABLE order_events ( event_id BIGINT, event_time DATETIME, order_id VARCHAR(50), user_id BIGINT, event_type VARCHAR(20), amount DECIMAL(10, 2) ) DUPLICATE KEY(event_id, event_time) PARTITION BY date_trunc(\u0026#39;day\u0026#39;, event_time) DISTRIBUTED BY HASH(event_id) BUCKETS 10; Aggregate Key: Real-time Statistics ★ # Data is automatically aggregated at ingestion time. This model was the key reason for adopting StarRocks.\nCREATE TABLE order_stats ( stat_time DATETIME NOT NULL COMMENT \u0026#39;5-minute intervals\u0026#39;, region VARCHAR(20) NOT NULL, delivery_type VARCHAR(20) NOT NULL, -- Aggregate columns: aggregate functions applied automatically at ingestion order_count BIGINT SUM DEFAULT \u0026#34;0\u0026#34;, total_amount DECIMAL(15, 2) SUM DEFAULT \u0026#34;0\u0026#34;, user_bitmap BITMAP BITMAP_UNION, max_amount DECIMAL(10, 2) MAX ) AGGREGATE KEY(stat_time, region, delivery_type) PARTITION BY date_trunc(\u0026#39;day\u0026#39;, stat_time) DISTRIBUTED BY HASH(stat_time) BUCKETS 10; Available aggregate functions:\nFunction Purpose Example SUM Summation Order count, total revenue MAX / MIN Maximum / minimum value Highest price, lowest price REPLACE Overwrite with latest value Latest status BITMAP_UNION Exact unique count Unique visitors HLL_UNION Approximate unique count High-cardinality sets Unlike HyperLogLog, BITMAP_UNION provides exact unique counts. For business KPI dashboards where accuracy matters, always use this approach.\nPrimary Key: Frequent UPDATEs # Best suited for scenarios where the same key is frequently updated, such as order status tracking or inventory management.\nCREATE TABLE orders ( order_id VARCHAR(50) NOT NULL, status VARCHAR(20), amount DECIMAL(10, 2), updated_at DATETIME ) PRIMARY KEY(order_id) DISTRIBUTED BY HASH(order_id) BUCKETS 10 PROPERTIES ( \u0026#34;enable_persistent_index\u0026#34; = \u0026#34;true\u0026#34; ); Enabling enable_persistent_index significantly improves UPDATE performance.\nData Ingestion # Routine Load: Real-time Kafka Integration # This approach continuously ingests data from a Kafka topic. It is the method used in most real-time pipelines.\nCREATE ROUTINE LOAD order_load ON orders COLUMNS( order_id, user_id, timestamp_ms, amount, order_date = FROM_UNIXTIME(timestamp_ms / 1000) ) PROPERTIES ( \u0026#34;format\u0026#34; = \u0026#34;json\u0026#34;, \u0026#34;jsonpaths\u0026#34; = \u0026#34;[\\\u0026#34;$.orderId\\\u0026#34;,\\\u0026#34;$.userId\\\u0026#34;,\\\u0026#34;$.timestamp\\\u0026#34;,\\\u0026#34;$.amount\\\u0026#34;]\u0026#34; ) FROM KAFKA ( \u0026#34;kafka_broker_list\u0026#34; = \u0026#34;kafka-broker:9092\u0026#34;, \u0026#34;kafka_topic\u0026#34; = \u0026#34;orders\u0026#34; ); When combined with an Aggregate Key table, transformation and aggregation happen simultaneously at ingestion time.\nCREATE ROUTINE LOAD order_stats_load ON order_stats COLUMNS( timestamp_ms, region, amount, user_id, -- Round to 5-minute intervals stat_time = FROM_UNIXTIME(FLOOR(timestamp_ms / 1000 / 300) * 300), order_count = 1, total_amount = amount, user_bitmap = BITMAP_HASH(user_id) ) WHERE amount \u0026gt; 0 PROPERTIES (\u0026#34;format\u0026#34; = \u0026#34;json\u0026#34;) FROM KAFKA ( \u0026#34;kafka_broker_list\u0026#34; = \u0026#34;kafka-broker:9092\u0026#34;, \u0026#34;kafka_topic\u0026#34; = \u0026#34;orders\u0026#34; ); This single pattern replaced the aggregation logic that previously required Flink \u0026ndash; using nothing but SQL.\nStream Load: Bulk Data Loading # Ideal for one-time bulk loads via files or APIs.\n# CSV file loading curl --location-trusted \\ -u user:password \\ -H \u0026#34;label:load_$(date +%Y%m%d%H%M%S)\u0026#34; \\ -H \u0026#34;column_separator:,\u0026#34; \\ -T data.csv \\ http://starrocks-fe:8030/api/mydb/mytable/_stream_load Performance Tuning Tips # Thread Pool Configuration # In high-load environments with 500+ RPS of concurrent connections, the default thread pool size is insufficient.\n# be.conf pipeline_scan_thread_pool_thread_num = 32 # default: 24 pipeline_exec_thread_pool_thread_num = 32 # default: 24 Bucket Count Guidelines # Data Size Recommended Buckets \u0026lt; 10 GB 10 10~50 GB 20 50~100 GB 30 \u0026gt; 100 GB 50+ Formula: buckets = max(1, data_size_GB / 10)\nPartitioning Strategy # Applying functions to partition columns prevents partition pruning. This is a more common mistake than you might think.\n-- ✅ Correct: partition pruning works WHERE event_time \u0026gt;= NOW() - INTERVAL 3 DAY -- ❌ Incorrect: partition pruning disabled WHERE DATE(event_time) \u0026gt;= CURRENT_DATE - 3 TTL Configuration # To automatically drop old partitions, configure TTL.\nPROPERTIES ( \u0026#34;partition_live_number\u0026#34; = \u0026#34;3\u0026#34; -- Keep only the 3 most recent partitions ) Operational Know-how # Materialized View Management # ASYNC refresh can stop without warning. Periodically check the status and manually recover when issues arise.\n-- Check status SHOW MATERIALIZED VIEWS; -- Force synchronous refresh REFRESH MATERIALIZED VIEW db.mv_name WITH SYNC MODE; -- Reactivate a deactivated MV ALTER MATERIALIZED VIEW db.mv_name ACTIVE; Routine Load Monitoring # The status frequently transitions to PAUSED. Common causes include Kafka offset issues or malformed messages.\n-- Check status SHOW ROUTINE LOAD FOR db.load_job; -- Resume RESUME ROUTINE LOAD FOR db.load_job; Scale-in Precautions # When scaling down nodes, you must perform a Decommission first. Removing nodes without this procedure will result in data loss.\n-- 1. Check current nodes SHOW PROC \u0026#39;/backends\u0026#39;; -- 2. Start decommission ALTER SYSTEM DECOMMISSION BACKEND \u0026#34;\u0026lt;BE_IP\u0026gt;:\u0026lt;HEARTBEAT_PORT\u0026gt;\u0026#34;; -- 3. Wait until TabletNum reaches 0, then remove ALTER SYSTEM DROP BACKEND \u0026#34;\u0026lt;BE_IP\u0026gt;:\u0026lt;HEARTBEAT_PORT\u0026gt;\u0026#34;; Things to Know Before Adopting # Known Limitations # Issue Description Workaround Routine Load Limited handling of malformed messages Pre-validate on the Kafka side datetime partitions Compatibility issues with Iceberg datetime partitions Use an alternative partitioning strategy Version upgrades Encountered bugs in 4.x releases Always test in a staging environment Always thoroughly validate version upgrades in a staging environment before applying them to production. We went through several rounds of upgrades and rollbacks ourselves. Have a rollback plan ready at all times.\nAdoption Checklist # Pre-deployment\nDefine use cases and requirements Estimate data volume and growth rate Choose table models Design partitioning strategy Post-deployment\nCreate and verify Routine Load jobs Configure user permissions Set data retention policies (TTL) Document scale-in/out procedures Build monitoring dashboards Conclusion # Here are the key lessons we learned from adopting StarRocks:\nThe Aggregate Key model is the centerpiece \u0026ndash; Automatic aggregation at ingestion time optimizes both storage and query performance Use BITMAP_UNION for exact unique counts \u0026ndash; Business KPIs demand precise numbers, not approximations Routine Load + Aggregate Key replaces Flink \u0026ndash; You can build a real-time aggregation pipeline with SQL alone Invest in operational automation \u0026ndash; Monitoring Materialized Views and Routine Load is essential For real-time analytics workloads, StarRocks is a powerful option that dramatically reduces pipeline complexity. That said, it is still maturing in terms of version upgrade stability and operational robustness, so we recommend conducting thorough PoC testing and staging validation before adopting it.\nReference: StarRocks Official Docs\n","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/posts/starrocks-adoption-guide/","section":"Posts","summary":"How we reduced pipeline latency from 5 minutes to sub-second by adopting StarRocks. Covers table model selection, data ingestion, performance tuning, and operational lessons from production.","title":"StarRocks Adoption Story: Revolutionizing Data Pipelines with Real-time OLAP","type":"posts"},{"content":" Why Compression Settings Matter # When you operate StarRocks long enough, there will inevitably come a point where your data grows to tens of terabytes. I have seen, time and again, cases where a single compression setting made a 30 to 50 percent difference in storage costs. But this is not just about disk space. Higher compression ratios reduce disk I/O and improve scan performance, while overly aggressive compression burns CPU and increases latency. Ultimately, choosing the right compression algorithm for your workload is one of the most important tuning decisions in StarRocks operations.\nComparing Supported Compression Algorithms # StarRocks supports several compression algorithms. Here is a comparison of the three most commonly used in practice.\nAlgorithm Compression Ratio Compression Speed Decompression Speed Best-fit Workload LZ4 Moderate (2-3x) Very fast Very fast Real-time analytics, low-latency queries ZSTD High (4-6x) Moderate Fast Batch analytics, cold data Snappy Low (1.5-2x) Fast Fast General purpose, legacy compatibility ZLIB High (4-5x) Slow Moderate Archiving, infrequently accessed data Personally, the combination I use most often is LZ4 for hot data and ZSTD for cold data. I occasionally use Snappy when dealing with data migrated from the Hadoop ecosystem, but I do not recommend it for new tables.\nSetting Compression When Creating a Table # You can specify the compression algorithm via the compression property in PROPERTIES when creating a table. If no value is set, StarRocks defaults to LZ4.\nReal-time Analytics Table (LZ4) # CREATE TABLE analytics.realtime_events ( event_id BIGINT, user_id BIGINT, event_type VARCHAR(64), event_time DATETIME, properties JSON ) ENGINE = OLAP DUPLICATE KEY(event_id) DISTRIBUTED BY HASH(user_id) BUCKETS 32 PROPERTIES ( \u0026#34;replication_num\u0026#34; = \u0026#34;3\u0026#34;, \u0026#34;compression\u0026#34; = \u0026#34;LZ4\u0026#34; ); LZ4 has overwhelmingly fast decompression speed, making it ideal for tables that need to serve dashboard queries with sub-second response times.\nBatch Analytics Table (ZSTD) # CREATE TABLE warehouse.order_history ( order_id BIGINT, customer_id BIGINT, order_date DATE, total_amount DECIMAL(18, 2), status VARCHAR(32), items ARRAY\u0026lt;STRUCT\u0026lt;sku STRING, qty INT, price DECIMAL(10,2)\u0026gt;\u0026gt; ) ENGINE = OLAP DUPLICATE KEY(order_id) PARTITION BY RANGE(order_date) ( PARTITION p2025 VALUES LESS THAN (\u0026#39;2026-01-01\u0026#39;), PARTITION p2026 VALUES LESS THAN (\u0026#39;2027-01-01\u0026#39;) ) DISTRIBUTED BY HASH(customer_id) BUCKETS 16 PROPERTIES ( \u0026#34;replication_num\u0026#34; = \u0026#34;2\u0026#34;, \u0026#34;compression\u0026#34; = \u0026#34;ZSTD\u0026#34; ); ZSTD achieves 1.5 to 2 times higher compression ratios than LZ4, which translates into significant storage savings on history tables where hundreds of millions of rows accumulate per partition.\nChanging Compression on an Existing Table # If you want to change the compression algorithm on a table that is already in production, you can use ALTER TABLE. Keep in mind, however, that the new setting only applies to data loaded after the change. Existing segments will not be recompressed until a compaction cycle runs against them.\nALTER TABLE warehouse.order_history SET (\u0026#34;compression\u0026#34; = \u0026#34;ZSTD\u0026#34;); Recommended Compression Settings by Workload # The following recommendations are based on patterns I have validated repeatedly in production environments.\nReal-time dashboards / Ad-hoc queries: Use LZ4. Its CPU overhead is negligible, minimizing the impact on P99 latency. Nightly batch reports / ETL result tables: Use ZSTD. When query frequency is low and data volume is high, the storage savings translate directly into cost reductions. High-volume log ingestion: Use ZSTD, but lower zstd_compression_level to 3 or below. This strikes a good balance between compression speed and compression ratio. Benchmark Results: Compression Ratio vs. Performance Tradeoffs # The following benchmark was run against an event log table containing approximately 5 billion rows (roughly 800 GB uncompressed).\nMetric LZ4 ZSTD (level 3) ZSTD (level 9) Compressed size 320 GB 195 GB 170 GB Compression ratio 2.5x 4.1x 4.7x Simple scan query (Avg) 1.2 s 1.5 s 1.8 s Aggregation query (Avg) 3.4 s 3.8 s 4.5 s Data ingestion throughput 120 MB/s 95 MB/s 60 MB/s Compared to LZ4, ZSTD level 3 reduced storage by approximately 39% while only increasing query latency by about 10 to 15 percent. ZSTD level 9, on the other hand, offered diminishing compression gains at the cost of a steep drop in ingestion throughput, making level 3 the optimal choice in most environments.\nOperational Tips and Monitoring # Here are three tips from production experience:\nAlways monitor compaction. Keep an eye on the compaction_score metric. When compression settings change or data volumes spike, compaction can fall behind, leading to degraded query performance from an excessive number of small segments. Separate compression strategies at the table level. Do not apply a single compression algorithm across your entire cluster. Match the algorithm to each table\u0026rsquo;s access pattern \u0026ndash; LZ4 for frequently queried tables, ZSTD for archival ones. Track disk usage trends over time. Use SHOW DATA to monitor how each table\u0026rsquo;s storage footprint evolves after compression changes. SHOW DATA FROM warehouse.order_history; Compression is not a set-it-and-forget-it decision. As data volumes grow and query patterns shift, it is worth revisiting your compression settings periodically. Even a small adjustment can yield meaningful improvements in both performance and infrastructure costs over time.\n","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/posts/starrocks-compression-guide/","section":"Posts","summary":"A practical guide to optimizing compression settings in StarRocks. Covers algorithm comparison, per-workload recommendations, benchmark results, and operational tips from production experience.","title":"StarRocks Compression Guide: Optimizing Performance and Storage","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/trends/","section":"Tags","summary":"","title":"Trends","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/trino/","section":"Tags","summary":"","title":"Trino","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/vector-store/","section":"Tags","summary":"","title":"Vector-Store","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/my-tech-blog/tags/workation/","section":"Tags","summary":"","title":"Workation","type":"tags"},{"content":" Hi, I\u0026rsquo;m nanta # I\u0026rsquo;m a data engineer focused on building reliable, scalable data infrastructure and pipelines. I enjoy solving the challenges that come with moving, transforming, and serving data at scale.\nAbout This Blog # This blog is a place where I share practical data engineering knowledge \u0026ndash; lessons learned from real-world projects, tutorials, architecture decisions, and tips that I hope will be useful to fellow engineers working in the data space.\nTech Stack # Here are the core technologies I work with on a daily basis:\nStream Processing: Apache Kafka Workflow Orchestration: Apache Airflow Query Engines: Trino, StarRocks Container Orchestration: Kubernetes Languages: Python, SQL Infrastructure: Docker Contact # Email: nanta0032@naver.com Feel free to reach out if you have questions, suggestions, or just want to connect!\n","externalUrl":null,"permalink":"/my-tech-blog/about/","section":"nanta - Data Engineering","summary":"About me and this blog","title":"About","type":"page"},{"content":" Who We Are # This blog is operated by nanta. The site address is https://nhosw.github.io/my-tech-blog/.\nWhat Data We Collect # Google Analytics # This site uses Google Analytics 4 (GA4) to understand how visitors use the site. GA4 collects:\nPages you visit and how long you stay Your approximate location (country/city level) Device type, browser, and operating system Referral source (how you found this site) Google Analytics uses cookies to distinguish unique visitors. No personally identifiable information is intentionally collected.\nFor more information, see Google\u0026rsquo;s Privacy Policy.\nGoogle AdSense # This site may display advertisements through Google AdSense. AdSense may use cookies and web beacons to serve ads based on your prior visits to this and other websites.\nGoogle uses the DART cookie to serve ads based on your browsing history You may opt out of personalized advertising by visiting Google Ads Settings For more information, see Google AdSense Privacy Policy.\nComments # This site does not currently have a comment system. If one is added in the future, this policy will be updated.\nCookies # Cookies are small text files stored on your device. This site uses cookies for:\nAnalytics: To measure site traffic (Google Analytics) Advertising: To serve relevant ads (Google AdSense) Preferences: To remember your theme preference (dark/light mode) You can control cookies through your browser settings. Disabling cookies may affect site functionality.\nThird-Party Links # Blog posts may contain links to external websites. We are not responsible for the privacy practices of other sites.\nYour Rights # You have the right to:\nKnow what data is being collected Opt out of tracking (via browser settings or Google Ads Settings) Request information about your data Changes to This Policy # This privacy policy may be updated from time to time. Changes will be posted on this page.\nContact # If you have questions about this privacy policy, please contact us via the information on the About page.\nLast updated: February 23, 2026\n","externalUrl":null,"permalink":"/my-tech-blog/privacy-policy/","section":"nanta - Data Engineering","summary":"Privacy Policy","title":"Privacy Policy","type":"page"}]