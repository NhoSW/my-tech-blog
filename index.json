[{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/airflow/","section":"Tags","summary":"","title":"Airflow","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/alluxio/","section":"Tags","summary":"","title":"Alluxio","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/argocd/","section":"Tags","summary":"","title":"Argocd","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/autoscaling/","section":"Tags","summary":"","title":"Autoscaling","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"Aws","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/blue-green/","section":"Tags","summary":"","title":"Blue-Green","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/cache/","section":"Tags","summary":"","title":"Cache","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/compaction/","section":"Tags","summary":"","title":"Compaction","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/csv/","section":"Tags","summary":"","title":"Csv","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/categories/data-engineering/","section":"Categories","summary":"","title":"Data Engineering","type":"categories"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/ebs/","section":"Tags","summary":"","title":"Ebs","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/eks/","section":"Tags","summary":"","title":"Eks","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/emr/","section":"Tags","summary":"","title":"Emr","type":"tags"},{"content":"EMR on EKS로 Spark 잡을 돌리면 executor 팟이 Kubernetes 위에 뜬다. 문제는 executor의 CPU와 메모리를 얼마나 줘야 하는지 잡마다 다르고 시간대마다 다르다는 거다. 너무 많이 주면 리소스가 낭비되고 너무 적게 주면 OOM으로 죽는다.\nVPA(Vertical Pod Autoscaler)를 쓰면 과거 실행 이력을 기반으로 적절한 리소스를 추천하고 자동으로 조정해준다. AWS가 EMR on EKS 전용 VPA 연동 기능을 제공한다고 해서 검토에 들어갔다. 약 한 달간 집중적으로 PoC를 진행했지만 결국 포기했다.\n이 글은 그 과정에서 배운 것을 정리한 기록이다.\nVPA 모드 이해 # EMR on EKS VPA는 세 가지 모드를 지원한다.\n모드 동작 영향 Off 추천값만 계산. 실제 조정 없음 (dry-run) 없음 Initial 잡 시작 시 추천값으로 리소스 설정 executor 재시작 없음 Auto 실행 중인 executor를 evict하고 추천값으로 재시작 지연 발생 가능 Initial 모드가 가장 안전해 보였다. executor 재시작 오버헤드가 없으니까 운영 잡에 바로 적용할 수 있을 거라 판단했다.\n계획은 이랬다. 먼저 Off 모드로 전체 Spark 잡에 켜서 추천값만 수집하고 추천값이 합리적인지 확인한 뒤에 Initial 모드로 전환한다.\n잡 시그니처 키 설계 # VPA가 추천값을 잡별로 관리하려면 각 잡을 식별할 키가 필요하다. EMR VPA는 잡 시그니처 키라는 개념을 쓴다. 같은 시그니처 키를 가진 잡 실행 이력이 쌓이면 그걸 기반으로 리소스 추천값을 계산한다.\n단순히 잡 이름만으로는 부족하다. 같은 잡이라도 피크 시간대와 새벽 시간대의 리소스 사용 패턴이 다르다. 요일에 따라서도 다르다.\n시그니처 키를 이렇게 설계했다.\nf\u0026#34;{app_name}-day-{target_date.day_of_week}-hour-{target_date.hour}-\u0026#34; f\u0026#34;{\u0026#39;holiday-\u0026#39; if is_holiday else \u0026#39;\u0026#39;}{resource_spec}\u0026#34; 구성 요소:\napp_name: Spark 잡 이름 day_of_week: 요일 (금요일 트래픽은 주중과 다르다) hour: 시간대 (피크 타임 구분) holiday: 공휴일 여부 resource_spec: DRA 활성화 여부 및 executor 개수 스펙 (VPA는 개별 팟의 버티컬 스케일링이므로 executor 수가 중요한 변수다) K8s 라벨 제한 문제 # 시그니처 키를 Kubernetes 라벨로 저장해야 하는데 라벨에는 제한이 있다.\n최대 63자 영숫자, -, _, .만 허용 영숫자로 시작하고 끝나야 함 시그니처 키 원문이 이 제한을 넘길 수 있어서 deterministic hash로 변환해서 사용했다. 원문과 해시의 매핑 정보는 Airflow XCom에 기록해서 나중에 추적할 수 있게 했다.\nYuniKorn과의 호환성 문제 # 첫 번째 벽은 스케줄러 호환성이었다.\nEMR on EKS에서 YuniKorn 스케줄러를 쓰고 있었는데 YuniKorn은 VPA를 지원하지 않는다. Gang scheduling 환경에서 VPA를 쓰면 문제가 생긴다. executor가 VPA에 의해 재시작될 때 처음 제출된 리소스 요청량과 VPA가 변경한 값이 달라지면서 잡이 실패하거나 좀비 상태에 빠질 수 있다.\nAuto 모드는 이 이유만으로 쓸 수 없었다. Initial 모드는 시작 시점에만 개입하니까 괜찮을 거라 판단하고 진행했다.\nPoC 진행 # 테스트 환경에 EMR VPA 오퍼레이터를 배포하고 세 가지 테스트 배치를 구성했다.\n쿠폰마트 적재 배치 (기존 EMR-on-EC2 환경에서 운영 중이던 잡) 딜리버리마트 적재 배치 DW 블로그 적재 배치 (1시간 단위 배치, 피크 기준 20분 소요 — 비교 측정에 적합) 각 배치를 Initial 모드와 Auto 모드로 돌려서 VPA 없는 기존 잡과 비교할 계획이었다.\nVPA 리소스가 생성되는 것까지는 확인했다. 그런데 VPA 상태가 NoPodsMatched로 나왔다. executor 팟의 리소스 사용 메트릭을 수집하지 못하고 있었다.\n오퍼레이터의 연쇄적 문제 # 트러블슈팅을 시작하자 문제가 줄줄이 나왔다.\n1. kube-apiserver 부하 # EMR VPA 오퍼레이터의 recommender가 타깃 네임스페이스를 제한하는 옵션이 없었다. 모든 네임스페이스의 모든 리소스를 주기적으로 조회했다. 심지어 kube-apiserver의 모든 엔드포인트를 찌르고 있었다.\nadmission controller의 타깃 네임스페이스는 OperatorGroup 리소스의 스펙을 수동으로 수정해서 제한할 수 있었지만 recommender는 방법이 없었다.\n2. VPA 리소스 인식 실패 # EMR VPA 컨트롤러(emr-dynamic-sizing-controller-manager)는 VPA 리소스를 정상적으로 생성했다. 그런데 같은 번들에 포함된 admission controller와 recommender가 생성된 VPA를 인식하지 못했다. 시그니처 키로 기존 VPA를 찾지 못하고 recommender도 추천값을 기록하지 못했다.\n3. 블랙박스 오퍼레이터 # AWS가 번들링한 오퍼레이터 패키지를 EKS에 설치하는 구조라 코드를 직접 확인하거나 수정할 수 없었다. 로그를 보고 추측하는 수밖에 없었다.\nAWS 서포트 케이스 # 두 차례에 걸쳐 서포트 케이스를 열었다.\n첫 번째는 네임스페이스 제한과 VPA 인식 실패 문제를 상세하게 정리해서 문의했다. AWS 측에서 매니페스트 번들을 통째로 공유해줬다. 직접 커스터마이징해서 재설치하라는 가이드였다.\n번들을 받아서 수정하고 재설치한 뒤 다시 테스트했다. 여전히 안 됐다. 같은 이슈가 재현됐고 관련 컴포넌트 로그를 정리해서 다시 문의했다.\n결론: 기능 자체가 유지보수되고 있지 않았다 # 한 달간의 검토 끝에 내린 결론이다.\nEMR VPA 연동 기능은 AWS 측에서 더 이상 개발/유지보수하고 있지 않은 것으로 보인다. 근거는 두 가지다.\n공식 문서 외에 이 기능을 실제로 사용하고 있다는 사례를 찾을 수 없었다 Kubernetes 1.27에서 도입된 In-place Pod Resize 기능이 VPA의 상위 호환이다. AWS가 EMR on EKS에 이 기능을 직접 연동할 계획이라는 얘기가 서밋에서 나왔다 VPA의 가장 큰 약점은 리소스를 조정하려면 팟을 재시작해야 한다는 점이다. In-place Pod Resize는 팟을 죽이지 않고 리소스를 변경할 수 있다. Kubernetes autoscaler 쪽에도 VPA에 in-place resize를 적용하는 PR이 올라온 상태다.\n배운 것 # AWS 공식 기능이라고 다 쓸 만한 건 아니다. 공식 문서에 있다고 실제로 동작한다는 보장이 없다. 검색해서 사용 사례가 나오지 않는 기능은 의심해봐야 한다.\n블랙박스 오퍼레이터는 트러블슈팅이 고통이다. 코드를 볼 수 없으니 로그만 보고 추측해야 한다. 서포트 케이스를 열어도 응답 주기가 길고 결국 매니페스트를 직접 고치라는 결론이 나온다.\nK8s 생태계의 방향을 읽어라. In-place Pod Resize가 나온 시점에서 VPA의 evict-and-recreate 방식은 레거시가 될 운명이었다. 기술 선택 전에 상위 프로젝트의 로드맵을 확인하는 게 중요하다.\nOff 모드(dry-run)부터 시작하자는 판단은 맞았다. 만약 운영 잡에 바로 적용했다면 오퍼레이터 문제로 잡 자체가 영향받을 수 있었다. Dry-run으로 시작한 덕분에 운영에는 영향을 주지 않고 문제를 파악할 수 있었다.\n참고 자료:\nEMR on EKS Vertical Autoscaling - AWS Documentation YuniKorn VPA Support Issue VPA In-place Resize PR ","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/emr-on-eks-vpa-review/","section":"글 목록","summary":"EMR on EKS 환경에서 Spark executor 리소스를 자동 최적화하려고 AWS 제공 VPA를 검토했다. 약 한 달간 PoC를 진행했지만 오퍼레이터 자체가 정상 동작하지 않았다. AWS 서포트 케이스를 여러 차례 열었고 매니페스트 번들까지 받아서 커스터마이징했지만 결국 포기했다.","title":"EMR on EKS VPA 검토기: AWS 공식 기능이 동작하지 않을 때","type":"posts"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/encoding/","section":"Tags","summary":"","title":"Encoding","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/helm/","section":"Tags","summary":"","title":"Helm","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/iceberg/","section":"Tags","summary":"","title":"Iceberg","type":"tags"},{"content":"Iceberg 테이블을 운영하다 보면 눈에 보이지 않는 곳에서 문제가 쌓인다. 작은 파일이 계속 늘어나는지, 컴팩션이 제대로 돌고 있는지, 특정 파티션에 데이터가 몰리고 있는지. 테이블 수가 적을 때는 수동으로 확인해도 되지만, 수십 개를 넘어가면 체계적인 모니터링이 필요하다.\n카카오의 로그 유형별 Iceberg 테이블 적재 및 운영 전략 글을 보고 본격적으로 모니터링 체계 구축에 착수했다.\n모니터링 대상 # Iceberg 메타테이블을 통해 확인해야 할 핵심 항목은 세 가지다.\n작은 파일 증가 여부: 작은 파일이 계속 쌓이면 쿼리 성능이 떨어진다. 스캔해야 할 파일 수 자체가 늘어나기 때문이다. 최적화 작업 상태: 컴팩션이 정상적으로 수행되고 있는지. 파일 수와 평균 파일 크기의 추이를 보면 알 수 있다. 파티션 분포: 특정 파티션에 데이터가 몰리는 현상이 있는지. 파티션별 파일 수와 크기를 비교하면 확인할 수 있다. 최신 스냅샷이 참조하는 데이터 파일의 개수, 평균 크기, 파티션별 분포를 주기적으로 수집하면 이 항목들을 모두 커버할 수 있다.\n아키텍처: 두 가지 접근 # 모니터링 데이터 소스를 어떻게 구성할지 두 가지 방안을 검토했다.\n1안: Grafana에서 Trino 직접 조회 # Grafana에 Trino 데이터소스를 연결하고 대시보드 조회 시마다 메타테이블 쿼리를 직접 제출하는 방식이다.\n장점은 구현이 간단하다는 거다. 단점이 많았다.\n대시보드를 열 때마다 Trino에 쿼리가 제출된다. 거의 같은 결과를 매번 다시 계산하는 셈이다. 무거운 테이블의 메타 쿼리는 수 분이 걸린다. 대시보드 로딩이 실용적이지 않다. Trino 자체의 제약으로 메트릭 계산 로직을 커스터마이징하기 어렵다. Grafana의 Trino 플러그인이 ROW 타입 같은 nested type을 인식하지 못한다. 2안: Prometheus Pushgateway 기반 # 카카오의 사례처럼 Prometheus Pushgateway를 통해 간접 연동하는 방식이다. 메타테이블 쿼리를 배치로 실행하고 결과를 Pushgateway에 밀어넣으면 Prometheus가 주기적으로 가져간다.\n장점이 확실했다.\nTrino에 대한 불필요한 중복 쿼리가 없다. 메트릭 계산 로직을 자유롭게 커스터마이징할 수 있다. Trino의 제약을 우회하기 위해 Spark를 쓰거나 Glue Catalog API를 직접 활용할 수도 있다. Airflow의 컴팩션 작업 후행으로 메트릭 푸시를 붙이면 자연스럽게 파이프라인이 구성된다. 결론은 1안으로 빠르게 시작하되 2안으로 점진적으로 전환하는 방향이었다. 최종적으로 두 가지를 병행해서 용도에 따라 사용한다.\n대시보드 구성 # 전체 테이블 헬스 모니터링 # Pushgateway 기반 대시보드다. 모든 Iceberg 테이블의 상태를 시간 흐름에 따라 추적한다.\n$files 메타테이블은 가장 최신 스냅샷이 참조하는 파일만 보여준다. 과거 스냅샷의 파일 정보까지 포함한 히스토리는 제공하지 않는다. 하지만 메타테이블 쿼리 결과를 주기적으로 Pushgateway에 밀어넣고 Prometheus에서 가져가면, 특정 시점의 스냅샷 상태가 시계열 메트릭으로 쌓인다. 시간에 따른 변동 추이를 볼 수 있게 되는 거다.\n테이블 수가 많아지면 각 테이블의 지표 스케일이 달라서 한 화면에서 직관적으로 보기 어렵다. 정규식 텍스트 검색 옵션을 추가해서 특정 테이블만 필터링할 수 있도록 했다.\n특정 테이블 상세 탐색 # Grafana의 Trino 데이터소스를 직접 활용하는 대시보드다. 특정 테이블을 선택하면 현 시점 기준으로 $partitions 메타테이블을 조회해서 각 날짜 파티션의 파일 수와 파일 크기를 보여준다.\n전체 테이블 대시보드는 시간 흐름에 따른 추이를 보는 용도고, 이 대시보드는 특정 시점의 파티션별 분포를 보는 용도다.\nTrino 메타테이블 버그와 체리피킹 # 모니터링 구축 당시 Trino v451을 운영하고 있었다. 두 가지 문제를 발견해서 상위 버전의 픽스를 체리피킹했다. 이후 v476으로 업그레이드하면서 이 픽스들은 자연스럽게 포함됐다.\n$files 메타테이블의 delete file 누락 # Iceberg v2의 delete file(positional, equality)을 $files 메타테이블이 인식하지 못하는 버그가 있었다. v455에서 수정됐다. 포크 레포의 stage 브랜치에 해당 커밋을 체리피킹해서 적용했고, 정상적으로 delete file 통계가 조회되는 것을 확인했다.\n$files 메타테이블에 partition 필드 없음 # v451까지는 $files 메타테이블에 각 파일이 속한 파티션 정보가 없었다. 파티션별 파일 통계를 수집하려면 이 필드가 필수적이다. v465에서 partition, spec_id, sort_order_id, readable_metrics 컬럼이 추가됐다. 이것도 체리피킹해서 적용했다.\ngrafana-trino 플러그인의 한계 # Grafana의 Trino 플러그인은 nested type과 ROW 타입 필드를 인식하지 못한다. $partitions 메타테이블의 ROW 타입 partition 필드가 대표적인 예다. partition.log_ts_day처럼 명시적으로 지정해줘야 한다.\n이 때문에 시간 파티션 외에 추가 파티션이 있는 테이블이나 day 단위가 아닌 파티션을 가진 테이블을 범용적으로 커버하기 어렵다. 플러그인 자체의 근본적인 제약이다.\n무거운 테이블의 메타 쿼리 성능 # 앱 로그나 웹 로그 같은 대용량 테이블의 메타테이블 쿼리는 매우 느렸다. 현업 적용이 불가능할 정도였다. 해당 테이블들은 모니터링 대상에서 일단 제외했다.\n이 테이블들은 규모도 크지만 rewrite_manifests() 최적화가 제대로 수행되지 않고 있는 상태이기도 했다. 매니페스트 최적화가 되면 스캔 플래닝이 빨라져서 메타 쿼리 성능도 개선될 것으로 기대했다. 그런데 rewrite_manifests()를 정상화해도 메타 쿼리 성능에 유의미한 개선은 없었다.\nSpark vs Trino 메타테이블 쿼리 성능 # Spark의 메타테이블 쿼리 성능이 Trino보다 월등히 좋다. 특히 무거운 테이블일수록 차이가 크다.\n이유가 있다. Spark는 iceberg-core의 Table API를 통해 Snapshot에서 ManifestList로 필요한 DataFile만 조립한다. Trino는 메타데이터 파일들을 읽고 JOIN하는 방식이라 스캔 오버헤드가 크다.\n배치성으로 Pushgateway에 메트릭을 밀어넣는 구성이라면 Trino 대신 Spark 엔진을 사용하는 것도 방법이다. Spark는 all_files 같은 메타테이블도 추가로 제공하니까 활용 범위가 더 넓다.\n메트릭 파이프라인 # Pushgateway 배포 # Prometheus Pushgateway를 Helm 차트로 EKS에 배포했다. Prometheus 서버에 Pushgateway 스크래퍼를 추가해서 메트릭을 수집하도록 구성했다.\nAirflow 배치 파이프라인 # Airflow DAG으로 Iceberg 테이블 메트릭 푸시 파이프라인을 구성했다.\n대상 테이블 추출: Glue API를 통해 Iceberg 포맷 테이블을 자동 추출한다. temp 스키마의 사용자 테이블은 제외하기 위해 스캔 대상 스키마 목록은 DAG 코드에서 관리한다. Dynamic task mapping: 추출된 각 테이블에 대해 메타 쿼리를 수행하고 메트릭을 Pushgateway에 푸시하는 태스크가 동적으로 생성된다. 메트릭 정의: iceberg_data_file_count 같은 메트릭을 env, partition, table 라벨로 구분해서 수집한다. 컴팩션 작업의 후행으로 메트릭 푸시 로직을 붙이는 것도 가능하다. 기존 컴팩션 태스크 그룹 구현체에 후행 로직을 추가하는 방식이다.\nIceberg 테이블 목록 자동 추출의 한계 # 모니터링 구축 초기(v451)에는 Trino에서 특정 스키마 내 Iceberg 포맷 테이블만 선별하는 것이 불가능했다. SHOW TABLES 쿼리나 information_schema.tables 조회 모두 Glue Catalog에 등록된 모든 테이블을 가져왔다. 테이블 리디렉션 비활성화 상태에서도 마찬가지였다. 그래서 Glue API를 통해 Iceberg 테이블을 추출하는 방식을 택했다.\nv475에 추가된 system.iceberg_tables 테이블을 쓰면 Iceberg 테이블만 리스팅할 수 있다. 현재 운영 중인 v476에서 사용 가능하다.\nDELETE FILE 현황 파악 # 모니터링 구축 과정에서 CDC 테이블들의 delete file 생성 현황도 파악했다.\n대부분의 CDC 싱크 테이블은 소스 RDB 자체가 히스토리성 테이블이라 사실상 append-only로 쌓이고 있다. 기존 레코드의 update가 드물게 발생하고 후행 컴팩션으로 정리되므로 delete file은 거의 쌓이지 않는다.\n다만 직전에 추가된 레코드에 대해 바로 update/delete가 발생하는 일부 테이블에서는 positional delete file도 생성되고 있었다. 이런 테이블은 컴팩션 주기나 전략을 따로 검토할 필요가 있다.\nv451에서 v476까지: 모니터링 관련 개선 사항 # 모니터링 구축은 v451에서 시작했고, 이후 v476으로 업그레이드했다. 그 사이에 Iceberg 모니터링에 직접적으로 도움이 되는 개선 사항이 많았다.\n버전 개선 내용 적용 방식 v455 $files 메타테이블에서 delete file 인식 버그 수정 v451에 체리피킹 v465 $files 메타테이블에 partition 필드 추가 v451에 체리피킹 v466 Glue Catalog 조회 병렬화로 성능 개선 v476 업그레이드 시 반영 v469 $all_entries 메타테이블 추가 (Spark의 all_entries에 대응) v476 업그레이드 시 반영 v470 $all_entries 버그 수정, optimize_manifests 프로시저 추가 v476 업그레이드 시 반영 v475 system.iceberg_tables 테이블 추가 (Iceberg 테이블만 리스팅) v476 업그레이드 시 반영 v455와 v465는 모니터링 구축에 필수적이어서 v451 시절에 체리피킹으로 먼저 적용했다. 나머지는 v476 업그레이드와 함께 자연스럽게 사용 가능해졌다.\n배운 것 # 메타테이블의 한계를 이해해야 한다. $files는 최신 스냅샷만 보여준다. 히스토리를 보려면 주기적으로 결과를 외부에 저장하는 방법밖에 없다. Pushgateway가 딱 맞는 구성이다.\nTrino의 메타 쿼리는 무거운 테이블에서 느리다. Spark가 훨씬 빠르다. 구현 방식 차이 때문이다. 배치 파이프라인이라면 Spark를 쓰는 것도 합리적이다.\n직접 조회와 배치 수집을 병행하자. 전체 테이블 추이는 Pushgateway 기반 배치로, 특정 테이블 심층 분석은 직접 쿼리로. 두 가지를 용도에 맞게 나누니까 효과적이었다.\nTrino 포크를 운영한다면 체리피킹은 피할 수 없다. 필요한 버그 수정이 상위 버전에만 있으면 가져와서 적용해야 한다. 특히 모니터링처럼 신뢰성이 중요한 영역에서는 부정확한 데이터가 더 위험하다.\n참고 자료:\n로그 유형별 Iceberg 테이블 적재 및 운영 전략 - kakao tech Iceberg Connector - Trino Documentation $files delete file bug fix (v455) $files partition field (v465) Glue catalog query parallelization (v466) $all_entries metadata table (v469) optimize_manifests procedure (v470) system.iceberg_tables (v475) Prometheus Pushgateway grafana-trino plugin ","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/iceberg-table-monitoring/","section":"글 목록","summary":"Iceberg 테이블 수가 늘어나면서 파일 상태를 체계적으로 모니터링할 필요가 생겼다. Trino 메타테이블을 활용해 파일 수, 크기, 파티션 분포를 추적하고 Prometheus Pushgateway와 Grafana로 대시보드를 구성했다. 과정에서 만난 Trino 버그와 성능 이슈도 정리했다.","title":"Iceberg 테이블 모니터링 구축: Trino 메타테이블과 Prometheus Pushgateway","type":"posts"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/java/","section":"Tags","summary":"","title":"Java","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka","type":"tags"},{"content":"Kafka 클러스터가 버전 업되면서 rack awareness를 통한 네트워크 비용 절약이 가능해졌다. 같은 AZ에 있는 컨슈머가 같은 AZ의 파티션 리플리카를 읽으면 cross-AZ 트래픽을 줄일 수 있다.\nEMR 기반 Spark 잡에도 이걸 적용하고 싶었다. 각 AZ에 배포된 executor가 같은 AZ의 Kafka 파티션을 담당하게 하면 된다. 결론부터 말하면 Spark가 이걸 지원하지 않는다.\nRack Awareness란 # Kafka의 rack awareness는 컨슈머가 자신의 위치(rack 또는 AZ)를 브로커에 알려주면, 브로커가 가장 가까운 리플리카에서 데이터를 제공하는 기능이다. KIP-881로 컨슈머 파티션 할당 시에도 rack 정보를 반영하도록 개선됐다.\n설정은 간단하다. 컨슈머에 client.rack 속성을 지정하면 된다. 예를 들어 ap-northeast-2a를 지정하면 해당 AZ의 리플리카에서 읽게 된다.\n문제는 Spark에서 이 설정을 어떻게 주입하느냐다.\n코드 분석: 어디에 주입해야 하나 # 먼저 코드베이스에서 client.rack 설정을 주입할 위치를 파악했다.\nKafka 토픽 읽기 흐름은 이랬다: 개별 잡 → LogStoreProcessor.getKafkaLogDataFrame() → SparkSessionManager.rowKafkaDF(). rowKafkaDF에서 Kafka 토픽을 읽어 데이터프레임으로 변환하는 부분에 client.rack을 넣으면 될 것으로 판단했다.\n쓰기 쪽도 확인했다. Kafka 토픽에 쓸 때는 rack awareness가 일반적으로 의미 없지만, 브로커 설정에 따라 파티션 리더 재선출에 영향을 줄 수 있다고 해서 쓰기 로직에도 주입하려 했다.\nAZ 정보 추출 방법 검토 # client.rack에 넣을 현재 AZ 정보를 어떻게 가져올지 두 가지 방안을 검토했다.\n1안: Kubernetes Downward API # Kubernetes의 Downward API를 통해 노드의 토폴로지 라벨 정보를 컨테이너에 주입하는 방식이다.\n문제가 있었다.\nAZ 정보는 팟 라벨이 아닌 노드 라벨에만 있다 현재 Downward API는 팟 라벨만 주입 가능하고 노드 라벨은 주입할 수 없다 KEP-4742에서 노드 토폴로지 라벨을 Downward API로 노출하는 기능이 제안됐고 알파 피쳐로 릴리즈됐지만, 운영 중인 EKS 버전에서는 사용 불가능하다 노드명을 spec.nodeName에서 추출하고 K8s API로 노드 라벨을 조회하는 우회 방법도 있지만, initContainer 추가나 코드 수정이 필요하고 EMR-on-EKS에서는 pod template 파일을 S3에 올려서 관리해야 한다. 효용 대비 공수가 너무 컸다.\n2안: AWS IMDS # 훨씬 간단한 방법이 있었다. AWS 인스턴스 메타데이터 서비스(IMDS)를 통해 현재 AZ 정보를 바로 가져올 수 있다.\ncurl -s http://169.254.169.254/latest/meta-data/placement/availability-zone # ap-northeast-2b EC2 인스턴스와 그 위에 배포된 컨테이너에서 고정 IP(169.254.169.254)로 접근 가능하다. Kubernetes Downward API 같은 추가 구성이 필요 없다. EMR-on-EC2 환경의 Spark 잡에도 동일하게 적용된다.\n이 방식으로 AZ 정보를 추출하는 유틸리티 클래스를 구현했다.\n그런데 Spark가 지원하지 않는다 # AZ 정보를 추출하는 것까지는 해결했다. 문제는 그다음이었다.\n추출한 AZ 정보를 SparkSessionManager.rowKafkaDF()에서 client.rack 형태로 Kafka 세션에 주입하려 했다. Spark의 Kafka 연동 공식 가이드를 확인했는데 rack awareness 관련 언급이 없었다.\n커뮤니티를 추가로 검색해보니 현재 Spark는 Kafka 연동 시 rack awareness를 지원하지 않는다는 걸 확인했다. client.rack을 설정하는 것만으로는 안 된다. Spark 드라이버가 executor에 Kafka 파티션을 할당할 때 rack 정보를 고려하는 로직이 Spark 자체 코드에 추가되어야 한다.\n관련 Jira 티켓(SPARK-46798)이 열려 있고, PR도 제출됐었다. 하지만 cloud vendor specific한 로직이라는 이유로 PR이 닫혀버렸다. 리뷰어들은 이런 기능이 들어가려면 정식 SPIP(Spark Improvement Proposal)가 필요하고 cloud-agnostic한 설계가 되어야 한다고 지적했다.\n현재 상태와 향후 방향 # Spark 커뮤니티에서 이 기능이 구현되기 전까지는 Spark Structured Streaming에서 Kafka rack awareness를 사용할 수 없다. SPARK-46798 티켓은 열려 있지만 활발하게 진행되고 있지는 않다.\n한편 Kubernetes 쪽에서는 KEP-4742가 진행되고 있다. 노드 토폴로지 라벨을 팟에 자동으로 복사해주는 기능이다. EKS에서 이 기능이 사용 가능해지면 AZ 정보 추출이 더 깔끔해지지만, Spark 쪽 지원이 없으면 의미가 없다.\n배운 것 # 컨슈머에 client.rack을 설정하는 것과 파티션 할당 시 rack을 고려하는 것은 별개다. Kafka 자체는 rack-aware 파티션 할당을 지원하지만, Spark의 Kafka 연동 레이어에서 이 정보를 활용하는 로직이 빠져 있다.\nIMDS는 AWS 환경에서 인스턴스 메타데이터를 가져오는 가장 간단한 방법이다. Kubernetes Downward API의 한계를 우회할 수 있다. EKS든 EMR-on-EC2든 동일하게 동작한다.\n오픈소스 커뮤니티의 방향을 미리 확인하자. 구현을 시작하기 전에 Spark 커뮤니티의 기존 논의를 먼저 확인했으면 불필요한 작업을 줄일 수 있었다.\n참고 자료:\nSPARK-46798: Kafka custom partition location assignment (rack awareness) SPARK-46798 PR (closed) Spark Kafka Rack Aware Consumer - Apache Mailing List Structured Streaming + Kafka Integration Guide KEP-4742: Expose Node Topology Labels via Downward API K8s Issue: Exposing node labels via Downward API KIP-881: Rack-aware Partition Assignment for Kafka Consumers ","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/kafka-rack-awareness-spark/","section":"글 목록","summary":"Kafka 클러스터의 rack awareness를 Spark에도 적용해서 cross-AZ 네트워크 비용을 줄이려 했다. AZ 정보 추출까지는 해결했지만, Spark 자체가 Kafka 연동 시 rack awareness를 지원하지 않았다. 관련 티켓은 커뮤니티에 열려 있지만 PR은 닫힌 상태다.","title":"Kafka Rack Awareness와 Spark: 현재 지원하지 않는다","type":"posts"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/kafka-connect/","section":"Tags","summary":"","title":"Kafka-Connect","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/karpenter/","section":"Tags","summary":"","title":"Karpenter","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/managed-service/","section":"Tags","summary":"","title":"Managed-Service","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"Monitoring","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/","section":"nanta - 데이터 엔지니어링","summary":"","title":"nanta - 데이터 엔지니어링","type":"page"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/networking/","section":"Tags","summary":"","title":"Networking","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/parquet/","section":"Tags","summary":"","title":"Parquet","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/postgresql/","section":"Tags","summary":"","title":"Postgresql","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"Prometheus","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/pushgateway/","section":"Tags","summary":"","title":"Pushgateway","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/rack-awareness/","section":"Tags","summary":"","title":"Rack-Awareness","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/routing/","section":"Tags","summary":"","title":"Routing","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/s3/","section":"Tags","summary":"","title":"S3","type":"tags"},{"content":"2025년 3월에 S3 테이블 버킷이 서울 리전에 출시됐다. 한 줄로 요약하면 매니지드 Iceberg 테이블이다. 자동 컴팩션과 스냅샷 관리를 제공하고 일반 S3 버킷보다 높은 TPS와 스루풋을 제공한다고 한다.\n현재 CDC 싱크 테이블 운영에서 컴팩션이 골칫거리다. 시간 파티션이 걸려 있어도 원천 테이블이 히스토리성이 아니면 전 기간 파티션에 걸쳐 update/delete가 발생한다. 컴팩션 대상 시간 범위를 최근 5일이나 30일로 설정해도 그 밖의 파티션에 쌓이는 파일은 정리되지 않는다. 전 기간 컴팩션은 비용과 관리 면에서 현실적이지 않다.\n자동 컴팩션이 이 문제를 해결해줄 수 있을까? PoC를 진행했다.\n매니지드 서비스에 대한 우려 # 기대도 있었지만 우려도 있었다.\n첫째, 블랙박스 문제다. 매니지드 서비스에서 이슈가 발생하면 내부를 들여다볼 수 없다. 출시된 지 얼마 안 된 서비스라 실사용 사례도 적다.\n둘째, 과거 경험이다. Glue/LakeFormation의 하위 기능으로 출시됐던 Iceberg 매니지드 컴팩션 기능을 써본 적이 있다. 내부적으로 Glue 기반 Spark 프로시저 호출 잡으로 수행되는데, 대용량 테이블에서 별다른 에러 로그도 없이 실패했다.\n셋째, 비용이다. 스토리지 비용이 일반 버킷보다 높고 자동 컴팩션에 따른 추가 비용도 발생한다.\nGlue REST Catalog 연동 # S3 테이블 버킷은 Glue Data Catalog의 Iceberg REST 엔드포인트를 통해 접근한다. 기존 Glue 카탈로그(glue 타입)가 아닌 REST 타입 카탈로그를 사용해야 한다.\n기존 테이블과 동시 관리 불가 # 가장 먼저 확인한 사실이다. 단일 Iceberg 카탈로그로 S3 테이블 버킷의 테이블과 기존 일반 S3 버킷의 Iceberg 테이블을 동시에 관리할 수 없다. LakeFormation 권한 정책 자체가 카탈로그 레벨에서 분리되어 있다.\n별도 Trino 카탈로그를 추가해야 한다.\n기존 테이블: iceberg.raw_log.weblog_common 테이블 버킷 테이블: iceberg_managed.raw_log.weblog_common_managed DDL 제한 # REST 타입 카탈로그에서는 일부 DDL이 지원되지 않는다.\nCREATE TABLE: Glue REST Catalog가 Trino Iceberg 커넥터가 의존하는 stage-create API를 지원하지 않는다 RENAME TABLE: Glue REST API에서 지원하지 않는다고 공식 문서에 명시되어 있다 SELECT, INSERT 등 읽기/쓰기 작업은 정상 동작한다. Hive → Iceberg 테이블 리디렉션은 되지만, Iceberg → Hive 리디렉션은 REST 카탈로그에서 지원되지 않는다.\nLakeFormation 권한 # S3 테이블 버킷은 기본적으로 LakeFormation 연동이 활성화되어 있다. Trino 워커, Kafka Connect, Spark 등 데이터에 접근하는 모든 IAM role에 대해 LakeFormation에서 테이블/스키마/카탈로그 단위 권한을 명시적으로 부여해야 한다.\n권한이 없으면 에러가 발생하는 게 아니라, 해당 카탈로그에 아무 테이블도 없는 것처럼 보인다. 디버깅하기 까다로운 부분이다.\nTrino 연동 # Trino v471부터 S3 테이블 버킷에 대한 읽기가 지원된다. 현재 운영 중인 v476에서 사용 가능하다.\nFault-Tolerant 모드 비호환 # Trino의 fault-tolerance 설정(retry_policy)이 활성화된 상태에서 S3 테이블 버킷에 쓰기를 시도하면 에러가 발생한다. S3 테이블 버킷의 내부 경로 구조가 일반 S3와 달라서 파일 리스팅 API가 호환되지 않는 것으로 보인다. Trino 커뮤니티에 이슈가 열려 있다.\n우회 방법은 해당 쿼리 세션에서 retry_policy를 NONE으로 지정하는 것이다. 이렇게 하면 정상적으로 쓰기가 동작한다.\nSpark 연동 # EMR Spark에서도 Glue REST Catalog 엔드포인트를 통해 S3 테이블 버킷의 Iceberg 테이블을 정상 조회할 수 있다.\nAWS가 오픈소스로 제공하는 S3 Tables Catalog 라이브러리를 사용하려면 EMR v7.5 이상이 필요하지만, Glue REST 엔드포인트를 직접 활용하면 EMR v6.14에서도 연동 가능하다.\nSpark 세션 설정 예시:\nspark.sql.catalog.iceberg_managed=org.apache.iceberg.spark.SparkCatalog spark.sql.catalog.iceberg_managed.type=rest spark.sql.catalog.iceberg_managed.uri=https://glue.ap-northeast-2.amazonaws.com/iceberg spark.sql.catalog.iceberg_managed.rest.sigv4-enabled=true spark.sql.catalog.iceberg_managed.rest.signing-name=glue Trino와 마찬가지로 LakeFormation에서 EMR Spark용 IAM role에 권한을 부여해야 한다. 권한이 없으면 에러 없이 빈 결과가 반환된다.\nKafka Connect 연동 # Kafka Connect의 Iceberg 싱크 커넥터에서 Glue REST 엔드포인트를 통해 S3 테이블 버킷으로의 실시간 싱크를 확인했다.\nCDC 토픽의 경우 Debezium 소스 커넥터가 프로듀스하는 메시지에 스키마 정보가 포함되어 있어서 테이블 자동 생성과 스키마 에볼루션이 정상 동작한다. 스키마 없이 인입되는 웹 로그 같은 경우에는 타임스탬프 타입 추론이 불가능해서 테이블을 미리 생성해둬야 한다.\n자동 컴팩션 검토 # 동작 확인 # S3 테이블 버킷에 생성된 모든 테이블에는 완전관리형 컴팩션이 자동 활성화된다. 기본 타겟 파일 크기는 512MB이고 64MB~512MB 사이에서 조정 가능하다.\n자동 컴팩션이 트리거되는 조건은 문서화되어 있지 않다. AWS CLI를 통해 잡 수행 히스토리는 확인할 수 있지만 트리거 조건 자체는 공개되지 않았다. 필요하면 서포트 케이스로 문의해야 한다.\n현재 1시간 단위 배치로 수행하는 수동 컴팩션에 비해 트리거 조건이 상당히 보수적으로 보였다.\n비용 # 스토리지 비용과 읽기/쓰기 비용 외에 자동 컴팩션에 따른 비용이 추가된다. 처리된 객체 1,000개당 $0.004, 처리된 GB당 $0.05가 과금된다.\n예를 들어 3만 개의 5MB 파일(약 146GB)을 컴팩션하면 약 $7.44가 든다. 수동 컴팩션의 컴퓨팅 비용과 관리 공수를 생각하면 받아들일 만한 수준이다.\nappend-only 테이블에는 불필요 # append-only로 동작하는 로그 테이블은 시간 파티션별로 데이터가 순차 적재되므로 기존 수동 컴팩션으로 충분하다. 자동 컴팩션의 보수적인 트리거 조건을 감안하면 현재 운영 방식을 유지하는 게 비용과 성능 면에서 합리적이다.\nS3 테이블 버킷은 어떤 파티션에서 update/delete가 발생할지 예측하기 어려운 CDC 테이블에 한해 적용하는 것이 합리적이다.\nCDC 싱크 테이블 PoC # append-only 로그 테이블에 대한 PoC에 이어서, 실제로 업데이트가 지속적으로 발생하는 CDC 테이블에 대해 추가 PoC를 진행했다. 자동 컴팩션이 CDC 싱크 커밋과 충돌 없이 동작하는지, 기존 수동 컴팩션 대비 유지보수성이 개선되는지 확인하는 것이 목적이다.\n운영 계획은 신규 CDC 테이블 싱크 요청이 들어올 때 기존 S3 버킷과 S3 테이블 버킷에 2벌의 싱크 파이프라인을 운영하면서 안정성을 검증하는 방식이다. 검증이 끝나면 기존 S3 버킷 쪽을 제거한다.\n배운 것 # 매니지드 서비스의 자동화가 모든 워크로드에 맞는 건 아니다. append-only 테이블은 기존 수동 컴팩션이 더 효율적이다. 자동 컴팩션은 예측 불가능한 파티션에 update/delete가 발생하는 CDC 테이블에 가치가 있다.\n카탈로그 분리는 피할 수 없다. S3 테이블 버킷용 카탈로그와 기존 Glue 카탈로그를 단일 Trino 카탈로그로 통합할 수 없다. 사용자에게 별도 카탈로그명을 안내해야 한다.\nLakeFormation 권한 누락은 에러가 아닌 빈 결과로 나타난다. 디버깅이 어렵다. IAM role에 권한을 미리 부여하는 체크리스트가 필요하다.\n과거 매니지드 컴팩션의 실패 경험을 잊지 말자. Glue/LakeFormation 매니지드 컴팩션은 조용히 실패했다. S3 테이블 버킷이 같은 전철을 밟지 않는다는 보장은 없다. 충분한 PoC와 2벌 운영으로 검증하는 게 맞다.\n참고 자료:\nAmazon S3 Tables New Amazon S3 Tables: Storage optimized for analytics workloads How Amazon S3 Tables use compaction to improve query performance by up to 3x Working with Amazon S3 Tables Trino: Add read support for S3 Tables in Iceberg (v471) Trino: Fault tolerant mode fails with S3 Table Buckets Iceberg Connector - Trino Documentation AWS S3 Tables Catalog ","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/s3-table-buckets-poc/","section":"글 목록","summary":"AWS S3 테이블 버킷은 자동 컴팩션을 제공하는 매니지드 Iceberg다. CDC 싱크 테이블의 컴팩션 문제를 해결할 수 있을지 PoC를 진행했다. Trino, Spark, Kafka Connect 연동을 확인하고 자동 컴팩션 동작과 비용을 검토했다.","title":"S3 테이블 버킷 도입 검토: 매니지드 Iceberg의 가능성과 한계","type":"posts"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/spark/","section":"Tags","summary":"","title":"Spark","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/superset/","section":"Tags","summary":"","title":"Superset","type":"tags"},{"content":"데이터 거버넌스팀에서 Superset을 적극적으로 활용하면서 발견한 이슈 6건을 공유해주셨다. 단순한 설정 변경으로 해결되는 것부터, RDS 로그를 뒤져서 원인을 찾아야 했던 것까지 난이도가 다양했다. 각 이슈의 원인과 대응을 정리한다.\n1. Slack 차트 스크린샷 확장자 누락 # 제보 내용: Superset에서 Slack으로 피딩되는 차트 스크린샷 파일을 다운로드하면 확장자가 없어서, OS가 이미지 파일로 인식하지 못한다.\n원인: Slack API를 호출할 때 파일명에 확장자를 포함하지 않고 있었다. Slack은 파일명을 그대로 사용하기 때문에, 확장자가 없으면 다운로드한 파일에도 확장자가 붙지 않는다.\n대응: Slack API 호출 인자에서 파일명을 .png 확장자와 함께 전달하도록 수정했다.\n2. 대시보드 자동 새로고침 초기화 # 제보 내용: 대시보드에서 auto-refresh interval을 설정해도 재접속하면 초기화된다.\n원인: 대시보드 우측 상단의 auto-refresh 메뉴는 현재 브라우저 세션에만 적용되는 설정이다. Superset의 설계상 이 값은 서버에 저장되지 않는다.\n대응: 대시보드 자체에 리프레시 인터벌을 설정하는 방법을 안내했다. 대시보드 편집 모드에서 설정한 리프레시 인터벌은 대시보드 메타데이터에 저장되므로, 누가 접속하든 동일하게 적용된다.\n이 케이스는 버그가 아니라 UX 혼동이었다. 같은 목적의 설정이 세션 레벨과 대시보드 레벨 두 곳에 있으면 사용자 입장에서 혼란스럽다.\n3. SqlLab 데이터셋 덮어쓰기 버그 # 제보 내용: SqlLab에서 \u0026ldquo;overwrite existing dataset\u0026rdquo; 옵션을 선택하면 기존 데이터셋 목록이 제대로 표시되지 않는다.\n원인: Superset 업스트림의 알려진 버그였다.\n대응: Apache Superset 커뮤니티에서 해당 이슈의 픽스 커밋을 확인하고 체리피킹해서 적용했다.\n4. 윈도우 환경 CSV 한글 깨짐 # 제보 내용: Superset에서 다운로드한 CSV를 윈도우의 Excel에서 열면 한글이 깨진다.\n원인: Superset이 CSV를 표준 UTF-8로 인코딩해서 내보내고 있었다. 문제는 Microsoft Excel이다. Excel은 CSV를 열 때 BOM(Byte Order Mark)이 없으면 시스템 기본 인코딩(한국어 윈도우에서는 EUC-KR)으로 해석한다.\nUTF-8로 인코딩된 파일이라도 BOM이 없으면 Excel은 UTF-8인지 알 수 없다. macOS나 Linux에서는 문제가 되지 않지만, 윈도우 Excel 사용자가 많은 환경에서는 반드시 고려해야 하는 부분이다.\n대응: CSV 인코딩을 utf-8에서 utf-8-sig로 변경했다. utf-8-sig는 파일 시작 부분에 BOM(EF BB BF)을 삽입하는 Python의 UTF-8 변형 인코딩이다. BOM이 있으면 Excel이 파일을 UTF-8로 올바르게 인식한다.\n5. 쿼리 결과 행 제한 10만 → 100만 # 제보 내용: Superset에서 쿼리 결과가 10만 행으로 제한된다. 100만 행까지 출력되면 좋겠다.\n대응: Superset의 ROW_LIMIT 관련 설정값을 변경해서 최대 100만 행까지 응답하도록 조정했다.\n단순한 설정 변경이지만, 행 제한을 무조건 올리는 것이 항상 좋은 것은 아니다. 행 수가 늘어나면 Superset 웹서버의 메모리 사용량과 응답 시간이 비례해서 증가한다. 100만 행이면 데이터 크기에 따라 수 GB의 메모리를 점유할 수 있다. 운영 환경의 리소스 상황을 보면서 결정해야 하는 설정이다.\n6. CSV 다운로드 500 에러 — 가장 까다로웠던 이슈 # 제보 내용: Superset에서 50만 행 이상의 쿼리 결과를 CSV로 다운로드하면 약 1분 뒤에 500 에러가 발생한다. 20만 행까지는 문제없다.\n이 이슈는 조건이 구체적이어서 원인 추적이 가능했다. \u0026ldquo;20만 행은 되고 50만 행은 안 된다\u0026quot;는 건 어딘가에 시간 기반 제한이 걸려 있다는 뜻이다.\n에러 로그 추적 # Superset 서버 로그에서 해당 시점의 에러를 확인했다.\npsycopg2.OperationalError: SSL connection has been closed unexpectedly Superset이 Redshift에서 데이터를 가져오는 과정에서 SSL 커넥션이 끊긴 것처럼 보이지만, 자세히 보면 이 에러는 Redshift 커넥션이 아니다. Superset의 메타데이터 데이터베이스(Aurora PostgreSQL) 커넥션에서 발생한 에러다.\n원인 분석 # CSV 다운로드 흐름을 추적해보면 이렇다:\n사용자가 CSV 다운로드 요청 Superset이 메타데이터 DB(Aurora PostgreSQL)에서 사용자의 다운로드 권한을 확인하는 트랜잭션을 연다 권한 확인 후, Redshift에서 실제 데이터를 가져오기 시작한다 50만 행 이상이면 Redshift에서 데이터를 가져오는 데 1분 이상 소요된다 이 동안 메타데이터 DB 쪽 트랜잭션은 열린 채 idle 상태로 대기한다 Aurora RDS의 idle_in_transaction_session_timeout 설정이 60초(60,000ms)로 되어 있어서, idle 트랜잭션이 강제 종료된다 트랜잭션이 끊기면서 Superset 서버가 500 에러를 반환한다 해당 시점의 Aurora RDS 에러 로그를 확인하니 이를 뒷받침하는 로그가 있었다.\nmaster@superset:[17810]:FATAL: terminating connection due to idle-in-transaction timeout 대응 # RDS 파라미터 그룹에서 idle_in_transaction_session_timeout을 60,000ms(1분)에서 600,000ms(10분)로 변경 요청했다. Superset 웹서버의 다른 타임아웃 설정(ALB, gunicorn 등)이 이미 10분으로 설정되어 있으므로, 이 값도 맞추는 것이 합리적이다.\n더 근본적인 해결 방법도 있다. 데이터 다운로드 중에 권한 체크용 트랜잭션이 열려 있을 필요가 없도록 Superset 코드 자체를 수정하거나, 메타데이터 DB 앞에 PgBouncer 같은 커넥션 풀러를 두는 방식이다. 하지만 RDS 파라미터 변경이 가장 빠르고 안전한 대응이어서 이 방식을 선택했다.\n설정 변경 후 50만 행 이상의 CSV 다운로드도 정상 동작함을 확인했다.\n기타: Helm 차트 db-init Job arm 노드 이슈 # Superset 베이스 Helm 차트에 포함된 db-init Job이 amd64 이미지를 사용하는데, 노드 셀렉터가 설정되어 있지 않아서 arm 노드에 스케줄링되면 실패하는 문제도 있었다. Helm 차트 버전을 v0.7.7에서 v0.8.10으로 올려서 해결했다.\n배운 것 # \u0026ldquo;N만 행은 되고 M만 행은 안 된다\u0026quot;는 시간 기반 제한의 신호다. 행 수 자체가 아니라 처리 시간에 걸리는 타임아웃을 의심해야 한다. 이 관점이 없으면 Redshift 커넥션만 계속 들여다보게 된다.\n에러가 발생하는 커넥션과 원인이 되는 커넥션이 다를 수 있다. SSL 커넥션 에러가 Redshift가 아닌 메타데이터 DB에서 발생했다. 로그에서 커넥션 대상을 정확히 확인하는 것이 중요하다.\n타임아웃 설정은 시스템 전체에서 일관되게 맞춰야 한다. ALB, gunicorn은 10분인데 DB만 1분이면, 가장 짧은 쪽에서 병목이 생긴다. 체인의 가장 약한 고리 원칙이다.\nCSV + 한글 + 윈도우 = BOM이 필수다. utf-8-sig는 Python에서 BOM을 삽입하는 가장 간단한 방법이다. 윈도우 Excel 사용자가 있는 환경이라면 기본으로 적용하는 것이 좋다.\n사용자 제보는 가장 가치 있는 QA다. 6건의 이슈 중 3건은 내부 테스트로는 발견하기 어려웠을 것이다. 윈도우 환경의 CSV 한글 깨짐이나 50만 행 이상의 다운로드 실패는 실제 사용 패턴에서만 나타난다.\n참고 자료:\nApache Superset - SqlLab overwrite dataset fix Apache Superset - Helm chart db-init fix PostgreSQL: idle_in_transaction_session_timeout Python: utf-8-sig encoding ","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/superset-user-reported-issues/","section":"글 목록","summary":"데이터 거버넌스팀에서 Superset 사용 중 발견한 이슈 6건을 대응했다. Slack 스크린샷 확장자 누락, 대시보드 자동 새로고침, SqlLab 데이터셋 덮어쓰기 버그, 윈도우 CSV 한글 깨짐, 쿼리 결과 행 제한, CSV 다운로드 500 에러까지. 가장 까다로웠던 건 50만 행 이상 CSV 다운로드 시 PostgreSQL idle_in_transaction_session_timeout이 메타데이터 DB 커넥션을 끊어버리는 문제였다.","title":"Superset 사용자 제보 이슈 6건 대응기","type":"posts"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/trino/","section":"Tags","summary":"","title":"Trino","type":"tags"},{"content":"OLAP 환경에서 Trino를 메인 쿼리 엔진으로 쓰고 있었다. ClickHouse 도입을 검토하던 중 기술 스택 단일화 측면에서 Trino를 개선하는 게 낫겠다는 판단이 섰다. 그래서 Trino 버전을 v433에서 v451로 올렸다. 주목적은 Alluxio 기반 캐시 기능 활용이었다.\n이전에 쓰던 Rubix 캐시는 deprecated 상태였고 Alluxio 캐시는 v439에서 추가된 뒤 v445에서 race condition 버그가 수정되면서 쓸 만한 상태가 됐다.\n이 글은 Blue/Green 배포 환경에서 Alluxio 캐시를 PoC한 과정과 그 과정에서 마주친 병목을 정리한 기록이다.\nAlluxio 캐시가 뭔가 # Trino가 S3에서 데이터를 읽을 때마다 네트워크를 타야 한다. 같은 파일을 여러 번 읽어도 매번 S3 API를 호출한다. Alluxio 캐시는 한 번 읽은 데이터를 워커 노드의 로컬 디스크에 저장해서 두 번째부터는 로컬에서 읽는 방식이다.\n카탈로그 설정에 몇 줄 추가하면 동작한다.\nfs.cache.enabled=true fs.cache.max-sizes=50GB fs.cache.directories=/mnt/cache Hive, Iceberg, Delta Lake 커넥터에서 사용할 수 있다. 우리는 hive_zeppelin과 iceberg 두 카탈로그에 적용했다.\n제약사항 # PoC를 시작하기 전에 알아둬야 할 제약이 있었다.\n캐시가 카탈로그/클러스터 간에 공유되지 않는다 # 같은 S3 버킷을 바라보는 카탈로그가 여럿 있어도 캐시는 각 카탈로그별로 따로 관리된다. 커넥터 구조상 파일 시스템을 카탈로그마다 독립적으로 인스턴스화하기 때문이다. 클러스터 간 공유는 당연히 안 된다.\n이 문제를 해결하려는 네이티브 Alluxio 파일 시스템 PR이 2024년 9월에 머지됐다(Trino 460). 카탈로그 간, 클러스터 간 캐시 공유가 가능해진다. 다만 공유 캐시 파일 시스템 접근이 S3 직접 접근보다 빠를지는 별도 검증이 필요하다.\n스팟 인스턴스와 캐시는 궁합이 안 맞다 # 워커의 캐시 데이터 생애주기는 해당 워커 노드의 생애주기와 같다. 노드가 사라지면 캐시도 증발한다.\n문제는 우리 환경이 거의 99% 스팟 노드라는 거다. 스팟 회수가 일어나면 해당 워커의 캐시가 통째로 날아간다. 오토스케일링으로 스케일인될 때도 마찬가지다. 캐시 웜업에 시간이 걸리는데 노드가 자주 바뀌면 웜업 효과를 보기 어렵다.\n스키마 단위 제어가 안 된다 # 캐시 활성화는 카탈로그 단위로 all-or-nothing이다. temp 스키마처럼 재사용 빈도가 거의 없는 테이블도 전부 캐싱된다. 주요 스키마만 선택적으로 캐싱할 수 있으면 좋겠지만 현재는 지원하지 않는다.\nfs.cache.skip-paths 옵션을 추가하는 PR이 있었지만 설계 방향에 대한 의견 차이로 닫혔다. 파일 경로 기반이 아니라 스키마/테이블 단위로 제어해야 한다는 게 리뷰어의 의견이었다.\nPoC 구성 # Blue/Green 중 한쪽에만 적용 # 비용 문제로 스테이지 환경을 프로덕션과 동일하게 구축하기 어려웠다. 그래서 OLAP과 BI 클러스터의 Blue/Green 중 Blue 클러스터에만 캐시를 적용하고 1~2주간 쿼리 성능 지표를 비교하기로 했다.\n앞단에 Trino Gateway가 있어서 각 백엔드의 현재 쿼리 수를 기준으로 부하를 분산한다. 유입되는 쿼리가 완전히 동일하진 않지만 2주 정도 데이터가 쌓이면 유의미한 비교가 된다고 판단했다.\n볼륨 구성 # 워커 팟과 노드를 1:1로 매핑하는 게 Trino 베스트 프랙티스다. StatefulSet + volumeClaimTemplate으로 팟 단위 볼륨을 구성할 수도 있지만 우리는 노드에 캐시 전용 EBS 볼륨을 마운트하고 워커 팟에서 hostPath로 접근하게 구성했다.\n초기 캐시 볼륨은 워커당 50GB. hive_zeppelin 카탈로그에 60%, iceberg 카탈로그에 30%를 할당했다.\n적용 순서 # OLAP Blue 클러스터에 선적용 (2024-07-30) BI Blue 클러스터에 선적용 (2024-07-31) Grafana에 p25/p50/p75/p99/avg/max 쿼리 수행시간 비교 패널 추가 JMX 메트릭으로 캐시 사용 현황 모니터링 BI 클러스터는 같은 대시보드 내 여러 차트가 동일 테이블의 같은 날짜 범위를 조회하는 경우가 많아서 캐시 효과가 더 클 거라 기대했다.\nEBS 스루풋 병목 # 캐시를 켜고 며칠 지켜봤다. 드라마틱한 차이는 없었다. 캐시 적중률은 괜찮은데 왜 성능 차이가 안 나지?\n캐시 볼륨 메트릭을 까봤더니 원인이 보였다. EBS 쓰기 스루풋이 설정된 최대치인 125MiB/s에 계속 도달하고 있었다. gp3 볼륨의 기본 스루풋이 125MiB/s인데 캐시 쓰기가 이 한도를 꽉 채우고 있던 거다.\n캐시가 아무리 빨라도 디스크 쓰기가 병목이면 소용이 없다.\n스루풋 상향 # gp3에서 설정 가능한 최대 스루풋인 1000MiB/s로 올렸다.\nBI 클러스터: 오후 2시 45분경 적용 OLAP 클러스터: 오후 3시 45분경 적용 결과가 바로 나타났다. 스루풋 상향 후 캐시 적용 클러스터(Blue)의 쿼리 수행 지표가 확실히 좋아졌다. 쓰기 스루풋이 200MiB 정도까지 올라가도 최대치(1000MiB)에는 한참 여유가 있었다. 읽기까지 합치면 대략 300MiB 수준이었다.\ngp3 스루풋만 올리면 됐지 io2 같은 고성능 볼륨 타입은 필요 없었다. io2의 최대 스루풋도 1000MiB/s로 동일하고 높은 IOPS가 필요한 워크로드도 아니었으니까.\n캐시 볼륨 사이징 # 초기 50GB는 금방 꽉 찼다. 허용된 최대 캐시 사이즈(hive 60% + iceberg 30% = 45GB)를 모두 채워 쓰고 있었다. 캐시 공간이 부족하면 오래된 데이터부터 밀려나는데 너무 빨리 밀려나면 캐시 효과가 떨어진다.\n볼륨 사이즈를 키웠다.\n클러스터 변경 전 변경 후 BI 워커 50GB 150GB → 1.5TB OLAP 워커 50GB 2TB OLAP 코디네이터 - 1TB (iceberg 메타데이터 캐시용) 카탈로그별 할당 비율도 실제 사용량을 보고 조정했다. hive가 iceberg보다 캐시를 훨씬 많이 쓰고 있어서 hive 60% → 85%, iceberg 30% → 10%로 재배분했다.\nEBS 스토리지 비용이 걱정됐지만 확인해보니 월 비용이 크지 않았다. 캐시 덕분에 워커 수가 20% 줄어든 효과가 EBS 비용 증가분을 상쇄하고도 남았다.\n필요 이상으로 크게 잡아놓고 Grafana에 각 노드의 캐시 볼륨 사용률 차트를 추가해서 지켜본 뒤 낭비되면 줄이는 방식으로 접근했다.\nFIFO가 LRU보다 나을 수 있다 # Trino의 Alluxio 캐시는 LRU 같은 정교한 교체 알고리즘이 아니라 FIFO 방식으로 동작한다. 가장 먼저 캐시된 데이터를 먼저 내보낸다.\n직관적으로는 LRU가 나아 보이지만 FIFO가 유리한 경우도 있다.\nOne-hit wonder 제거: 한 번만 읽히고 다시 안 읽히는 데이터를 빠르게 밀어낸다. LRU는 최근 접근 시간을 갱신하느라 이런 데이터가 캐시에 더 오래 남을 수 있다. SSD 친화적: FIFO는 랜덤 액세스를 최소화한다. SSD나 플래시 메모리 기반 스토리지에서 쓰기 증폭(write amplification)이 적다. 초기 단계 기능이라 교체 알고리즘이 단순한 건 맞지만 OLAP 워크로드에서 FIFO가 반드시 나쁜 선택은 아니다.\n결과 # 쿼리 성능 # 캐시 활성화 + EBS 스루풋 상향 이후 Blue 클러스터가 Green 대비 일관되게 빠른 경향을 보였다.\n흥미로운 건 Blue가 더 적은 워커로 더 높은 쿼리 스루풋을 기록했다는 점이다. 쿼리를 빨리 처리하니까 앞단 게이트웨이가 더 많은 쿼리를 라우팅해줬고 그 결과 Green보다 워커 수가 20% 적은 상태에서도 처리량이 더 높았다.\n캐시 웜업에는 약 3시간이 걸렸다. 워커가 새로 뜨고 캐시가 충분히 찰 때까지 이 시간이 지나야 성능 차이가 드러났다.\nS3 API 비용 절감 # 캐시 적용 전후로 S3 GetObject 비용이 월 약 440만 원 줄었다. 캐시 적용 외에도 쿼리 수 변동 같은 다른 요인이 영향을 줬을 수 있지만 비용 절감 규모가 꽤 컸다.\n비용 정리 # 항목 변화 S3 API 비용 월 ~440만 원 절감 EBS 스토리지 비용 소폭 증가 (캐시 볼륨) 노드 비용 워커 수 ~20% 감소로 절감 인스턴스 스토어 검토 # EBS는 리모트 블록 스토리지다. S3 직접 접근 대비 캐시 성능 개선이 드라마틱하지 않을 수 있다. 만약 EBS 캐시로 충분한 효과가 없었다면 NVMe SSD 인스턴스 스토어가 달린 노드 타입으로 교체할 계획이었다.\n인스턴스 타입 인스턴스 스토어 r7gd.4xlarge 1 x 950GB NVMe SSD m7gd.8xlarge 1 x 1,900GB NVMe SSD 두 타입의 인스턴스 스토어 크기가 다르지만 Trino의 캐시 설정이 전체 디스크 용량 대비 퍼센트로도 지정할 수 있어서 문제가 되지 않는다. 다만 EBS 스루풋 상향만으로도 충분한 효과를 봤기 때문에 인스턴스 스토어로 교체는 진행하지 않았다.\n같이 알아두면 좋은 것: Project Hummingbird # Trino v451로 올리면서 눈여겨본 게 하나 더 있다. Project Hummingbird라는 성능 개선 프로젝트다.\nJava 22의 Vector API를 활용한 벡터화 연산을 Trino에 적용하는 작업이다. Parquet 파일 읽기에 벡터화 디코딩이 v448부터 반영됐다. 단 256비트 이상 벡터 레지스터가 필요해서 Graviton 2 인스턴스(r6g, m6g)에서는 비활성화된다. Graviton 3 이상이면 자동으로 켜진다.\nTrino가 v447부터 Java 22를 필수로 요구하게 된 것도 이 프로젝트의 일환이다.\n마치며 # Alluxio 캐시 PoC에서 배운 건 단순하다.\n캐시를 붙이는 것만으로는 안 된다. 디스크 I/O가 병목이면 캐시 적중률이 아무리 높아도 체감 성능이 안 나온다. EBS gp3 기본 스루풋 125MiB/s가 캐시 성능의 천장이었다. 1000MiB/s로 올리자마자 차이가 드러났다.\nBI 워크로드에 캐시가 잘 먹힌다. 같은 테이블을 반복 조회하는 대시보드 쿼리는 캐시 히트율이 높다.\n스팟 환경에서 캐시는 타협이 필요하다. 노드가 수시로 바뀌면 캐시 웜업 시간이 사실상 손실이다. 그래도 3시간 웜업 후에는 효과가 나타났으니 완전히 무용한 건 아니다.\n앞으로 네이티브 Alluxio 파일 시스템(Trino 460)이 안정화되면 카탈로그/클러스터 간 캐시 공유가 가능해진다. 스팟 회수 시 캐시 손실 문제도 완화될 수 있을 거다.\n참고 자료:\nA cache refresh for Trino Trino File System Cache Documentation Alluxio cache PR (v439) Cache race condition fix (v445) Cross-catalog cache sharing issue Native Alluxio file system PR (v460) Project Hummingbird Vectorized Parquet decoding PR (v448) ","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/trino-alluxio-cache-poc/","section":"글 목록","summary":"Trino의 Alluxio 기반 파일 시스템 캐시를 프로덕션 OLAP 환경에서 PoC했다. 캐시를 붙이기만 하면 빨라질 줄 알았는데 EBS 기본 스루풋 125MiB/s가 병목이었다. 스루풋 상향 후 쿼리 성능이 눈에 띄게 개선됐고 S3 API 비용도 월 440만 원 줄었다.","title":"Trino Alluxio 캐시 PoC: EBS 스루풋이 병목이었다","type":"posts"},{"content":"Trino 코디네이터는 단일 장애점이다. HA를 지원하지 않아서 코디네이터가 내려가면 클러스터 전체가 쿼리를 받지 못한다. 프로덕션에서 이게 문제가 되는 순간은 크게 두 가지다.\n하나는 노드 교체. Kubernetes 환경에서 Karpenter로 노드를 관리하는데 장기간 실행 중인 노드는 네트워크 이슈가 생길 수 있다. ttlSecondsUntilExpired 옵션으로 주기적으로 교체하고 싶지만 코디네이터가 올라간 노드가 교체되는 시점을 제어하기 어렵다.\n다른 하나는 배포. 코디네이터 팟을 재배포하면 새 팟이 뜰 때까지 다운타임이 발생한다. 새벽에 CronJob으로 돌리는 방법도 있지만 그 시간대에 실행 중인 배치 쿼리가 실패할 수 있다.\nLyft에서 이 문제를 presto-gateway로 해결했다는 걸 Deview 발표에서 알게 됐다. 여러 Trino 클러스터 앞에 게이트웨이를 두면 Blue/Green 배포가 가능해지고 다운타임 없이 코디네이터를 교체할 수 있다.\nTrino Gateway란 # Trino Gateway는 여러 Trino 클러스터 앞에 놓는 로드 밸런서이자 라우팅 프록시다. 원래 Lyft가 presto-gateway라는 이름으로 개발했고 지금은 trinodb 조직 아래에서 trino-gateway로 활발하게 유지보수되고 있다.\n핵심 기능은 세 가지다.\n멀티 클러스터 라우팅: 쿼리를 조건에 따라 다른 클러스터 그룹으로 보낼 수 있다 백엔드 헬스 체크: 뒤에 있는 클러스터가 정상인지 주기적으로 확인하고 장애 클러스터를 자동으로 제외한다 큐 체크: 각 백엔드의 현재 쿼리 수를 기준으로 부하를 분산한다 클라이언트 입장에서는 게이트웨이 주소 하나만 알면 된다. 뒤에 클러스터가 몇 개인지, 어느 클러스터가 살아있는지 신경 쓸 필요가 없다.\n아키텍처 # 최종 목표 구성은 이렇다.\nTrino Gateway ├── BI Cluster Group │ ├── BI Cluster 1 (Coordinator: B존) │ └── BI Cluster 2 (Coordinator: C존) │ └── OLAP Cluster Group ├── OLAP Cluster 1 (Coordinator: A존) └── OLAP Cluster 2 (Coordinator: B존) 코디네이터를 서로 다른 가용 영역(AZ)에 분산시킨 건 의도적이다. 과거에 특정 AZ의 노드 프로비저닝 장애로 코디네이터가 뜨지 못한 적이 있어서다. AZ를 분산하면 한 AZ가 장애를 겪어도 다른 AZ의 클러스터가 쿼리를 받을 수 있다.\n운영 방식 # 리소스 낭비를 줄이기 위해 각 클러스터 그룹에서 동시에 하나의 클러스터만 활성화한다. 롤링 배포 시점에만 일시적으로 두 클러스터가 동시에 뜬다.\n평소: Gateway → Cluster 1 (활성) Cluster 2 (비활성) 롤링 중: Gateway → Cluster 1 (활성) + Cluster 2 (부팅 중) 완료 후: Gateway → Cluster 2 (활성) Cluster 1 (비활성) 게이트웨이가 헬스 체크로 새 클러스터가 준비됐음을 확인하면 트래픽을 넘기고 기존 클러스터의 실행 중인 쿼리가 끝나길 기다린 뒤 비활성화한다.\n헤더 기반 라우팅 룰 # 쿼리 소스에 따라 적절한 클러스터 그룹으로 라우팅한다. Trino 클라이언트가 보내는 HTTP 헤더를 기준으로 분기한다.\n# Superset → BI 클러스터 - name: \u0026#34;superset\u0026#34; condition: \u0026gt; request.getHeader(\u0026#34;X-Trino-Source\u0026#34;) == \u0026#34;Apache Superset\u0026#34; \u0026amp;\u0026amp; request.getHeader(\u0026#34;X-Trino-Client-Tags\u0026#34;) == null actions: - \u0026#34;result.put(\\\u0026#34;routingGroup\\\u0026#34;, \\\u0026#34;bi\\\u0026#34;)\u0026#34; # Querybook → OLAP 클러스터 - name: \u0026#34;querybook\u0026#34; condition: \u0026gt; request.getHeader(\u0026#34;X-Trino-Source\u0026#34;) == \u0026#34;trino-python-client\u0026#34; \u0026amp;\u0026amp; request.getHeader(\u0026#34;X-Trino-Client-Tags\u0026#34;) == null actions: - \u0026#34;result.put(\\\u0026#34;routingGroup\\\u0026#34;, \\\u0026#34;olap\\\u0026#34;)\u0026#34; # Zeppelin → OLAP 클러스터 - name: \u0026#34;zeppelin\u0026#34; condition: \u0026gt; request.getHeader(\u0026#34;X-Trino-Source\u0026#34;) ~= \u0026#34;^zeppelin-.+\u0026#34; actions: - \u0026#34;result.put(\\\u0026#34;routingGroup\\\u0026#34;, \\\u0026#34;olap\\\u0026#34;)\u0026#34; X-Trino-Source 헤더는 Trino 클라이언트가 자동으로 붙여준다. Superset은 Apache Superset을 보내고 Querybook은 trino-python-client를 보낸다. Zeppelin은 zeppelin-으로 시작하는 소스명을 쓴다.\nX-Trino-Client-Tags가 null인 조건을 추가한 건 특정 태그가 붙은 쿼리를 별도 처리하기 위한 여지를 남겨둔 것이다.\n백엔드 헬스/큐 체크 정상화 # 게이트웨이를 배포하고 나서 백엔드 헬스 체크와 큐 체크에 문제가 있음을 발견했다.\n헬스 체크 문제 # 게이트웨이가 백엔드 클러스터의 상태를 확인할 때 Trino의 /v1/info 엔드포인트를 찌른다. 그런데 코디네이터가 시작 중인 상태에서도 이 엔드포인트가 200을 반환하는 경우가 있었다. 게이트웨이는 클러스터가 준비됐다고 판단하고 쿼리를 보내는데 실제로는 아직 쿼리를 처리할 수 없는 상태였다.\n큐 체크 문제 # 각 백엔드의 현재 실행 중인 쿼리 수와 대기 중인 쿼리 수를 확인해서 부하를 분산하는 로직이 제대로 동작하지 않고 있었다. 특정 클러스터에 쿼리가 쏠리는 현상이 발생했다.\n수정 내용 # 게이트웨이 코드와 Trino 클러스터 설정 양쪽을 수정했다.\n게이트웨이 쪽: 헬스 체크 로직을 보강해서 코디네이터가 완전히 준비된 상태인지 확인하도록 수정. 큐 체크에서 쿼리 수 기반 부하 분산이 정확하게 동작하도록 수정 Trino 클러스터 쪽: 차트 템플릿과 설정을 변경해서 게이트웨이와의 연동이 올바르게 동작하도록 조정 매일 돌리는 클러스터 롤링 배치 # 게이트웨이 도입의 가장 큰 이점은 주간 시간대에도 다운타임 없이 클러스터를 교체할 수 있다는 점이다.\nAirflow DAG으로 매일 클러스터 롤링 배치를 구성했다. 동작 방식은 이렇다.\n비활성 클러스터의 새 코디네이터와 워커를 띄운다 게이트웨이 헬스 체크가 새 클러스터를 정상으로 판단할 때까지 대기한다 새 클러스터가 준비되면 게이트웨이가 새 쿼리를 새 클러스터로 라우팅한다 기존 클러스터에서 실행 중인 쿼리가 완료될 때까지 기다린다 기존 클러스터를 비활성화한다 이렇게 하면 장기 실행 노드에서 생기는 네트워크 이슈를 예방하면서도 실행 중인 쿼리에 영향을 주지 않는다.\n도입 과정 # 단계적으로 진행했다.\n1단계. 테스트 환경 PoC # 테스트 환경에서 게이트웨이를 먼저 구축하고 기본적인 라우팅과 헬스 체크 동작을 확인했다.\n2단계. 스테이지 환경 구축 # 프로덕션 환경(D01)에 스테이지를 구축하고 Superset, Querybook 등 실제 클라이언트 연동을 검증했다. 이 단계에서 헬스 체크와 큐 체크 문제를 발견하고 수정했다.\n3단계. 프로덕션 적용 # Superset, Querybook, Zeppelin에 우선 적용했다. 게이트웨이 주소로 엔드포인트를 전환하고 라우팅이 정상 동작하는지 모니터링했다.\n4단계. 롤링 배치 운영 # 매일 클러스터 롤링 배치를 Airflow DAG으로 운영하기 시작했다. Beta 환경에서 정상 동작을 확인한 뒤 프로덕션에 적용했다.\n게이트웨이 내부 구조 # Stateful 라우팅과 MetaDB # 게이트웨이는 단순한 리버스 프록시가 아니다. Trino 쿼리는 제출 후에도 상태 확인, 결과 조회 등 후속 요청이 같은 코디네이터로 가야 한다. 게이트웨이는 query_id별로 어느 백엔드에 라우팅했는지를 MetaDB(MySQL)에 저장한다. 후속 요청이 들어오면 MetaDB를 조회해서 같은 백엔드로 보낸다.\n인증 연동 # 게이트웨이 팟에 nginx와 nginx-ldap를 사이드카로 띄워서 LDAP 인증을 연동했다. 게이트웨이 앞에 인증 레이어를 두는 방식이다. 별도의 인증 엔드포인트를 통해 사용자 인증 후 게이트웨이로 요청이 전달된다.\nHelm 차트 구성 # Base + 클러스터별 Values 분리 # Blue/Green 두 클러스터를 관리하려면 Helm values를 잘 나눠야 한다. CI 러너에서 쓰던 방식을 참고해서 base yaml과 클러스터별 yaml을 분리했다.\nvalues.prod.base.yaml # 공통 설정 values.prod.blue.yaml # Blue 클러스터 전용 (AZ, 노드 셀렉터 등) values.prod.green.yaml # Green 클러스터 전용 배포 시 base와 클러스터별 yaml을 머지해서 적용한다. 공통 설정 변경은 base만 수정하면 되고 클러스터별 차이는 개별 yaml에서 관리한다.\nArgoCD 독립 앱 등록 # Blue와 Green 클러스터를 ArgoCD에 별도 앱으로 등록했다. 두 앱이 독립적으로 관리되니까 한쪽만 업데이트하거나 한쪽만 비활성화하는 게 자유롭다. 롤링 배포 시 Green을 먼저 올리고 Blue를 내리는 식의 제어가 ArgoCD UI에서 바로 가능하다.\n프로덕션 적용 전 선행 작업 # Audit 로그 분리와 통합 # Blue/Green 클러스터가 각각 독립된 쿼리 엔진이다 보니 audit 로그도 클러스터별로 나뉜다. 기존에는 단일 클러스터의 audit 로그만 보면 됐는데 이제는 두 클러스터의 로그를 합쳐서 봐야 한다.\n두 가지 방안을 검토했다.\n클러스터별 audit 로그 테이블을 따로 두고 유니온 뷰를 제공 기존 audit 로그 테이블에 소스 클러스터 필드를 추가하고 한 테이블에 적재 기존 Airflow audit 로그 덤프 DAG도 수정이 필요했고 클러스터별로 EFS 볼륨(Exchange Manager용 포함)도 추가로 생성해야 했다.\n부하 테스트 # 게이트웨이가 앞단에 추가되면서 쿼리 라우팅에 오버헤드가 생긴다. 운영 워크로드를 소화할 수 있는지 검증하기 위해 실제 프로덕션 쿼리를 덤프해서 재생하는 스크립트로 부하 테스트를 수행했다.\nActive-Active 검토 # 현재는 클러스터 그룹당 하나의 클러스터만 활성화하는 Active-Standby 방식이다. 본래 목적이 무중단 배포였으니까 이걸로 충분하다.\n한편 같은 클러스터 그룹 안에 여러 코디네이터를 동시에 활성화하는 Active-Active 방식도 검토했다. 코디네이터 한 대의 처리 한계를 넘는 워크로드가 들어오면 필요해질 수 있다.\n기본 설정에서는 쿼리가 랜덤하게 배정된다. 엄밀한 라운드 로빈은 아니다. 실제 운용하려면 개별 클러스터의 부하량에 따라 라우팅을 조절해야 한다. 현재 큐 체크로 쿼리 수 기반 분산이 가능하지만 정교한 부하 인지 라우팅은 추가 작업이 필요하다.\n당장은 Active-Standby로 운영하되 워크로드가 커지면 Active-Active 전환을 고려할 계획이다.\n마치며 # Trino Gateway 도입으로 해결한 문제를 정리하면 이렇다.\n제로 다운타임 배포. 코디네이터 재배포 때 다운타임이 사라졌다. Blue/Green 방식으로 새 클러스터가 준비된 후 트래픽을 넘기니까 쿼리 하나 떨어뜨리지 않고 교체할 수 있다.\nAZ 장애 내성. 코디네이터를 서로 다른 AZ에 분산시켜서 단일 AZ 장애에도 쿼리를 처리할 수 있게 됐다.\n워크로드 분리. BI 쿼리는 BI 클러스터로, OLAP 쿼리는 OLAP 클러스터로 자동 라우팅된다. 클라이언트는 게이트웨이 주소 하나만 알면 된다.\n노드 신선도 유지. 매일 클러스터를 롤링해서 장기 실행 노드에서 발생하는 네트워크 이슈를 예방한다.\n헬스 체크와 큐 체크 로직을 수정하는 데 시간이 좀 들었지만 그 과정을 거치고 나니 게이트웨이가 안정적으로 동작한다. Trino를 프로덕션에서 운영한다면 게이트웨이는 선택이 아니라 필수에 가깝다.\n참고 자료:\nTrino Gateway (trinodb) Presto Infrastructure at Lyft Trino Open Source Infrastructure Upgrading at Lyft Presto Gateway (Lyft, legacy) ","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/trino-gateway-zero-downtime/","section":"글 목록","summary":"Trino는 코디네이터 HA를 지원하지 않는다. 코디네이터를 재배포하면 다운타임이 생긴다. Trino Gateway를 도입해서 Blue/Green 배포로 제로 다운타임을 달성하고 BI/OLAP 클러스터를 헤더 기반으로 라우팅한 과정을 정리했다.","title":"Trino Gateway 도입기: 제로 다운타임 배포와 멀티 클러스터 라우팅","type":"posts"},{"content":"Trino v451에서 v476으로 업그레이드를 진행했다. v476은 2025년 6월 5일에 릴리즈됐다. 25개 마이너 버전을 건너뛰는 업그레이드라서 변경사항이 방대했고, 실제로 배포 과정에서 두 건의 리그레션을 만났다. 그중 하나는 커뮤니티에도 보고된 적 없는 새로운 이슈였다.\n이 글은 업그레이드에서 기대한 것, 실제로 겪은 것, 그리고 리그레션을 추적하고 해결한 과정을 정리한 기록이다.\n왜 v476인가 # Java 22에서 Java 24로 # Trino는 최신 JVM에 적극적으로 올라타는 프로젝트다. v447에서 Java 22를 요구하기 시작했고, v464에서 Java 23, v476에서 Java 24로 올라갔다. 단순히 JVM 버전만 올라가는 게 아니다. Project Hummingbird라는 이름의 성능 개선 프로젝트가 Java의 Vector API를 활용해서 Parquet 파일 읽기에 벡터화 디코딩을 적용하고 있다. v448에서 Parquet 벡터화 디코딩이 도입됐고, 256비트 이상의 벡터 레지스터가 필요해서 Graviton 2(r6g, m6g)에서는 비활성화되지만 Graviton 3 이상에서는 자동 활성화된다.\nJava 24의 최신 JIT 컴파일러 최적화와 메모리 관리 개선이 쿼리 처리 성능에 직접 영향을 준다.\nS3 테이블 버킷 읽기 지원 # v471부터 Iceberg 커넥터에서 S3 테이블 버킷에 대한 읽기가 가능해졌다. 매니지드 Iceberg 테이블인 S3 테이블 버킷의 도입을 검토하고 있었기 때문에, Trino에서 읽기를 지원하는 건 필수 전제 조건이었다. Glue Data Catalog의 Iceberg REST 엔드포인트를 통해 연동하며, 기존 glue 타입이 아닌 rest 타입 카탈로그를 별도 구성해야 한다.\n버킷 파티션 푸시다운 # v468에 추가된 파티셔닝 푸시다운이 있다. 버킷 파티션이 걸린 테이블에 대해 쿼리 조건을 파티션 레벨로 푸시다운해서 스캔 범위를 줄이는 최적화다.\n우리 환경에서 앱 로그와 웹 로그 테이블이 screen_name에 버킷 파티션을 사용하고 있다. 이 테이블들은 규모가 크기 때문에 버킷 파티션 푸시다운의 효과가 클 것으로 기대했다.\nIceberg 메타테이블 및 프로시저 개선 # Iceberg 테이블 모니터링을 구축하면서 v451에 체리피킹했던 픽스들이 v476에 네이티브로 포함된다.\n버전 개선 내용 v455 $files 메타테이블에서 delete file 인식 버그 수정 v465 $files 메타테이블에 partition 필드 추가 v466 Glue Catalog 조회 병렬화로 성능 개선 v469 $all_entries 메타테이블 추가 (Spark의 all_entries에 대응) v470 $all_entries 버그 수정, optimize_manifests 프로시저 추가 v475 system.iceberg_tables 테이블 추가 (Iceberg 테이블만 리스팅) 체리피킹 없이 이 기능들을 모두 쓸 수 있게 되는 것만으로도 업그레이드의 가치가 있었다.\nAlluxio 파일 시스템 캐시 # 기존에 사용하던 Alluxio 기반 로컬 디스크 캐시와 별개로, 외부 Alluxio 클러스터를 파일 시스템으로 연동하는 기능이 추가됐다. 카탈로그 간, 클러스터 간 캐시 공유가 가능해진다. 당장 적용할 계획은 아니지만 향후 캐시 아키텍처 개선에 활용할 수 있는 선택지다.\nApache Ranger 플러그인 # Apache Ranger 인가 플러그인이 Trino 공식 레포에 통합됐다. 기존에 별도 레포로 관리되던 것이 메인 프로젝트에 포함돼서 버전 호환성 관리가 수월해진다.\n배포 전략 # Blue/Green 점진 배포 # OLAP과 BI 두 클러스터 그룹 각각에 Blue/Green 쌍이 있다. 업그레이드 계획은 이랬다.\n각 클러스터 그룹의 한쪽(Green)에 먼저 배포 1주일간 모니터링 이상 없으면 나머지(Blue)에 적용 이슈 발생 시 Trino Gateway에서 해당 클러스터를 라우팅에서 즉시 빼고 롤백할 수 있다. 게이트웨이를 운영하고 있기 때문에 가능한 전략이다. 실제로 이 전략 덕분에 리그레션을 발견하고도 운영에 영향 없이 대응할 수 있었다.\nOLAP-Green 클러스터부터 시작하기로 했다. BI보다 OLAP이 쿼리 패턴이 다양해서 리그레션을 더 빨리 발견할 수 있을 거라 판단했다.\nv451 때부터 이어진 미해결 이슈 # v451 배포 때 문제가 됐던 이슈 두 가지가 여전히 해결되지 않은 상태였다. 업그레이드 전에 대응 방안을 준비해뒀다.\n워커 stuck 이슈\nv451 배포 당시 부하 상황에서 일부 워커가 멈추는 현상이 있었다. 원인은 ThreadPerDriverTaskExecutor라는 실험적 기능이었다. 별다른 문서화도 없이 디폴트로 활성화된 채 추가됐었다. 커뮤니티에 이슈가 올라와 있지만 아직 해결되지 않았다.\n이번에도 동일하게 experimental.thread-per-driver-scheduler-enabled=false로 비활성화해서 대응할 계획이었다.\n파티션 프루닝 리그레션\n필터 조건에 형변환이 포함된 경우 파티션 프루닝이 동작하지 않는 이슈다. 예를 들어 WHERE date_column = CAST('2024-01-01' AS DATE) 같은 쿼리에서 파티션이 제대로 걸러지지 않는다.\nv451 때는 리그레션을 유발한 커밋 자체를 리버트해서 대응했다. 그 사이에 Trino 커뮤니티에서 이 리그레션을 우회하는 escape hatch 설정이 추가됐다. unsafe pushdown을 허용하는 설정인데, 이번에는 이 설정을 적용해서 리그레션이 발생하지 않는지 확인하는 방향으로 진행했다.\n배포 개시와 첫 번째 롤백 # 2025년 6월 23일, 코어타임 이후에 OLAP-Green 클러스터에 v476을 배포했다. 사용자들에게 Zeppelin 지원 채널을 통해 사전 공지를 했다.\nMaterialized View REFRESH stuck # 배포 직후 Airflow에서 Iceberg materialized view를 REFRESH하는 쿼리가 stuck 되고 있다는 제보가 들어왔다. 쿼리가 FINISHING 상태에서 끝나지 않고 멈춰 있었다.\nTrino 레포를 확인하니 v475에 추가된 \u0026ldquo;Cleanup previous snapshot files during materialized view refresh\u0026quot;라는 변경이 리그레션을 유발한 것으로 확인됐고, 바로 전날 리버트 PR이 머지된 상태였다. 타이밍이 절묘했다. 해당 리버트 커밋을 포크 레포에 반영해서 이슈를 해소했다.\nParquet 파일 데이터 미인식 # 더 심각한 이슈가 뒤따랐다. 사용자가 JupyterLab에서 쿼리 결과가 이상하다고 제보했다. 확인해보니 특정 Hive 테이블에서 데이터가 아예 조회되지 않았다.\n문제의 패턴은 이랬다. 사용자가 Parquet 파일을 S3에 미리 적재해두고, 해당 경로를 바라보는 Hive 테이블을 나중에 생성하는 방식이었다. v451에서는 정상적으로 동작하던 패턴인데, v476에서는 테이블 생성 전에 올려둔 데이터 파일을 인식하지 못했다.\n확인한 사항들:\n데이터 파일을 어디서 생성했든(Trino v451, v476, Spark) 동일하게 재현 Glue API 버전(v1, v2)과 무관 Spark의 MSCK REPAIR TABLE이나 Trino의 system.sync_partition_metadata() 호출로도 파티션 데이터 인식 불가 이 이슈의 영향이 크다고 판단해서 OLAP-Green을 v451로 롤백했다. 원인을 파악한 뒤 재배포하기로 했다.\n바이너리 서치로 원인 버전 특정 # 커뮤니티에 이 이슈가 보고된 적이 없었다. 직접 찾아야 했다.\nv451과 v476 사이에 메타스토어와 Hive 커넥터에 대한 코드 변경이 매우 많았다. 릴리즈 노트만 훑어봐서는 원인을 특정할 수 없었다. 바이너리 서치 방식으로 접근했다.\nZeppelin에 테스트 노트북을 만들어서 각 버전에서 이슈 재현 여부를 체계적으로 확인했다.\nv451 — 정상 ↓ (중간 지점 테스트) v463 — 정상 ↓ v469 — 재현! ↓ (범위 좁힘) v466 — 정상 v467 — 정상 v468 — 정상 v469 — 재현! v469에서 리그레션이 시작됨을 확인했다.\n다음은 v469의 Hive 커넥터 변경사항 중 어떤 커밋이 원인인지 찾는 작업이었다. v469 릴리즈 노트에서 Hive 커넥터 관련 변경사항을 확인하고, 해당 PR들을 하나씩 리버트하면서 이슈 재현 여부를 확인했다.\n원인: Parquet Footer 최적화와 레거시 pyarrow # 원인은 v469에 머지된 Parquet footer 파싱 최적화 PR이었다. 이 PR은 Parquet 파일의 footer 메타데이터를 읽을 때 RowGroup.getFile_offset 값을 신뢰하고 그 값을 기반으로 읽기 범위를 최적화하는 로직을 추가했다.\n문제는 모든 Parquet 파일의 row group 오프셋이 정확하지는 않다는 점이었다. 이슈가 된 파일은 pandas를 통해 pyarrow v5.0.0(parquet-cpp-arrow v5.0.0)으로 작성된 Parquet 파일이었다. 이 레거시 버전에서 기록한 row group 오프셋에 오류가 있었다.\n추가 테스트로 확인한 사항:\n최신 pyarrow 버전으로 작성한 파일에서는 재현되지 않는다 row group의 크기가 일정 규모 이상이 되어야만 오프셋 오류가 발생한다. 작은 파일에서는 재현되지 않았다 v469 이전 버전에서는 getFile_offset 값을 사용하지 않았기 때문에 오프셋이 부정확해도 문제가 없었다 커뮤니티 이슈 제보와 하루 만의 픽스 # 원인 PR과 재현 방법을 정리해서 Trino 커뮤니티 Slack에 공유했다. \u0026ldquo;Incorrect results on parquet files written by parquet-cpp-arrow version 5.0.0\u0026quot;이라는 제목으로 이슈도 올렸다.\n원래 최적화 PR의 작성자가 바로 근본 원인을 파악했다. 원인은 명확했다. 최적화 로직이 RowGroup.getFile_offset 값이 항상 정확하다고 가정하고 있었는데, 레거시 라이터가 부정확한 값을 기록하는 경우가 있었다.\n픽스는 RowGroup.getFile_offset 대신 ColumnChunk의 first page offset을 사용하도록 변경하는 내용이었다. 방어 로직을 추가해서 오프셋 정보가 부정확한 파일에서도 정상 동작하도록 했다. 이슈 공유 후 하루 만에 픽스 PR이 올라왔고 v477 마일스톤에 머지됐다.\n해당 픽스 커밋을 포크 레포에 체리피킹해서 스테이지에 배포하고 이슈 해소를 확인한 뒤, 운영에 반영했다.\n재배포 및 결과 # 두 이슈를 모두 해결한 뒤 배포를 재개했다.\n날짜 클러스터 작업 6월 23일 OLAP-Green v476 최초 배포 → 이슈 발견, 롤백 6월 26일 오전 OLAP-Green 픽스 반영 후 재배포 6월 26일 오후 OLAP-Blue v476 배포 BI 클러스터 그룹은 금요일에 배포할 경우 주말 간 이슈 대응이 어렵다고 판단해서 별도 티켓으로 분리하고 다음 주 초에 진행했다.\n배포 후 모니터링에서 클러스터 상태와 쿼리 실패 지표에 이상은 없었다. 사용자로부터 추가 이슈 제보도 없었다.\n아직 충분한 기간은 아니지만, v476이 선배포된 클러스터의 쿼리 스루풋이 상대적으로 높은 양상이 확인됐다. 두 클러스터의 워커 수가 거의 동일한 상태에서 게이트웨이가 실행 쿼리 수 기반으로 라우팅한다는 점을 감안하면, v476 쪽이 더 많은 쿼리를 처리하면서도 부하가 낮았다는 뜻이다. 반대쪽에 헤비 쿼리가 몰린 영향일 수 있지만, 신규 버전의 전반적인 쿼리 성능이 개선됐다고 볼 여지가 있다.\n배운 것 # Blue/Green 배포가 아니었으면 운영 장애가 됐다. 한쪽 클러스터에 먼저 배포했기 때문에 리그레션을 발견해도 즉시 롤백하고 다른 쪽으로 트래픽을 돌릴 수 있었다. 25개 버전을 한 번에 올리는 메이저 업그레이드에서 Blue/Green은 선택이 아니라 필수다.\n바이너리 서치는 리그레션 디버깅의 정석이다. 25개 버전 사이의 변경사항을 하나씩 확인하는 건 비현실적이다. 중간 버전에서 재현 여부를 확인하면서 범위를 절반씩 좁히면 5~6번의 테스트로 원인 버전을 특정할 수 있다. 원인 버전을 특정한 뒤에는 해당 버전의 관련 커밋을 하나씩 리버트하면서 원인 PR을 찾았다.\n잘 정리된 이슈 리포트는 빠른 해결로 이어진다. 원인 PR, 재현 방법, 문제가 되는 파일의 특성(pyarrow 레거시 버전, row group 크기 조건)을 함께 공유하니까 원래 PR 작성자가 하루 만에 근본 원인을 파악하고 픽스를 작성해줬다. 이슈만 올리고 \u0026ldquo;안 돼요\u0026quot;라고 쓰는 것과 원인 분석까지 해서 공유하는 건 응답 속도에서 큰 차이가 난다.\n실험적 기능이 디폴트 활성화되는 걸 경계하자. ThreadPerDriverTaskExecutor는 v451 때도 문제가 됐고 v476에서도 여전히 디폴트 활성화 상태였다. 커뮤니티 이슈가 열려 있는데도 해결이 안 되고 있다. 업그레이드 시 릴리즈 노트의 실험적 기능 관련 변경사항을 반드시 확인해야 한다.\n이전 업그레이드의 교훈을 체크리스트로 남겨두자. v451 때 발생한 두 이슈(워커 stuck, 파티션 프루닝 리그레션)에 대한 대응 방안을 미리 준비해뒀기 때문에 이번에는 빠르게 대응할 수 있었다. 업그레이드마다 겪은 이슈와 대응 방안을 기록해두면 다음 업그레이드가 수월해진다.\n참고 자료:\nTrino v475 Release Notes Trino v476 Release Notes Require Java 24 (v476) Add partitioning push down (v468) Add Apache Ranger authorizer plugin Alluxio file system support Add read support for S3 Tables in Iceberg (v471) ThreadPerDriverTaskExecutor issue Partition pruning regression Escape hatch for unsafe pushdowns MV refresh revert PR Parquet footer optimization (v469, regression cause) Parquet file_offset issue report Parquet fix PR (v477) ","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/trino-v476-upgrade/","section":"글 목록","summary":"Trino를 v451에서 v476으로 업그레이드했다. Blue/Green 배포로 OLAP 클러스터부터 점진 적용하는 과정에서 Materialized View 리그레션과 Parquet 읽기 리그레션을 발견했다. 바이너리 서치로 원인 버전을 v469로 좁히고, 커밋 단위 리버트로 원인 PR을 특정해서 커뮤니티에 이슈를 올렸더니 하루 만에 픽스가 나왔다.","title":"Trino v451에서 v476으로: 25개 버전을 건너뛴 운영 클러스터 업그레이드","type":"posts"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/trino-gateway/","section":"Tags","summary":"","title":"Trino-Gateway","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/troubleshooting/","section":"Tags","summary":"","title":"Troubleshooting","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/upgrade/","section":"Tags","summary":"","title":"Upgrade","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/tags/vpa/","section":"Tags","summary":"","title":"Vpa","type":"tags"},{"content":"","date":"2026년 2월 27일","externalUrl":null,"permalink":"/posts/","section":"글 목록","summary":"","title":"글 목록","type":"posts"},{"content":"","date":"2026년 2월 25일","externalUrl":null,"permalink":"/tags/alb/","section":"Tags","summary":"","title":"Alb","type":"tags"},{"content":"클라우드 인프라팀에서 요청이 왔다. 장애 분석할 때 ALB 로그를 테이블로 조회할 수 있게 해달라고.\nALB 로그는 S3에 CSV 형태로 쌓이고 있었다. 필요할 때마다 Athena로 직접 조회했는데 느리고 불편했다. 텍스트 포맷이니 조회 성능에도 한계가 있었다. Iceberg로 Parquet 파일로 변환해서 적재하면 실시간 대시보드까지 만들 수 있다.\n이 글은 그 시스템을 구축하면서 겪은 시행착오를 정리한 것이다.\n처음 설계: Flink filesystem connector # 가장 단순한 구조로 시작했다.\nS3 파일 → Flink (filesystem connector) → Iceberg 테이블 Flink의 filesystem connector가 S3 버킷을 감시하다가 새 파일이 들어오면 읽어서 각 라인을 레코드로 방출한다. 거기에 Iceberg sink를 붙이면 끝이다.\n문제는 메모리였다. filesystem connector는 이미 처리한 파일 목록 전체를 Flink state에 들고 있는다. ALB 로그가 하루에도 수만 개씩 쌓이니 시간이 갈수록 state가 불어나서 메모리가 터졌다. 확장의 한계에 부딪혀서 구조를 바꿔야 했다.\n현재 구조: S3 → SQS → Filebeat → Kafka → Flink → Iceberg # 파일 감시를 Flink에서 분리했다. 메시지 큐와 경량 수집기를 앞단에 두는 구조다.\nS3 버킷 → SQS (파일 이벤트) → Filebeat → Kafka → Flink → Iceberg 각 구간이 하는 일은 이렇다.\nS3 → SQS: 버킷 설정으로 ALB 로그 파일이 추가될 때마다 SQS에 해당 파일 경로가 메시지로 들어간다 SQS → Filebeat → Kafka: Filebeat가 SQS에서 메시지를 읽고 S3 파일을 직접 가져와 각 라인을 Kafka 토픽으로 보낸다 Kafka → Flink → Iceberg: Flink가 Kafka에서 레코드를 읽고 CSV 파싱 후 가공(파일 경로에서 계정번호, 서비스명, 역할 추출)하여 Iceberg 테이블에 쓴다 Flink는 Kafka만 바라보면 된다. 파일 목록을 state에 들고 있을 필요가 없어졌다.\nFilebeat에 정착하기까지 # Filebeat를 선택하기 전에 Kafka Connect Filesystem Connector를 먼저 시도했다. 결과적으로 실패했고 그 과정에서 배운 것이 있다.\nKafka Connect Filesystem Connector — 포기 # 써드파티 커넥터였는데 문제가 여러 개 있었다.\nMaven에 올라가 있지 않아서 직접 빌드해야 했다 GitHub 마지막 커밋이 4년 전이었고 Java 8 기반이었다 같은 메시지가 두 번 이상 중복 발행되는 현상이 발생했다 S3 파일 move(클린업)가 정상적으로 동작하지 않았다. FileUtil.copy()가 S3A 파일시스템에서 제대로 안 돌아가는 문제였다 커뮤니티에서 충분히 검증되지 않은 도구를 억지로 쓰는 것보다 잘 관리되는 도구를 쓰는 게 낫다고 판단했다.\nFilebeat 버킷 리스팅 모드 — 스케일링 한계 # Filebeat는 S3 input을 두 가지 모드로 지원한다. 처음에는 버킷 리스팅 모드를 썼다. 주기적으로 S3를 스캔해서 새 파일을 감지하는 방식이다.\n동작은 잘 됐는데 수평 스케일링이 안 됐다. 여러 Filebeat 인스턴스를 띄우면 같은 파일을 중복 처리한다. 각 인스턴스가 서로의 존재를 모르기 때문이다. 공식 문서에도 버킷 리스팅 모드에서는 single instance + 수직 스케일링만 하라고 명시되어 있다.\n또 다른 문제도 있었다. 처리 완료된 파일을 백업 버킷으로 옮기는 기능(backup_to_bucket_arn)이 대부분의 Filebeat 버전에서 버킷 리스팅 모드와 제대로 동작하지 않았다. GitHub Issue를 찾아보니 fix가 머지됐다가 실수로 롤백된 상태였다. 8.15.3 버전에서만 정상 동작을 확인했다.\nSQS 모드로 전환 # SQS 모드는 수평 스케일링이 가능하다. SQS가 메시지 분배를 해주기 때문에 여러 Filebeat 파드가 서로 겹치지 않고 처리할 수 있다. S3 API 호출 비용도 줄어든다.\n단점은 기존 파일을 소급 처리하지 못한다는 점이다. SQS에 이벤트가 들어온 시점 이후의 파일만 처리한다. 필요하면 기존 파일에 대해 수동으로 SQS 이벤트를 발행하면 된다. ALB 로그 상황에서는 문제가 안 됐다.\n한 가지 더. SQS 모드에서는 backup_to_bucket_arn 설정을 아예 사용할 수 없다. config 검증 단계에서 에러가 난다. 생각해 보면 SQS를 쓰면 새로 추가된 파일과 기존 파일을 구분할 필요가 없으니 백업 기능이 필요 없는 게 맞다.\n최종 Filebeat 설정은 이렇다.\nfilebeat.inputs: - type: aws-s3 queue_url: https://sqs.ap-northeast-2.amazonaws.com/xxxx/s3-elb-logs-events file_selectors: - regex: \u0026#34;.*\\\\.log\\\\.gz\u0026#34; number_of_workers: 64 decompress: true codec.line: format: message output.kafka: hosts: - kafka-cluster:9092 topic: cloudinfra.prod.streaming.alb-access-log.csv codec.format: string: \u0026#39;%{[aws.s3.object.key]} %{[message]}\u0026#39; compression: zstd 핵심은 codec.format에서 S3 파일 경로(aws.s3.object.key)를 메시지 앞에 붙여서 보내는 부분이다. Flink에서 이 경로를 파싱해 계정번호와 서비스명을 추출한다.\n오토스케일링 # Filebeat: SQS 기반 KEDA 스케일링 # SQS의 Visible Messages 수를 기준으로 KEDA ScaledObject를 구성했다.\nspec: cooldownPeriod: 300 maxReplicaCount: 128 minReplicaCount: 1 pollingInterval: 30 triggers: - type: aws-sqs-queue metadata: queueLength: \u0026#39;50\u0026#39; queueURL: https://sqs.ap-northeast-2.amazonaws.com/xxxx/s3-elb-logs-events 처음 적용했을 때 KEDA operator의 IAM Role 권한 문제로 SQS 메트릭 조회가 실패했다. ScaledObject에서 identityOwner: operator로 설정하면 KEDA operator가 자기 role로 직접 SQS를 조회한다. 이 role에 sqs:GetQueueAttributes 권한을 추가해서 해결했다.\nFlink: K8s Operator 오토스케일링 # Flink는 Kubernetes Operator의 내장 오토스케일링을 사용한다.\njob.autoscaler.target.utilization: \u0026#34;0.75\u0026#34; job.autoscaler.target.utilization.boundary: \u0026#34;0.15\u0026#34; pipeline.max-parallelism: \u0026#39;480\u0026#39; Flink 체크포인트 튜닝 # 체크포인트 간격을 1분에서 5분으로 늘렸더니 연쇄적으로 문제가 터졌다.\nHeartbeat timeout # 체크포인트가 오래 걸리면서 TaskManager의 heartbeat가 타임아웃됐다. 기본값이 너무 짧았다.\nheartbeat.interval: \u0026#39;60000\u0026#39; heartbeat.timeout: \u0026#39;300000\u0026#39; pekko.ask.timeout: 10m OOM: Java heap space # 체크포인트 간격이 길어지면서 Kafka fetch 버퍼가 힙에 쌓이는 양이 늘었다. 파드 메모리를 2G → 4G → 6G로 올려도 같은 에러가 반복됐다.\n원인은 JVM Task Heap 메모리 할당이 부족한 것이었다. 파드 전체 메모리는 여유가 있었지만 Task Heap은 2.32GB만 할당되어 꽉 찼다. taskmanager.memory.task.heap.size를 직접 지정해서 해결했다.\ntaskmanager.memory.task.heap.size: 4608m taskmanager.memory.managed.size: 512m taskmanager.memory.network.fraction: \u0026#39;0.02\u0026#39; 최종 설정 # 여러 차례 튜닝 끝에 체크포인트 간격을 10분까지 늘릴 수 있었다.\nexecution.checkpointing.interval: 10m execution.checkpointing.timeout: 5m heartbeat.interval: \u0026#39;60000\u0026#39; heartbeat.timeout: \u0026#39;300000\u0026#39; pekko.ask.timeout: 10m taskmanager.memory.task.heap.size: 4608m taskmanager.memory.managed.size: 512m taskmanager.memory.network.fraction: \u0026#39;0.02\u0026#39; 컴팩션 실패와 Upsert 제거 # 체크포인트 간격을 10분으로 바꾼 뒤에도 Iceberg 컴팩션이 계속 실패했다. 원인으로 추정되는 요소를 전부 제거하기로 했다.\n새 Iceberg 테이블을 만들었다. 기존 테이블의 메타데이터가 오염(?)된 상태일 수 있어서 깨끗한 테이블로 교체했다. Upsert 로직을 제거했다. 원래 중복 방지를 위해 UPSERT 모드로 적재하고 있었는데 이게 컴팩션 충돌의 원인이었다. Upsert를 빼면 중복 데이터가 생길 수 있다. 확인해 보니 중복이 있긴 했는데 적재 과정에서 생긴 게 아니라 원본 ALB 로그 자체에 중복이 있었다. 실질적으로 문제없다고 판단하고 append-only로 전환했다. 이후 컴팩션 작업이 빠르게 성공했다.\n운영 중 마주친 사건: ALB 로그 컬럼 수 변경 # 어느 날 새벽 4시 무렵부터 Flink 앱의 CSV 파싱이 실패하기 시작했다. 원인은 ALB 로그 컬럼이 31개에서 34개로 늘어난 것이었다. AWS에서 사전 공지 없이 변경한 거였다.\n첫 번째 시도: DLQ (Dead Letter Queue) # csv-dlq 포맷을 적용해서 파싱 에러가 난 레코드를 DLQ 토픽으로 보내도록 했다. 앱은 돌아왔지만 새로운 문제가 생겼다. 포맷 에러가 TaskManager 로그에 기록되면서 ephemeral-storage가 가득 찼다. 파드가 Evicted 상태로 죽었다 살아났다를 반복했다.\nThe node was low on resource: ephemeral-storage. log4j 설정을 수정해서 포맷 에러 로그 레벨을 ERROR로 올려 해결했다.\nlogger.format.name = com.woowahan.dataservice.format logger.format.level = ERROR 두 번째 문제: DLQ 토픽 과부하 # DLQ 토픽에 메시지가 너무 많이 들어가면서 Kafka 클러스터에 부하가 생겼다. 결국 DLQ를 롤백하고 csv.ignore-parse-error 옵션으로 파싱 에러를 무시하도록 바꿨다. 소스 테이블에 dummy 컬럼 3개를 추가해서 31개든 34개든 모두 수용할 수 있게 처리했다.\n중복 방지: UPSERT와 Primary Key # 초기에는 중복 방지를 위해 Iceberg UPSERT를 사용했다. PK(Primary Key)를 잡아야 하는데 ALB 로그에는 고유 식별자가 없다.\n46만 건의 로그를 조사해서 file_path, time, http_request, client_addr, target_addr, request_creation_time 5개 필드 조합이 거의 유니크하다는 걸 확인했다. 실제 중복은 전체에서 1건뿐이었고 그 2개 레코드는 모든 필드가 동일해서 사실상 구분 불가능한 로그였다.\nIceberg 테이블에서 PK를 지정하려면 Flink SQL의 SET IDENTIFIER FIELDS를 써야 했다. Spark SQL에서는 PRIMARY KEY 문법을 지원하지 않고 Flink SQL에서는 bucketing과 hidden partitioning을 지원하지 않아서 테이블 생성에 애를 먹었다.\n결국 앞서 설명한 대로 컴팩션 안정성을 위해 UPSERT를 제거했다.\n모니터링 # 핵심 지표 # 지표 의미 SQS Approximate Number Of Messages Visible 처리 대기 중인 파일 수 SQS Approximate Number Of Messages Not Visible 현재 Filebeat가 처리 중인 파일 수 Kafka consumer lag Flink의 처리 지연 Flink job status 앱 실행 상태 알럿 설정 # SQS: Visible Messages가 임계치 초과 시 (밀림 감지) Filebeat: CPU/메모리 사용률 과다, 파드 미실행 Flink: job이 non-running 상태 전환 시 정리 # 돌아보면 가장 큰 교훈은 세 가지다.\nFlink에 파일 감시를 맡기지 말 것. Filesystem connector는 처리한 파일 목록을 state에 들고 있어서 장기 운영하면 메모리가 터진다. 파일 감시는 SQS + Filebeat 같은 외부 도구에 맡기고 Flink는 스트림 처리에만 집중시키는 게 맞다.\nUpsert는 컴팩션과 충돌한다. Iceberg CDC 테이블의 position delete 문제다. 로그성 데이터라면 append-only가 운영 안정성 면에서 훨씬 낫다. 원본 데이터 자체의 중복은 소비자 쪽에서 처리하면 된다.\nAWS는 사전 공지 없이 포맷을 바꿀 수 있다. ALB 로그 컬럼이 갑자기 늘어나는 일이 실제로 일어났다. CSV 파싱을 하는 파이프라인이라면 ignore-parse-error 같은 방어 로직이 필수다.\n참고 자료:\nFilebeat AWS S3 Input Flink Filesystem Connector Apache Iceberg - Flink Writes KEDA AWS SQS Queue Scaler ","date":"2026년 2월 25일","externalUrl":null,"permalink":"/posts/alb-log-collection-system/","section":"글 목록","summary":"S3에 CSV로 쌓이는 ALB 로그를 Iceberg 테이블로 바꾸는 시스템을 구축했다. Flink filesystem connector의 메모리 한계를 겪고 filebeat + SQS + Kafka 구조로 재설계한 과정, 오토스케일링 적용, 체크포인트 튜닝, 그리고 운영 중 마주친 여러 삽질을 정리했다.","title":"ALB 로그를 Iceberg 테이블로 만들기까지","type":"posts"},{"content":"","date":"2026년 2월 25일","externalUrl":null,"permalink":"/tags/filebeat/","section":"Tags","summary":"","title":"Filebeat","type":"tags"},{"content":"","date":"2026년 2월 25일","externalUrl":null,"permalink":"/tags/flink/","section":"Tags","summary":"","title":"Flink","type":"tags"},{"content":"","date":"2026년 2월 25일","externalUrl":null,"permalink":"/tags/sqs/","section":"Tags","summary":"","title":"Sqs","type":"tags"},{"content":"앱 로그를 해외 트래킹 서버로 보내야 하는 상황이 생겼다. 문제는 로그 안에 회원번호, 디바이스 ID, 주문번호 같은 개인식별정보(PII)가 섞여 있다는 것. 개인정보보호법상 그대로 국외 전송하면 안 된다.\n그래서 중간에 프록시 서버를 뒀다. 앱에서 보낸 로그를 프록시가 받아서 PII를 해싱하고 해외 서버로 넘긴다. 원본 데이터는 사내 Kafka로 보내서 광고나 추천 같은 내부 데이터 프로덕트에 활용할수 있게 했다.\n이 글은 하루 평균 13억건 로그를 처리하는 프록시 서버를 Apache APISIX로 구축한 기록이다.\n왜 APISIX인가 # 처음에는 Fluent-bit을 생각했다 # 로그 수집이니까 Fluent-bit이 자연스러운 선택이었다. 근데 요구사항을 정리해보니 단순한 로그 포워딩이 아니라 API Gateway 기능이 필요했다.\nHTTP 요청을 받아서 request body를 조작해야 한다 (PII 해싱) 해외 트래킹 서버 응답(400 에러 등)을 클라이언트에 그대로 전달해야 한다 Kafka로 원본 데이터를 동시에 발행해야 한다 Fluent-bit은 이 중 어느 것도 깔끔하게 처리하지 못한다.\nKong vs APISIX # 둘 다 nginx 기반이고 Lua 스크립팅을 지원한다. Kong을 먼저 테스트했는데 Helm 차트 배포 단계에서 알려진 이슈에 걸려 실패했다.\nAPISIX로 전환한 이유는 간단하다.\n경량화 설계. Kong보다 메모리를 적게 쓰고 코어당 처리량이 높다 선언적 라우팅. Admin API로 설정을 보내면 etcd에 저장돼 영구 보관된다 커스텀 플러그인. rewrite → access → header_filter → body_filter → log 단계로 요청 처리 파이프라인이 나뉘어 있어서 원하는 단계에 로직을 끼워넣기 좋다 커스텀 플러그인 개발 # 왜 기존 플러그인으로는 안 되나 # APISIX에는 kafka-logger라는 내장 플러그인이 있다. 요청 데이터를 Kafka로 보내주는 건데 비식별화 플러그인(encrypt-pii)을 먼저 실행하면 kafka-logger가 원본이 아닌 해싱된 데이터에 접근하게 된다.\n원하는 흐름은 이렇다.\n앱 → APISIX → (1) 원본을 Kafka로 발행 (2) PII를 해싱 (3) 해싱된 데이터를 해외 서버로 전송 (4) 해외 서버의 응답을 앱에 반환 (1)과 (2)의 순서가 중요하다. 원본을 Kafka에 먼저 보내고 그다음 해싱해야 한다. 기존 플러그인 조합으로는 이 순서를 보장할수 없어서 kafka-logger 기능까지 포함한 커스텀 플러그인을 만들었다.\nLua로 비식별화 구현 # APISIX는 LuaJIT 위에서 돌아간다. lua-resty-string 모듈로 SHA-256 해싱을 구현했다.\nlocal resty_sha256 = require \u0026#34;resty.sha256\u0026#34; local str = require \u0026#34;resty.string\u0026#34; local function hash_value(value) local sha256 = resty_sha256:new() sha256:update(value) local digest = sha256:final() return str.to_hex(digest) end 비식별화 대상은 세 가지다.\n회원번호 디바이스 ID 주문번호 request body에서 JSON을 파싱하고 대상 필드를 찾아 해싱한 뒤 다시 직렬화한다.\nKafka 메시지 발행 # 같은 플러그인 안에서 lua-resty-kafka를 써서 원본 데이터를 Kafka로 보낸다.\nlocal producer = require \u0026#34;resty.kafka.producer\u0026#34; local broker_list = { { host = \u0026#34;kafka-broker-01\u0026#34;, port = 9092 }, } local p = producer:new(broker_list, { producer_type = \u0026#34;async\u0026#34; }) local ok, err = p:send(\u0026#34;app-log-topic\u0026#34;, nil, original_body) lua-resty-kafka를 쓰려면 의존 모듈이 필요하다.\nluarocks install lua-cjson luarocks install penlight 개발 중에 두 가지 이슈를 만났다.\nKafka 메시지에 timestamp가 안 남는 문제. Kafka API version을 1에서 2로 올리니 해결됐다 Kafka metadata 조회 실패. API version 2에서 발생해서 다시 1로 내렸다 결국 메시지 발행은 API version 2로, metadata 조회는 API version 1로 분리 처리했다.\n아키텍처 변천사 # 처음 설계했던 구조와 최종 구조가 꽤 다르다.\n초기 설계. 프록시 → Kafka → Flink → 해외 서버 # 앱 → APISIX → Kafka → Flink(비식별화) → 해외 트래킹 서버 프록시는 Kafka로만 보내고 비식별화는 Flink에서 처리하는 구조였다. 문제는 해외 서버가 HTTP 400 에러를 내릴 때 그걸 클라이언트에 전달할 방법이 없다는 것. Kafka로 보내는 순간 응답은 끊기니까.\n최종 설계. 프록시에서 직접 비식별화 # 앱 → APISIX → (원본 → Kafka) → (해싱 후 → 해외 트래킹 서버 → 응답 → 앱) 프록시에서 직접 비식별화를 수행하고 해외 서버로 전송한다. 장단점이 뚜렷하다.\n장점:\n해외 서버 응답(400 포함)을 클라이언트에 그대로 전달할 수 있다 Flink 애플리케이션이 빠진다 실시간 처리가 된다 단점:\n프록시 서버에 부하가 몰린다 request body 파싱 + 해싱 + Kafka 발행을 한 요청 안에서 다 처리한다 부하 집중이 걱정됐지만 APISIX 성능이 충분히 받쳐줬다. 수치는 아래에서 다룬다.\n부하 테스트 # 테스트 환경 # nGrinder로 부하를 걸었다. 프록시 서버 스펙은 Pod당 2 core, 4GiB 메모리.\n코어당 처리량 # APISIX 공식 문서에는 1 core당 10,000 QPS를 처리할 수 있다고 나와 있다. 근데 request body를 파싱하고 해싱하는 Lua 스크립트가 붙으면 그보다 낮아진다.\n실측 결과 (2 core 기준, 1분 30초씩 테스트):\nVuser Peak TPS CPU 사용률 에러 990 2,381 32% 0 1,980 4,907 48% 1 3,960 7,000 99% 0 안정적으로 운영하려면 2 core 기준 5,000 TPS 정도가 적당하다. 피크 트래픽이 약 55,000 TPS이므로 Pod 12개면 커버된다.\n튜닝 포인트 # Keepalive 설정. 적용 전에는 p95 레이턴시가 요청마다 흔들렸는데 keepalive 설정 후 p95 기준 10ms 이하로 안정화됐다. warm-up 이후에는 1ms 미만도 나왔다.\nnginx worker_connections. APISIX 기본값은 auto인데 이게 Pod 코어 수가 아니라 노드 코어 수를 기준으로 worker를 할당한다. worker가 과하게 생기면 CPU 병목이 생길 수 있어서 Pod 코어 수에 맞춰 수동 설정했다. 효과가 극적이진 않지만 CPU 사용률이 내려가면서 TPS는 올라갔다.\n스케일 아웃 전략. 트래픽이 갑자기 밀려오면 스케일 아웃되기 전에 CPU가 100%를 찍으면서 에러가 터진다. 스트리밍과 달리 백프레셔가 없기 때문이다. 두 가지로 대응했다.\nKEDA로 CPU 50% 기준 스케일 아웃 — 여유 있게 잡아야 한다 스케줄 기반 스케일링 — 피크 타임(점심, 저녁)에 맞춰 미리 Pod를 늘려놓는다 502 에러 원인 # 테스트 중 간헐적으로 502가 발생했다. upstream 서버에서 response header를 읽다가 타임아웃이 걸린 것이다. 원인이 복합적이었다.\n해외 서버 스테이징 환경이 spot 인스턴스라 응답이 불안정했다 트래픽이 급격히 몰리면 스케일 아웃 전에 CPU가 포화됐다 nGrinder agent당 vuser를 과하게 잡으면 클라이언트 쪽 네트워크 병목이 생겨서 서버 응답이 느려 보이는 현상이 있었다 운영 환경에서는 min Pod를 보수적으로 잡고 스케줄 기반 스케일링을 병행하니 502가 사라졌다.\nKubernetes 배포 # Helm 차트 구성 # APISIX를 decoupled 모드로 배포했다. control-plane과 data-plane을 분리하는 방식이다.\ncontrol-plane (apisix-control). etcd와 함께 배포하고 라우팅 설정을 관리한다 data-plane (apisix-data). 실제 트래픽을 처리한다. externalEtcd 설정으로 control-plane의 etcd에 연결한다 etcd는 PVC(gp3)로 데이터를 영구 보관한다. 표준 EKS 기본 스토리지 클래스가 gp3이므로 별도 설정 없이 PVC를 생성하면 된다.\nHPA 설정 (KEDA) # # KEDA ScaledObject (요약) triggers: - type: prometheus metadata: query: avg(rate(container_cpu_usage_seconds_total{...}[1m])) * 100 threshold: \u0026#34;50\u0026#34; - type: memory metadata: value: \u0026#34;80\u0026#34; CPU 50%나 메모리 80%를 넘으면 스케일 아웃한다. 여기에 스케줄 기반 스케일링을 더해서 피크 타임에 미리 Pod를 확보한다.\n보안 # 퍼블릭으로 노출되는 리버스 프록시 ALB에 AWS WAF를 연동했다. 정보보안팀 검토 결과 WAF만 적용하면 클라이언트 보안에 문제없다는 결론이 나왔다.\n모니터링 # Grafana 대시보드를 구성해서 아래 지표를 실시간으로 본다.\nSystem. CPU 사용률, 메모리 사용률 (Pod별) Nginx. 총 요청 수, accepted/handled connections, connection state HTTP. 상태 코드별 RPS, 서비스/라우트별 RPS Latency. APISIX 레이턴시, upstream 레이턴시, 전체 요청 레이턴시 Bandwidth. 서비스/라우트별 ingress/egress etcd. modify indexes, reachable 상태 APISIX 라우트 설정에 prometheus 플러그인을 추가해야 request 단위 메트릭이 수집된다. 이걸 빠뜨리면 시스템 메트릭만 나오고 HTTP 관련 지표가 빈다.\n알럿은 Grafana → OpsGenie → Slack 채널로 연동했다.\n운영 결과 # 약 3개월간 운영한 결과를 정리하면 이렇다.\n지표 목표 실제 비용 절감 기존 대비 20% 29.8% 절감 레이턴시 (p95) 40ms 25ms 가용성 99.99% 100% PII 비식별화 성공률 100% 100% 비용 절감이 목표보다 높았던 건 Flink 애플리케이션이 빠지면서다. 프록시에서 직접 비식별화를 처리하니 별도 스트림 프로세싱 비용이 사라졌다.\n운영 초기에 499 에러(클라이언트가 연결을 끊는 경우)와 408 에러(서버 타임아웃)가 소량 발생했다. 400 에러를 제외한 나머지는 SDK에서 재전송 처리하므로 데이터 유실은 없었다.\n마치며 # APISIX를 프록시 서버로 쓰면서 배운 것을 정리한다.\nAPI Gateway를 프록시로 쓰는 게 자연스러울 때가 있다. request body 조작과 HTTP 응답 전달이 필요하면 로그 수집기보다 API Gateway가 맞다. 커스텀 플러그인은 피할 수 없다. 기존 플러그인 조합으로 해결하려고 붙잡지 말고 빨리 커스텀 플러그인을 만드는 게 낫다. APISIX 플러그인 구조가 잘 되어 있어서 개발 자체는 어렵지 않다. 스케일 아웃보다 미리 확보하는 게 낫다. 트래픽이 급격히 몰리면 리액티브 스케일링은 늦다. 스케줄 기반으로 피크 타임에 미리 Pod를 올려놓는 게 안정적이다. nginx worker 수를 확인하라. APISIX auto 설정이 Pod가 아닌 노드 기준으로 worker를 만든다. 컨테이너 환경에서는 수동으로 맞춰야 한다. 참고 자료:\nApache APISIX 공식 문서 lua-resty-kafka (GitHub) APISIX Custom Plugin Development Apache APISIX Grafana Dashboard ","date":"2026년 2월 24일","externalUrl":null,"permalink":"/posts/apisix-proxy-server-guide/","section":"글 목록","summary":"해외 트래킹 서버로 앱 로그를 전송할 때 개인정보를 비식별화해야 했다. Fluent-bit에서 시작해 Kong을 거쳐 APISIX에 정착한 과정, 커스텀 Lua 플러그인 개발, 5.5만 TPS 부하 테스트, Kubernetes 배포까지 실전 경험을 정리했다.","title":"Apache APISIX로 프록시 서버 구축하기. 일 13억 건 로그 비식별화 처리","type":"posts"},{"content":"","date":"2026년 2월 24일","externalUrl":null,"permalink":"/tags/apisix/","section":"Tags","summary":"","title":"Apisix","type":"tags"},{"content":"","date":"2026년 2월 24일","externalUrl":null,"permalink":"/tags/cdc/","section":"Tags","summary":"","title":"Cdc","type":"tags"},{"content":"","date":"2026년 2월 24일","externalUrl":null,"permalink":"/tags/deletion-vector/","section":"Tags","summary":"","title":"Deletion-Vector","type":"tags"},{"content":"Iceberg CDC 테이블에 컴팩션을 돌리면 이런 에러가 뜬다.\norg.apache.iceberg.exceptions.ValidationException: Cannot commit, found new position delete for replaced data file append-only 테이블에서는 안 나는 에러다. CDC에서만 난다. 왜 그런지 이해하려면 Iceberg v2의 delete 메커니즘부터 알아야 한다.\nIceberg v2의 row-level delete # 데이터를 지우는 두 가지 방법. COW vs MOR # Iceberg에서 행을 삭제하거나 업데이트하는 방법은 두 가지다.\nCopy-on-Write (COW). 변경이 생기면 데이터 파일을 통째로 다시 쓴다. 삭제할 행을 빼고 나머지를 새 파일에 복사하는 식이다. 읽기 성능은 좋지만 쓰기 비용이 크다. 배치성 대량 갱신에 적합하다.\nMerge-on-Read (MOR). 데이터 파일은 건드리지 않는다. 대신 \u0026ldquo;이 행은 삭제되었다\u0026quot;는 정보를 별도 delete file에 기록한다. 쓰기가 빠른 대신 읽을 때 원본과 delete file을 병합해야 하므로 읽기 비용이 올라간다. CDC/Upsert 테이블에 적합하다.\nCDC 파이프라인은 업데이트가 쉴 새 없이 들어오니까 MOR이 맞다. 문제는 이 MOR 경로에서 생기는 delete file이 컴팩션과 충돌한다는 점이다.\nDelete file의 두 종류 # MOR에서 사용하는 delete file에는 두 가지가 있다.\nEquality delete. 삭제할 행의 키값만 기록한다. \u0026ldquo;이 PK를 가진 행을 지워라\u0026quot;는 뜻이다. 여러 데이터 파일에 걸쳐 있어도 delete file 하나로 표현할 수 있다. 대신 읽기 시 키 매칭 비용이 든다.\nPosition delete. 특정 데이터 파일의 특정 위치(행 번호)를 기록한다. \u0026ldquo;A.parquet 파일의 42번째 행을 지워라\u0026quot;는 식이다. 읽을 때 정확히 그 위치만 건너뛰면 되니까 equality delete보다 읽기 성능이 좋다.\n유형 기록 내용 읽기 비용 적합한 경우 Equality delete 키 값 높음 (키 매칭) PK 기반 대량 삭제 Position delete 파일 경로 + 행 번호 낮음 (위치 점프) 스트림 내 빈번한 업데이트 엔진마다 지원 범위가 다르다 # Iceberg 스펙에는 두 종류 모두 정의되어 있지만 모든 엔진이 전부 구현한 건 아니다.\n엔진 Equality delete 읽기 Position delete 읽기 Equality delete 쓰기 Position delete 쓰기 Spark O O X O Flink O O O (UPSERT 모드) 상황에 따라 Trino O O X O Athena O O X O Hive/Impala O O X O Spark은 equality delete를 읽을 수는 있지만 쓰지는 못한다. Trino와 Athena도 마찬가지. 행 삭제 시 position delete만 기록한다. UPSERT 모드에서 equality delete 쓰기를 지원하는 건 Flink뿐이다.\n이 차이가 컴팩션 전략에 영향을 준다.\nDelta Writer. 같은 커밋 안에서 delete 전략이 갈린다 # CDC 싱크에서 iceberg-core의 BaseTaskWriter가 레코드를 처리하는 로직이 흥미롭다.\npublic void deleteKey(T key) throws IOException { if (!internalPosDelete(asStructLikeKey(key))) { eqDeleteWriter.write(key); } } 한 커밋을 만드는 과정에서 삭제 요청이 들어오면 먼저 이번 스트림(커밋)에 포함된 데이터인지 확인한다.\n이번 스트림에 있는 레코드 → position delete로 기록 이번 스트림에 없는 레코드 → equality delete로 기록 왜 전부 equality delete로 통일하지 않을까? 읽기 성능 때문이다. position delete가 MOR 읽기에서 훨씬 효율적이라 가능한 한 position delete를 쓰려고 한다. iceberg-core 쓰기 로직에 읽기 성능 최적화가 녹아있는 셈이다.\n이 설계가 컴팩션에서 문제를 일으킨다.\n스냅샷 타입과 커밋 충돌 # Iceberg의 네 가지 스냅샷 operation # Iceberg 스냅샷에는 operation 필드가 있다. 어떤 종류의 변경이 일어났는지를 나타낸다.\nOperation 의미 대표 작업 append 데이터 파일 추가 배치 적재, INSERT replace 데이터/삭제 파일 교체 컴팩션 overwrite 논리적 덮어쓰기 업서트, UPDATE, DELETE delete 데이터 파일 제거 또는 삭제 파일 추가 DROP PARTITION 등 컴팩션은 replace다. 작은 파일 여러 개를 큰 파일로 합치고 기존 걸 교체한다. CDC 싱크는 overwrite다. 업서트 과정에서 delete file을 만든다.\n낙관적 동시성 제어 # Iceberg는 낙관적 동시성(optimistic concurrency)으로 커밋 충돌을 처리한다. Git이랑 비슷하다.\n현재 스냅샷을 기준으로 새 메타데이터 트리를 만든다 원자적 커밋(atomic swap)을 시도한다 그 사이에 다른 커밋이 끼어들었으면 검증(validation) 을 수행한다 검증 통과하면 커밋 성공. 실패하면 재시도 핵심은 3번의 검증 규칙이다. 어떤 스냅샷 타입끼리 충돌하고 어떤 조합은 자동으로 병합되는가.\nAppend-only vs CDC. 왜 차이가 나는가 # Append-only 테이블은 충돌이 거의 없다 # Append-only 테이블은 데이터를 추가만 한다. delete file이 없다. 컴팩션(replace)이 돌아가는 동안 새 데이터가 들어와도(append) 서로 다른 파일을 건드리니까 자동 병합된다. Git에서 서로 다른 파일을 수정한 두 브랜치가 충돌 없이 머지되는 것과 같다.\nCDC 테이블은 position delete가 문제다 # CDC 테이블은 다르다. 업서트가 일어나면 delete file이 생긴다. 컴팩션이 데이터 파일 A를 새 파일 B로 교체하려는 순간 CDC 싱크가 파일 A에 대한 position delete를 만들어 버리면 어떻게 될까?\n시간 순서: 1. 컴팩션 시작: 파일 A를 읽어서 새 파일 B를 만드는 중 2. CDC 싱크: 파일 A의 42번째 행을 삭제 (position delete 생성) 3. 컴팩션 완료: 파일 A → 파일 B 교체 커밋 시도 4. 검증 실패: \u0026#34;파일 A에 대한 새 position delete가 생겼는데 파일 A를 교체하면 그 delete가 유실된다\u0026#34; ValidationException: Cannot commit, found new position delete for replaced data file 컴팩션 입장에서는 파일 A를 이미 읽어서 새 파일을 만들었는데 그 사이에 파일 A에 대한 삭제가 추가된 거다. 이 삭제를 반영하지 않고 교체하면 데이터 정합성이 깨지니까 Iceberg가 커밋을 거부한다.\nEquality delete는 왜 덜 부딪히나 # Equality delete는 \u0026ldquo;이 키를 가진 행을 지워라\u0026quot;라는 논리적 선언이다. 특정 파일에 바인딩되지 않는다. 컴팩션이 파일을 교체해도 키 기반 삭제는 새 파일에도 그대로 적용할 수 있다.\n실제로 Iceberg에는 컴팩션 시 시작 시점의 sequence-number를 유지하는 메커니즘이 있다(PR #3480). 이걸로 equality delete와의 충돌을 자동 우회한다. 기본 옵션으로 활성화되어 있다.\nposition delete는 그게 안 된다. 특정 파일의 특정 위치를 가리키기 때문에 파일이 바뀌면 위치도 의미를 잃는다.\n커밋 인터벌 딜레마 # \u0026ldquo;그럼 싱크 커밋 인터벌을 조정하면 되지 않나?\u0026rdquo; 싶을 수 있다. 쉽지 않다.\n인터벌을 길게 잡으면 한 커밋에 포함되는 레코드가 많아진다. 같은 레코드가 INSERT 후 UPDATE되거나 DELETE되는 확률이 올라간다. delta writer가 이번 스트림 내 레코드를 position delete로 처리하니까 position delete 발생이 늘어난다.\n인터벌을 짧게 잡으면 position delete 발생은 줄어들지만 커밋이 자주 일어난다. 컴팩션이 완료되기 전에 새 커밋이 끼어들 확률이 높아진다. position delete가 한 건만 있어도 충돌이 발생한다.\n어느 쪽이든 충돌 확률을 0으로 만들 수는 없다.\nv2에서의 개선 시도 — 전부 실패했다 # 이 문제를 해결하려는 PR이 여러 개 올라왔지만 전부 머지되지 못하고 닫혔다.\nPR 접근 방식 결과 #4703 컴팩션 검증 시 position delete를 선택적 무시 리뷰어들이 \u0026ldquo;위험하다\u0026rdquo; 판단, 닫힘 #4748 Flink upsert에서 position delete와 데이터 파일의 sequence number가 같다는 점을 이용 닫힘 #5760 manifest entry에 min-data-sequence-number 필드를 추가해 비충돌 delete를 필터링 stale bot이 닫음 #7249 position-deletes-within-commit-only 스냅샷 프로퍼티로 같은 커밋 내 position delete 선언 stale bot이 닫음 결국 v2 안에서는 깔끔한 해법이 안 나왔다. 커뮤니티 방향은 v3에서 구조적으로 해결하는 쪽으로 수렴했다.\nIceberg v3. Deletion Vector가 바꾸는 것 # v3 스펙은 2025년 초 확정되었고 Iceberg 1.8.0(2025년 2월)부터 구현이 들어가기 시작했다. 핵심 변경은 Deletion Vector(DV) 도입이다.\nPosition delete file → Deletion Vector # v3에서 position delete file은 새로 만들 수 없다. DV가 그 자리를 대신한다.\nPosition delete files must not be added to v3 tables, but existing position delete files are valid.\nDV는 Puffin 파일에 저장되는 Roaring bitmap이다. 데이터 파일 하나당 \u0026ldquo;몇 번째 행이 삭제되었는지\u0026quot;를 비트맵으로 표현한다.\n항목 v2 Position delete v3 Deletion Vector 저장 형식 Parquet (파일 경로 + 행 번호 컬럼) Puffin (Roaring bitmap) 데이터 파일당 수 무제한 (N개 누적 가능) 최대 1개 새 삭제 발생 시 별도 delete file 추가 기존 DV를 읽어서 병합 후 교체 왜 컴팩션 충돌이 줄어드는가 # v2에서 충돌이 나는 이유는 position delete file이 데이터 파일과 독립적으로 존재하기 때문이었다. 컴팩션이 데이터 파일을 교체하는 동안 CDC 싱크가 같은 파일에 대한 새 position delete file을 만들면 교체 후에 그 delete가 가리키는 파일이 사라진다.\nDV는 구조가 다르다.\n데이터 파일에 종속된다. DV는 데이터 파일의 sidecar다. 컴팩션이 데이터 파일을 새로 쓰면 DV 삭제분도 함께 반영되고 기존 DV는 제거된다. 독립적으로 누적되지 않는다. 데이터 파일 하나에 DV는 최대 하나다. 새 삭제가 들어오면 기존 DV를 읽어서 병합한 뒤 교체한다. v2처럼 delete file이 쌓이면서 \u0026ldquo;파일 A에 대한 새 position delete\u0026rdquo; 문제가 생길 여지가 줄어든다. 컴팩션 빈도 자체가 줄어든다. DV는 compact한 비트맵이라 v2 position delete file처럼 소파일 문제가 없다. 컴팩션을 덜 돌려도 되니까 충돌 윈도우도 줄어든다. Row Lineage와 행 수준 충돌 검출 # v3는 DV 외에 Row Lineage도 도입한다. 모든 행에 고유 _row_id와 _last_updated_sequence_number가 부여된다. DV + Row Lineage를 결합하면 행 수준(row-level) 충돌 검출이 가능해진다(Issue #14613).\nv2에서는 같은 데이터 파일을 건드리면 무조건 충돌이었다. v3에서는 같은 파일이라도 서로 다른 행을 수정했으면 자동 병합할 수 있다. CDC 싱크가 42번째 행을 삭제하고 컴팩션이 다른 행을 정리하는 상황이라면 충돌 없이 커밋이 가능해진다.\n아직 완벽하지는 않다 # OCC(낙관적 동시성)는 여전히 적용된다. 두 writer가 같은 데이터 파일의 DV를 동시에 갱신하면 한쪽은 재시도해야 한다. 그래도 v2와는 다른 점이 있다.\n재시도 비용이 낮다. 비트맵 병합만 다시 하면 된다. 데이터를 처음부터 스캔할 필요가 없다. 충돌 범위가 좁다. 파일 단위가 아니라 DV 단위다. Row lineage 기반 행 수준 충돌 검출이 엔진에 구현되면 같은 파일 내 다른 행 수정은 충돌에서 제외된다. 엔진 지원 현황 (2025년 기준) # 엔진 v3 DV 지원 Spark (Iceberg 1.8.0+) 지원 AWS EMR / Athena / Glue 2025년 11월 발표, 지원 Databricks 지원 (row-level concurrency 포함) Trino (Starburst) 지원 추가 중 Flink 구현 진행 중 v3 마이그레이션은 기존 v2 테이블에서 ALTER TABLE ... SET TBLPROPERTIES ('format-version' = '3')로 전환할 수 있다. 기존 position delete file은 유효하게 유지되며 새 delete부터 DV로 기록된다.\n현재 시점의 운영 우회책. CDC를 잠시 멈추고 컴팩션하기 # 현실적으로 가장 안정적인 방법은 컴팩션 윈도우 동안 CDC 싱크를 일시 중단하는 것이다.\n컴팩션 파이프라인: 1. 새벽 저부하 시간대에 CDC 싱크 커넥터를 pause 2. 컴팩션 실행 (rewrite_data_files) 3. 컴팩션 완료 후 CDC 싱크 재개 카카오 테크 블로그에서도 비슷한 운영을 시사하고 있다. 12시간 간격으로 실시간 CDC 싱크를 중단하고 컴팩션을 돌리는 구조다.\n주의사항 # 컴팩션 시간이 예상보다 길 수 있다. 하루치 CDC 데이터가 쌓인 테이블을 컴팩션하면 생각보다 오래 걸린다. 윈도우 길이를 실측으로 결정해야 한다. 파티션별 분할 실행을 고려하라. 전체 테이블을 한 번에 컴팩션하지 말고 파티션 단위로 나눠서 실행하면 시간을 줄일 수 있다. delete file 정리도 별도로 해야 한다. rewrite_position_delete_files로 delete 소파일을 정리하는 minor compaction도 주기적으로 돌려야 한다. 단 버전별 버그가 보고되어 있으니 호환성을 확인하라. 운영 체크리스트 # 런타임 버전 확인. 사용 중인 EMR/Spark/Flink/Trino 버전에서 rewrite_data_files 검증 규칙과 rewrite_position_delete_files 지원 여부를 확인한다 충돌률 모니터링. CDC 커밋 주기 대비 컴팩션 수행 시간을 실측하고 충돌 빈도를 추적한다 메타데이터 대시보드. 스냅샷/매니페스트 테이블에서 파일 수, 평균 크기, delete file 누적량을 시각화한다 컴팩션 윈도우 확보. 야간 CDC pause 또는 파티션별 분할 컴팩션 전략을 수립한다 커뮤니티 패치 추적. PR #4703, #7249 같은 개선안 반영 여부를 버전별로 점검한다 마치며 # 정리하면 이렇다.\nIceberg v2 MOR 경로에서 position delete는 읽기 성능을 위한 최적화다 그런데 position delete는 특정 파일에 바인딩되어 있어서 컴팩션(replace)과 충돌한다 Equality delete는 sequence-number 메커니즘으로 자동 우회되지만 position delete는 안 된다 v2 안에서 이 문제를 해결하려는 PR은 전부 머지되지 못했다 v3 Deletion Vector가 구조적 해결책이다. Position delete file 대신 데이터 파일당 하나의 비트맵으로 삭제를 관리하고 Row Lineage로 행 수준 충돌 검출까지 가능해진다 v3 전환 전까지는 컴팩션 윈도우 동안 CDC를 잠시 멈추는 게 현실적인 답이다 v3 스펙은 확정되었고 엔진 지원도 빠르게 확대되고 있다. v3로 전환하면 이 글에서 다룬 운영 부담 대부분이 사라진다. 아직 v2를 쓰고 있다면 v3 마이그레이션 계획을 세우는 게 장기적으로 맞다.\n참고 자료:\nRow-Level Changes on the Lakehouse: COW vs MOR in Apache Iceberg (Dremio) Apache Iceberg Spec - Row-level Deletes Apache Iceberg - Reliability (Concurrency) PR #3480: Core: support rewrite data files with starting sequence number PR #4703: API: Optionally ignore position deletes in rewrite validation PR #7249: Avoid conflicts between rewrite datafiles and flink CDC writes 로그 유형별 Iceberg 테이블 적재 및 운영 전략 (Kakao Tech) Improve Position Deletes in V3 (Issue #11122) Row Lineage for V3 (Issue #11129) Row-level concurrency (Issue #14613) What\u0026rsquo;s New in Apache Iceberg v3 (Google Open Source Blog) Iceberg V3 Deletion Vectors on Amazon EMR (AWS Blog) ","date":"2026년 2월 24일","externalUrl":null,"permalink":"/posts/iceberg-cdc-compaction-challenge/","section":"글 목록","summary":"Iceberg v2의 row-level delete 구현, position delete와 equality delete의 차이, 실시간 CDC 싱크와 컴팩션 간 커밋 충돌 원인을 코드 레벨에서 분석했다. v3 Deletion Vector가 이 문제를 어떻게 바꾸는지, 그리고 v2 환경에서의 운영 우회책까지 함께 정리한다.","title":"Iceberg - 왜 CDC 테이블의 컴팩션이 까다로운가","type":"posts"},{"content":"","date":"2026년 2월 24일","externalUrl":null,"permalink":"/tags/iceberg-v3/","section":"Tags","summary":"","title":"Iceberg-V3","type":"tags"},{"content":"","date":"2026년 2월 24일","externalUrl":null,"permalink":"/tags/lakehouse/","section":"Tags","summary":"","title":"Lakehouse","type":"tags"},{"content":"","date":"2026년 2월 24일","externalUrl":null,"permalink":"/tags/lua/","section":"Tags","summary":"","title":"Lua","type":"tags"},{"content":"","date":"2026년 2월 24일","externalUrl":null,"permalink":"/tags/privacy/","section":"Tags","summary":"","title":"Privacy","type":"tags"},{"content":"","date":"2026년 2월 24일","externalUrl":null,"permalink":"/tags/proxy/","section":"Tags","summary":"","title":"Proxy","type":"tags"},{"content":"Joe Reis가 데이터 실무자 1,101명을 대상으로 서베이를 돌리고 2026년 데이터 엔지니어링 트렌드를 발표했다. 대규모 플랫폼 데이터 엔지니어링 팀을 이끄는 입장에서 이 트렌드를 우리 팀 아키텍처와 대조하며 잘하고 있는것과 앞으로 해야 할것을 정리해본다.\n우리 아키텍처 한 줄 요약 # S3를 중심 데이터 레이크로 두고 Apache Iceberg 테이블 포맷 위에 Trino(배치/애드혹 분석)와 StarRocks(실시간 OLAP)를 얹은 하이브리드 구조다. 수집은 Kafka + Debezium CDC와 Flink 스트리밍으로 처리하고 오케스트레이션은 Airflow를 깊이 커스터마이징해서 운영한다.\n[Services] → Kafka + Debezium CDC → Flink → S3 (Iceberg) ↓ ┌─────┴─────┐ │ │ Trino StarRocks (배치/애드혹) (실시간 OLAP) │ │ └─────┬─────┘ ↓ Dashboard 1. AI 활용 — 이미 하고 있는것과 넘어야 할 벽 # 트렌드 요약 # 서베이 응답자 82%가 AI를 매일 쓰지만 64%는 아직 실험 단계나 단순 작업에만 머물러있다. Joe Reis는 2026년 말이면 \u0026ldquo;AI-assisted\u0026quot;라는 수식어가 직무 기술에서 사라질 거라 예측한다.\n우리가 하고 있는 것 # AI 코딩 도구로 파이프라인을 개발하는 건 이미 일상이다. SQL 최적화나 코드 리뷰, 트러블슈팅에 LLM을 쓰고 있고 데이터 카탈로그와 연계한 자연어 기반 데이터 탐색도 시도하고있다.\n해야 할 것 # 개인 단위로 AI를 쓰는 수준을 넘어 팀 워크플로우 전체에 AI를 심는 게 과제다.\n파이프라인 이상 감지 자동화 스키마 변경에 자동 대응 데이터 품질 룰 자동 생성 Joe Reis가 말한 \u0026ldquo;10%의 AI-mature 팀\u0026quot;에 들려면 AI를 단순 보조 도구가 아닌 플랫폼 자체를 구성하는 요소로 녹여야 한다.\n2. 데이터 모델링 위기와 시맨틱 레이어 — 가장 큰 숙제 # 트렌드 요약 # 응답자 89%가 데이터 모델링에서 고통을 호소하고 시맨틱 모델을 쓰는 팀은 고작 5%다. Joe Reis는 시맨틱 레이어가 먼저 주류가 된 뒤 LLM이 스키마를 즉석에서 해석하는 방향으로 갈 거라 본다.\n우리가 하고 있는 것 # 데이터 카탈로그로 리니지와 메타데이터를 관리하고 있고 테이블 레이어 체계(L1/L2/L3)를 정의해 품질을 계층적으로 관리하려 한다. Airflow 커스텀 오퍼레이터를 통해 데이터 검증을 자동화하는 것도 운영 중이다.\n해야 할 것 # dbt 도입을 검토했으나 차세대 데이터 플랫폼 전환과 맞물려 중단된 상태다. 기존 파이프라인을 dbt로 이관하기보다 새 플랫폼으로 바로 이관하는 쪽을 검토하고 있는데 그 사이 데이터 변환을 표준화하고 모듈화하는 작업이 공백으로 남아있다. Joe Reis가 말한 \u0026ldquo;89%의 고통\u0026quot;과 정확히 겹친다.\n시맨틱 레이어도 손을 못 대고 있다. 비즈니스 메트릭 정의가 팀마다 다르고 같은 지표인데 SQL이 제각각인 문제가 있다. 서베이에서 시맨틱 모델 교육 수요가 19%로 높게 나온 것처럼 조직 전체가 데이터를 읽는 수준을 끌어올리는 일이 시급하다.\nAI 에이전트가 데이터를 자율적으로 활용하는 미래를 대비하면 잘 정의된 시맨틱 레이어는 선택이 아니라 필수다. 플랫폼 전환이 밀리더라도 모델링 표준과 시맨틱 정의는 따로 진행할 수 있고 진행해야 한다.\n3. 오케스트레이션 통합 — Airflow의 미래 # 트렌드 요약 # Airflow가 아직 지배적이지만 Dagster가 소규모 기업에서 12% 점유율을 보이며 바텀업으로 성장하고 있다. 오케스트레이션이 아예 없는 팀이 기업 규모와 상관없이 20%라는 점도 놀랍다.\n우리가 하고 있는 것 # Airflow를 깊이 커스터마이징해서 쓰고 있다. 자체 Provider 패키지를 만들었고 데이터 검증 자동화 오퍼레이터, 커스텀 전송 오퍼레이터 등 플랫폼에 맞는 기능을 직접 구현했다. 지금은 Airflow 3.x 메이저 버전 업그레이드를 진행하면서 Python 버전 업그레이드와 Breaking Change 대응을 계획하고 있다.\n해야 할 것 # Airflow에 깊이 투자한 건 강점이면서 동시에 기술 부채이기도 하다.\n커스텀 Provider 유지보수 부담 버전 업그레이드 때마다 호환성 이슈 여기에 AI 에이전트 오케스트레이션이라는 새 패러다임까지 대비해야 한다. Joe Reis가 예측한 대로 오케스트레이션이 플랫폼에 흡수되는 흐름도 지켜봐야 하고. 차세대 데이터 플랫폼과 맞물리는걸 고려하면 오케스트레이션 중장기 로드맵을 세우는 일이 시급하다.\n4. Lakehouse vs. Warehouse — 이미 답을 낸 영역 # 트렌드 요약 # 서베이에서 44%가 Warehouse, 27%가 Lakehouse, 12%가 Hybrid를 쓴다. Snowflake과 Databricks 기능이 수렴하면서 이 논쟁 자체가 의미를 잃어가고 있다. Joe Reis는 2026년 말이면 \u0026ldquo;warehouse vs. lakehouse\u0026rdquo; 논쟁이 구식으로 느껴질 거라 예측한다.\n우리가 하고 있는 것 # 여기서 우리 팀은 이미 정답에 가깝다. S3 위에 Iceberg 오픈 테이블 포맷을 표준으로 채택하고 용도에 따라 Trino와 StarRocks를 골라 쓰는 구조는 Warehouse도 Lakehouse도 아닌 양쪽 장점을 취한 아키텍처다. CDC 파이프라인으로 실시간 데이터를 Iceberg 테이블에 적재하고 배치와 실시간 분석을 같은 데이터 위에서 돌릴 수 있다.\n해야 할 것 # Iceberg v3의 Deletion Vector, Row Lineage 같은 새 기능을 쓰려면 쿼리 엔진 전반에서 호환성을 확보해야 한다. 지금 Trino와 StarRocks의 Iceberg v3 지원이 제한적이라 엔진 업그레이드 로드맵과 Iceberg 버전 전략을 연계해야 한다. 오픈 테이블 포맷 기반 아키텍처 거버넌스—카탈로그 통합, 접근 제어, 품질 보장—도 더 강화할 필요가 있다.\n5. 리더십이 병목이 되는 문제 — 가장 어렵고 가장 중요한 과제 # 트렌드 요약 # 데이터 엔지니어 22%가 \u0026ldquo;리더십 방향 부재\u0026quot;를 주요 이슈로 꼽았다. 레거시 기술 부채(26%)에 버금가는 수치다. Joe Reis는 2026년에 더 많은 데이터 팀이 해체되거나 엔지니어링 조직에 합병될 거라 경고한다.\n우리가 하고 있는 것 # 데이터 플랫폼 팀이 독립 조직으로 존재하며 인프라부터 수집, 변환, 분석 환경까지 End-to-End로 책임진다. 비즈니스 팀과 직접 소통하며 데이터 요건을 수렴하고 있다.\n해야 할 것 # 기술력만으로는 팀이 존재하는 이유를 증명할 수 없다. Joe Reis가 강조한 대로 \u0026ldquo;비즈니스 가치를 증명한 팀만 살아남는다.\u0026rdquo;\n데이터 플랫폼 ROI를 숫자로 측정하고 소통하는 체계 수립 AI 시대에 데이터 플랫폼이 맡을 역할에 대한 비전 수립 데이터 옵저버빌리티를 도입해 다운타임 줄이기 파이프라인 개발 생산성을 지표화해서 비즈니스 임팩트 보여주기 정리. 잘 하고 있는 것 vs. 해야 할 것 # 영역 잘 하고 있는 것 해야 할 것 AI 활용 개인 단위 AI 코딩 도구 적극 활용 팀 워크플로우에 AI 임베드, 운영 자동화 데이터 모델링 카탈로그 기반 메타데이터 관리, 레이어 체계 정의 시맨틱 레이어 도입, 데이터 변환 표준화 오케스트레이션 Airflow 깊은 커스터마이징, 3.x 업그레이드 진행 장기 오케스트레이션 전략, AI 에이전트 대응 Lakehouse/Warehouse Iceberg 기반 하이브리드 아키텍처 구축 완료 Iceberg v3 호환성, 거버넌스 체계 강화 리더십 End-to-End 플랫폼 팀 운영 비즈니스 임팩트 정량화, 데이터 옵저버빌리티 마치며 # Joe Reis 서베이에서 가장 인상적이었던 문장이 있다.\n\u0026ldquo;2026년 데이터 엔지니어링은 올바른 도구를 고르는 것보다 그 도구를 잘 활용할 조직적 근육을 키우는 게 더 중요하다.\u0026rdquo;\n우리 팀은 기술 스택 면에서 트렌드 앞쪽에 서 있다. Iceberg 기반 오픈 데이터 레이크, 실시간과 배치를 아우르는 하이브리드 아키텍처, 깊이 있는 Airflow 커스터마이징. 아직 여기까지 못 온 조직이 많다.\n하지만 기술적 우위만으로는 부족하다. 데이터 변환 표준화, 시맨틱 레이어, 데이터 옵저버빌리티, AI 네이티브 워크플로우. 그리고 무엇보다 비즈니스 가치를 증명하는 리더십. 2026년에 우리가 집중해야 할 방향이다.\n과거의 빚은 이자를 물고 있고 페이데이가 다가오고 있다.\n원문 참고: Where Data Engineering Is Heading in 2026 — Joe Reis\n","date":"2026년 2월 23일","externalUrl":null,"permalink":"/posts/2026-data-engineering-trends/","section":"글 목록","summary":"Joe Reis의 1,101명 서베이를 기반으로 한 2026년 데이터 엔지니어링 트렌드를 대규모 플랫폼 데이터 엔지니어링 팀의 현재 아키텍처와 대조하며 정리했습니다. 이미 잘 하고 있는 것과 앞으로 해야 할 것을 솔직하게 돌아봅니다.","title":"2026 데이터 엔지니어링 트렌드와 대규모 플랫폼의 현주소","type":"posts"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"Ai","type":"tags"},{"content":"Airflow 2.x End of Life가 2026년 4월 22일로 다가오고 있다. 우리 팀은 수백 개 DAG을 운영하는 프로덕션 환경에서 Airflow 3.x 마이그레이션을 진행했다. 그 과정에서 마주친 Breaking Changes와 단계적 업그레이드 전략, 대규모 DAG 환경에서 얻은 실전 교훈을 정리한 기록이다.\n왜 지금 마이그레이션해야 하는가 # Airflow 3.x에서 바뀐 것 # Airflow 3.x는 단순한 메이저 버전 업데이트가 아니다. 아키텍처 수준에서 큰 변화가 있었다.\nDAG 버전 관리. dag_id에 버전 서픽스를 붙이거나 스케줄 변경 시 스케줄링이 꼬이던 문제에서 해방된다. 네이티브 백필. CLI나 커스텀 플러그인에 의존하던 백필을 웹 UI에서 바로 실행할 수 있다. 이벤트/애셋 기반 트리거. 단순 cron 표현식을 넘어서 여러 스케줄링 방식을 쓸 수 있게 됐다. React 기반 웹 UI. Flask App Builder 기반에서 React로 전면 개편됐고 사용성이 많이 좋아졌다. 아키텍처 변화. API Server 등장 # 3.x에서 가장 큰 아키텍처 변화는 API Server가 메타 DB에 접근하는 유일한 관문이 됐다는 점이다.\nAirflow 2.x: Webserver ─── MetaDB Worker ────── MetaDB Scheduler ─── MetaDB DAG Code ──── MetaDB (직접 접근 가능) Airflow 3.x: API Server ── MetaDB (유일한 접근 경로) Webserver ─── API Server Worker ────── API Server Scheduler ─── API Server DAG Code ──── API Server (직접 접근 불가) 이 변화 때문에 DAG 최상위 코드에서 메타 DB에 직접 접근하던 패턴이 전부 깨진다. 마이그레이션에서 가장 영향이 큰 변경사항이다.\n사실 이 패턴은 2.x에서도 안티패턴이었다. DAG 최상위에서 Variable.get()이나 Connection.get_connection_from_secrets()를 호출하면 스케줄러가 DAG을 파싱할 때마다 DB 쿼리가 발생한다. DAG이 수백 개면 파싱 루프 한 바퀴에 DB 호출이 수천 번 일어날 수도 있다. 2.x에서는 모든 컴포넌트가 메타 DB에 직접 연결되어 있어서 동작은 했고 deprecation 경고만 뜨다보니 고칠 동기가 약했다. 3.x는 아키텍처 수준에서 이 안티패턴을 강제로 차단한 셈이다.\n# ❌ 안티패턴: DAG 파싱 시점에 DB를 찌르는 코드 my_var = Variable.get(\u0026#34;some_config\u0026#34;) # ✅ 올바른 방식: Jinja 템플릿으로 태스크 실행 시점에 접근 task = PythonOperator( task_id=\u0026#34;my_task\u0026#34;, python_callable=my_func, op_kwargs={\u0026#34;config\u0026#34;: \u0026#34;{{ var.value.some_config }}\u0026#34;}, ) 단계적 업그레이드 전략 # 한 번에 최신 버전으로 올리는 건 위험하다. 우리는 네 단계로 나눠서 접근했다.\n1단계. 2.x 최신 버전(2.11)으로 업데이트 (선택) # 3.x로 직접 올리다가 이슈가 생길 경우를 대비한 안전장치다. 2.11에서는 3.x에서 제거될 기능에 대한 deprecation 경고가 표시되므로 수정 대상 코드를 미리 파악할 수 있다.\n2단계. 3.0.x로 업데이트 # Python 3.9 환경에서는 최신 3.1.x가 아닌 3.0.x까지만 지원된다. Python 버전을 올리기 전에 Airflow 메이저 버전을 먼저 올린다.\n3단계. Python 버전 업그레이드 (3.9 → 3.12+) # Airflow 3.1.x는 Python 3.9를 지원하지 않는다. Python 3.12 이상을 목표로 하되 의존성 호환 이슈가 있으면 3.10이나 3.11로 타협한다.\n4단계. 3.1.x로 업데이트 # 최종적으로 최신 stable 릴리스로 올린다.\n환경별 순차 적용 # DEV → BETA \u0026amp; 개인환경 → STAGE → PROD 각 환경에서 충분히 검증한 후 다음 환경으로 넘어간다. DEV 환경에서 약 2주, BETA에서 1주간 검증했다.\nBreaking Changes와 대응 방법 # 1. schedule_interval → schedule # 가장 흔하게 마주치는 변경사항이다. 기존 schedule_interval에 전달하던 cron 표현식을 그대로 schedule에 넘기면 된다.\n# Before (Airflow 2.x) DAG( dag_id=\u0026#34;my_dag\u0026#34;, schedule_interval=\u0026#34;5 2 * * *\u0026#34;, ) # After (Airflow 3.x) DAG( dag_id=\u0026#34;my_dag\u0026#34;, schedule=\u0026#34;5 2 * * *\u0026#34;, ) 단순 치환이지만 DAG 수가 수백 개라면 누락 없이 전부 바꿔야 한다. CI에서 자동으로 검증하는 방법은 뒤에서 다룬다.\n2. 존재하지 않는 오퍼레이터 인자 전달 불가 # Airflow 3.x에서는 개별 태스크가 메타 DB상에 시리얼라이즈된 DAG을 받아 실행하는 구조로 바뀌었다. allow_illegal_arguments 설정이 제거되면서 오퍼레이터에 정의되지 않은 인자를 전달하면 DAG 임포트 자체가 실패한다.\n# 이런 코드가 2.x에서는 경고 없이 동작했지만, 3.x에서는 에러가 발생한다 MyOperator( task_id=\u0026#34;my_task\u0026#34;, num_partition=10, # 실제 인자명은 num_partitions (복수형) ) TypeError: Invalid arguments were passed to MyOperator (task_id: my_task). Invalid arguments were: **kwargs: {\u0026#39;num_partition\u0026#39;: 10} 이 변경은 오히려 잠재적 버그를 발견하는 계기가 된다. 오랫동안 오타가 있는 인자가 무시되고 있었다면 이번에 바로잡을 수 있다.\n3. Deprecated 컨텍스트/템플릿 변수 제거 # 2.x에서 deprecated 경고만 뜨던 변수가 3.x에서는 완전히 제거됐다. 가장 영향이 큰 건 execution_date다.\nDeprecated 변수 대체 변수 {{ execution_date }} {{ logical_date }} 또는 {{ data_interval_start }} {{ next_execution_date }} {{ data_interval_end }} {{ prev_execution_date_success }} {{ prev_data_interval_start_success }} Jinja 템플릿과 Python 코드 양쪽 모두 수정해야 한다.\n# Jinja 템플릿 # Before \u0026#34;SELECT * FROM table WHERE dt = \u0026#39;{{ execution_date }}\u0026#39;\u0026#34; # After \u0026#34;SELECT * FROM table WHERE dt = \u0026#39;{{ logical_date }}\u0026#39;\u0026#34; # Python context # Before execution_date = context[\u0026#34;execution_date\u0026#34;] # After logical_date = context[\u0026#34;logical_date\u0026#34;] 4. DB별 Operator 통합 → SQLExecuteQueryOperator # MySQL, PostgreSQL, Trino 등 DB별로 따로 있던 Operator가 SQLExecuteQueryOperator 하나로 합쳐졌다. 내부적으로 커넥션 타입에 따라 적절한 Hook을 알아서 골라 쓴다.\n# Before (Airflow 2.x) from airflow.providers.mysql.operators.mysql import MySqlOperator MySqlOperator( task_id=\u0026#34;task\u0026#34;, mysql_conn_id=\u0026#34;my_conn\u0026#34;, sql=\u0026#34;SELECT 1\u0026#34; ) # After (Airflow 3.x) from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator SQLExecuteQueryOperator( task_id=\u0026#34;task\u0026#34;, conn_id=\u0026#34;my_conn\u0026#34;, # DB별 conn_id → 통합 conn_id sql=\u0026#34;SELECT 1\u0026#34; ) 5. DummyOperator → EmptyOperator # 2.x와 3.x 양쪽에서 모두 동작하는 임포트 경로를 써야 한다.\n# v2에서만 동작 (3.x에서 에러) from airflow.operators.dummy import DummyOperator # v3에서만 동작 from airflow.providers.standard.operators.empty import EmptyOperator # v2 \u0026amp; v3 모두 호환 (권장) from airflow.operators.empty import EmptyOperator 6. SimpleHttpOperator → HttpOperator # # Before from airflow.providers.http.operators.http import SimpleHttpOperator # After from airflow.providers.http.operators.http import HttpOperator 7. Connection getter 메서드 → 속성 직접 참조 # Connection 클래스 인터페이스가 좀 더 Pythonic하게 바뀌었다.\n# Before conn = BaseHook.get_connection(\u0026#34;my_conn\u0026#34;) password = conn.get_password() host = conn.get_host() # After conn = BaseHook.get_connection(\u0026#34;my_conn\u0026#34;) password = conn.password host = conn.host 8. 기타 패키지 경로 변경 # # cached_property # Before: from airflow.compat.functools import cached_property # After: from functools import cached_property (Python 내장) # KubernetesPodOperator # Before: from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import ... # After: from airflow.providers.cncf.kubernetes.operators.pod import ... 9. provide_context=True 제거 # Airflow 2.0부터 PythonOperator에서 context가 자동 주입된다. provide_context=True는 아무 역할도 하지 않는데 남아 있는 코드가 꽤 많다.\n# Before PythonOperator( task_id=\u0026#34;my_task\u0026#34;, python_callable=my_func, provide_context=True, ) # After PythonOperator( task_id=\u0026#34;my_task\u0026#34;, python_callable=my_func, ) 10. from airflow.models import DAG → from airflow import DAG # airflow.models.DAG은 v3에서 deprecated다. from airflow import DAG으로 바꾸면 v2/v3 양쪽 다 동작한다.\n# Before from airflow.models import DAG # After from airflow import DAG from airflow import models 후 models.DAG()으로 쓰는 패턴은 바꿀 필요 없다.\n11. @apply_defaults 데코레이터 제거 # Airflow 2.0부터 BaseOperator가 자동 처리한다. 커스텀 Operator에 남아 있으면 제거하면 된다.\n# Before from airflow.utils.decorators import apply_defaults class CustomOperator(BaseOperator): @apply_defaults def __init__(self, **kwargs): super().__init__(**kwargs) # After class CustomOperator(BaseOperator): def __init__(self, **kwargs): super().__init__(**kwargs) 12. concurrency → max_active_tasks # DAG 파라미터 concurrency가 v3에서 제거된다. 의미는 동일하지만 이름이 더 명확해졌다.\n# Before with DAG(dag_id=\u0026#34;my_dag\u0026#34;, concurrency=5) as dag: # After with DAG(dag_id=\u0026#34;my_dag\u0026#34;, max_active_tasks=5) as dag: 13. TaskInstance 직접 생성 제거 # kwargs[\u0026quot;ti\u0026quot;]가 이미 TaskInstance 객체다. 별도로 생성할 이유가 없다. ti.execution_date 접근과 TaskInstance(task, execution_date) 생성자는 v3에서 deprecated다.\n# Before def my_callable(**kwargs): ti = TaskInstance(kwargs[\u0026#34;ti\u0026#34;].task, kwargs[\u0026#34;ti\u0026#34;].execution_date) ti.xcom_push(key=\u0026#34;my_key\u0026#34;, value=result) # After def my_callable(**kwargs): ti = kwargs[\u0026#34;ti\u0026#34;] ti.xcom_push(key=\u0026#34;my_key\u0026#34;, value=result) 대규모 DAG 환경에서 마이그레이션하기 # ruff로 비호환 항목 자동 검출 # 수백 개 DAG을 눈으로 훑는 건 불가능하다. ruff의 Airflow 전용 규칙을 쓰면 비호환 코드를 자동으로 잡아준다.\n# 특정 파일 검사 ruff check --preview --select AIR dags/my_dag.py # 팀 디렉터리 전체 검사 ruff check --preview --select AIR dags/my_team/ Rule 대상 AIR301 deprecated 데코레이터 (@apply_defaults 등) AIR302 deprecated 파라미터, import 경로, 템플릿 변수 CI 파이프라인에 v3 호환성 검증 추가 # MR(Merge Request) 단계에서 v3 호환성을 자동 검증하는 CI 잡을 추가했다.\n# .gitlab-ci.yml 예시 airflow-v3-compat-check: stage: test image: apache/airflow:3.0.6-python3.12 script: - pip install -r requirements.txt - ruff check --preview --select AIR dags/ - python -m py_compile dags/**/*.py - airflow dags list --output table allow_failure: true # 초기에는 경고만, 이후 필수로 전환 처음에는 allow_failure: true로 시작해서 현황을 파악하고 마이그레이션 기한이 다가오면 필수 검증으로 전환한다.\n실전 수정 이력: 3라운드에 걸친 일괄 수정 # 우리는 전체 DAG 레포지토리에 대해 3차에 걸쳐 비호환 항목을 수정했다.\n차수 변경 파일 수 주요 수정 항목 1차 252개 schedule_interval→schedule, DummyOperator→EmptyOperator 2차 35개 1차 누락분 보완, provide_context 제거, 파라미터 오입력 수정 3차 21개 DAG import 경로, Jinja 템플릿 변수, concurrency→max_active_tasks, TaskInstance 직접 생성 제거 1차에서 252개 파일을 한꺼번에 수정했는데, 그래도 누락이 생겼다. ruff가 잡아주지 못하는 패턴(파라미터 오입력, 사내 커스텀 오퍼레이터의 불필요 인자 등)은 수작업으로 찾아야 했다. 한 번에 끝낼 수 있을 거라고 기대하지 않는 게 좋다.\n이관 허들을 의도적으로 높여라 # 우리가 얻은 가장 큰 교훈이다.\n모든 DAG 코드에 일괄로 호환성 패치를 적용할 수도 있었다. 하지만 의도적으로 이관 난이도를 유지하기로 했다. 이유는 분명하다.\n관성적으로 운영되고 있지만 실제로는 쓰지 않는 DAG이 상당수 존재한다.\n마이그레이션을 계기로 DAG 소유자가 \u0026ldquo;이 DAG이 정말 필요한가?\u0026ldquo;를 스스로 검토하도록 유도한 것이다. 결과적으로 상당수의 불필요한 DAG이 정리됐고 운영 부담도 줄었다.\n구체적으로는 이렇게 진행했다.\n비활성 DAG 목록을 취합해 공유 시트에 정리 DAG 소유자와 소속 부서에 유지 여부를 기한 내 확인하도록 안내 기한 내 응답이 없으면 비활성화 v3 호환성 패치는 소유자가 직접 수행 커스텀 Provider 패키지 선제 대응 # 사내 커스텀 오퍼레이터나 유틸리티를 Provider 패키지로 제공하고 있다면 Airflow 코어의 Breaking Changes를 흡수하는 호환 레이어를 먼저 준비해야 한다.\n커스텀 Provider 패키지를 네 차례에 걸쳐 점진적으로 업데이트했다.\nv3.0.0. 기본 호환성 확보 v3.0.1. 오퍼레이터 인자 검증 대응 v3.0.2. deprecated 컨텍스트 변수 호환 레이어 추가 v3.0.3. 문서 및 마이너 버그 수정 사용자 코드 변경은 최소화하되 Provider 패키지 내부에서 v2/v3 분기 처리를 하는 식으로 접근했다.\nHelm Chart 업데이트 # Kubernetes 환경에서 Airflow를 운영한다면 Helm Chart도 같이 업데이트해야 한다. 3.x에서 도입된 DAG Processor 컴포넌트와 API Server 분리를 반영해야 하기 때문이다.\n기존 차트 버전에서 호환성을 먼저 확인하고 안정화되면 최신 stable 버전으로 올리는 2단계 접근이 안전하다.\nFAB Auth Manager 이슈 # 3.x에서 React 기반으로 웹이 전면 개편되면서 기존 Flask App Builder(FAB) 기반 Auth Manager가 기본 패키지에서 빠졌다. 커스텀 Security Manager를 쓰고 있다면 별도 설치와 코드 수정이 필요하다.\nFailed to import WoowaSecurityManager, using default security manager 이런 에러가 나오면 FAB Auth Manager 패키지를 명시적으로 설치하고 임포트 경로를 업데이트해야 한다.\n마치며 # Airflow 3.x 마이그레이션은 단순한 버전 업그레이드가 아니다. 아키텍처가 바뀌었고 코드 호환성이 깨졌으며 인프라도 같이 손봐야 한다.\n배운 것을 정리하면 이렇다.\n단계적으로 올려라. 한 번에 최신 버전으로 뛰지 말고 2.11 → 3.0.x → Python 업그레이드 → 3.1.x 순서로 진행하라. CI에서 자동 검증하라. 수백 개 DAG 호환성을 사람이 확인하는 건 불가능하다. 마이그레이션을 정리 기회로 삼아라. 이관 허들을 유지해서 불필요한 DAG을 자연스럽게 걸러내라. 커스텀 Provider를 선제 업데이트하라. 사용자 코드 변경을 최소화하는 호환 레이어를 먼저 만들어라. Airflow 2.x EOL까지 아직 시간이 있다고 안심하지 말자. 대규모 환경에서 마이그레이션은 예상보다 오래 걸린다. 지금 시작해도 늦지 않다.\n참고 자료\nUpgrading to Airflow 3 - Apache Airflow Documentation Apache Airflow 3 is Generally Available! Airflow 3.x Release Notes ","date":"2026년 2월 23일","externalUrl":null,"permalink":"/posts/airflow-3-migration-guide/","section":"글 목록","summary":"Airflow 2.x EOL을 앞두고 3.x로 마이그레이션한 실전 경험을 공유한다. Breaking Changes, 단계적 업그레이드 전략, DAG 호환성 확보 방법, 수백 개 DAG을 운영하는 환경에서 배운 교훈을 정리했다.","title":"Airflow 3.0 마이그레이션 가이드: 대규모 DAG 환경에서의 실전 경험","type":"posts"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/bali/","section":"Tags","summary":"","title":"Bali","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/compression/","section":"Tags","summary":"","title":"Compression","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/data-engineering/","section":"Tags","summary":"","title":"Data-Engineering","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/data-pipeline/","section":"Tags","summary":"","title":"Data-Pipeline","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/digital-nomad/","section":"Tags","summary":"","title":"Digital-Nomad","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/family/","section":"Tags","summary":"","title":"Family","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"Gpu","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/infrastructure/","section":"Tags","summary":"","title":"Infrastructure","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/categories/life/","section":"Categories","summary":"","title":"Life","type":"categories"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/mig/","section":"Tags","summary":"","title":"Mig","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/migration/","section":"Tags","summary":"","title":"Migration","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/nvidia/","section":"Tags","summary":"","title":"Nvidia","type":"tags"},{"content":"GPU가 비싸다. A100 한 장이 수천만 원인데 대부분의 워크로드는 80GB 메모리를 다 쓰지 않는다. Jupyter 노트북에서 간단한 실험 돌리는 데 GPU 한 장을 통째로 할당하면 나머지 70GB는 그냥 놀게 된다.\nNVIDIA MIG(Multi-Instance GPU)는 이 문제를 풀어준다. 물리 GPU 하나를 최대 7개 독립 인스턴스로 쪼개서 여러 워크로드가 동시에 돌아간다. 이 글은 A100 GPU 4장 환경에서 MIG를 설정하고 Kubernetes까지 연동한 과정을 기록한 것이다.\nMIG란 무엇인가 # MIG는 하나의 물리 GPU를 여러 독립 GPU 인스턴스로 분할하는 기술이다. 각 인스턴스가 자체 메모리와 컴퓨트 유닛을 갖고있어서 서로 간섭하지 않는다. 한 인스턴스에서 OOM이 나도 다른 인스턴스에는 영향이 없다.\nA100 80GB 기준 분할 옵션은 이렇다.\n프로필 메모리 SM 수 GPU당 최대 인스턴스 1g.10gb ~10GB 14 7 2g.20gb ~20GB 28 3 3g.40gb ~40GB 42 2 7g.80gb ~80GB 98 1 7분할(1g.10gb)을 하면 GPU 4장에서 인스턴스 28개가 나온다. 단순 계산으로 GPU 활용률이 4배 이상 올라간다.\n지원 GPU 확인 # MIG는 A100, H100 등 특정 GPU에서만 쓸 수 있다. A40이나 V100은 MIG를 지원하지 않는다. 우리도 처음에 A40 서버에서 시도했다가 안 되는 걸 확인하고 A100 서버로 전환했다.\nnvidia-smi 출력에서 MIG M. 컬럼을 확인하면 된다.\n# MIG 미지원 GPU (A40) | MIG M. | | N/A | ← 지원하지 않음 # MIG 지원 GPU (A100) | MIG M. | | Disabled | ← 지원하지만 비활성화 상태 | Enabled | ← 활성화됨 지원 GPU 목록은 NVIDIA 공식 문서에서 확인할 수 있다.\nMIG 설정하기 # 사전 준비. nvidia-mig-manager 비활성화 # MIG를 켜기 전에 nvidia-mig-manager.service를 반드시 비활성화해야 한다. 이 데몬이 부팅할 때마다 MIG를 disabled로 되돌린다.\nsudo systemctl disable nvidia-mig-manager.service mig-parted 설치 # nvidia-mig-parted는 NVIDIA에서 제공하는 MIG 설정 도구다. 선언적으로 config 파일을 작성하면 원하는 분할 구성을 한번에 적용해준다.\n# deb 패키지 설치 (Ubuntu/Debian) curl -LO https://github.com/NVIDIA/mig-parted/releases/download/v0.5.2/nvidia-mig-manager_0.5.2-1_amd64.deb sudo dpkg -i nvidia-mig-manager_0.5.2-1_amd64.deb MIG 모드 활성화 # sudo nvidia-smi -mig 1 이 명령을 실행하면 MIG 모드가 pending 상태가 된다. 적용하려면 리부트가 필요하다.\nsudo reboot VM 환경 주의사항. GPU passthrough로 사용하는 가상머신에서는 nvidia-smi --gpu-reset을 지원하지 않는다. 리부트 없이 MIG를 활성화할 수 없으므로 베어메탈 서버를 쓰는 게 좋다. 우리도 이 문제 때문에 VM에서 베어메탈로 옮겼다.\nconfig.yaml 작성 # mig-parted는 YAML 파일로 분할 구성을 선언한다. 한 파일에 여러 프리셋을 정의해놓고 필요할 때 골라 쓸 수 있다.\nversion: v1 mig-configs: # MIG 비활성화 all-disabled: - devices: all mig-enabled: false # 전체 GPU를 7분할 (1g.10gb × 7) all-1g.10gb: - devices: all mig-enabled: true mig-devices: \u0026#34;1g.10gb\u0026#34;: 7 # 전체 GPU를 3분할 (2g.20gb × 3) all-2g.20gb: - devices: all mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 # 전체 GPU를 2분할 (3g.40gb × 2) all-3g.40gb: - devices: all mig-enabled: true mig-devices: \u0026#34;3g.40gb\u0026#34;: 2 # GPU를 통째로 사용 (7g.80gb × 1) all-7g.80gb: - devices: all mig-enabled: true mig-devices: \u0026#34;7g.80gb\u0026#34;: 1 # GPU별로 다른 분할 구성 custom-config: - devices: [0] mig-enabled: true mig-devices: \u0026#34;3g.40gb\u0026#34;: 2 - devices: [1] mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 \u0026#34;1g.10gb\u0026#34;: 1 - devices: [2] mig-enabled: true mig-devices: \u0026#34;2g.20gb\u0026#34;: 3 \u0026#34;1g.10gb\u0026#34;: 1 - devices: [3] mig-enabled: true mig-devices: \u0026#34;1g.10gb\u0026#34;: 7 마지막 custom-config가 포인트다. GPU마다 다른 프로필을 섞어서 구성할 수 있다. 왜 이렇게 해야하는지는 뒤에서 설명한다.\n설정 적용 # # 균일한 7분할 적용 sudo nvidia-mig-parted apply -f ./config.yaml -c all-1g.10gb # 또는 커스텀 구성 적용 sudo nvidia-mig-parted apply -f ./config.yaml -c custom-config -d 플래그를 추가하면 디버그 로그를 볼 수 있다. 적용이 끝나면 nvidia-smi -L로 인스턴스 목록을 확인한다.\nGPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-xxxx) MIG 3g.40gb Device 0: (UUID: MIG-xxxx) MIG 3g.40gb Device 1: (UUID: MIG-xxxx) GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-xxxx) MIG 2g.20gb Device 0: (UUID: MIG-xxxx) MIG 2g.20gb Device 1: (UUID: MIG-xxxx) MIG 2g.20gb Device 2: (UUID: MIG-xxxx) MIG 1g.10gb Device 3: (UUID: MIG-xxxx) ... 재부팅 시 자동 적용 # MIG 설정은 재부팅하면 날아간다. systemd 서비스로 등록해서 부팅할 때 자동으로 적용되게 만든다.\n# /etc/systemd/system/nvidia-mig-config.service [Unit] Description=Apply NVIDIA MIG Configuration After=nvidia-persistenced.service [Service] Type=oneshot ExecStart=/usr/bin/nvidia-mig-parted apply -f /home/user/config.yaml -c custom-config RemainAfterExit=yes [Install] WantedBy=multi-user.target sudo systemctl daemon-reload sudo systemctl enable nvidia-mig-config.service 실전에서 배운 것. 균일 분할로는 부족하다 # 처음에는 단순하게 생각했다. GPU 4장을 전부 1g.10gb × 7로 쪼개면 인스턴스 28개가 나온다. 가장 많은 사용자가 동시에 쓸 수 있으니 좋지 않을까?\n큰 모델은 MIG에서 안 돌아간다 # 현실은 달랐다. 추천 모델 학습하는 팀에서 문제가 터졌다. 모델이 GPU 메모리를 30GB 넘게 쓰는데 MIG 인스턴스 하나는 10GB밖에 없으니 CUDA OOM으로 로드 자체가 안 됐다.\nLLM도 마찬가지였다. Llama 2 7B 모델은 4bit 양자화를 해도 MIG 디바이스에서 로드가 안 됐다. 13B은 말할것도 없다.\n결론은 명확하다. 워크로드에 따라 분할 전략을 다르게 가져가야 한다.\nGPU별 커스텀 분할 # 팀 내 워크로드를 분석해보니 이런 구성이 됐다.\nGPU 프로필 용도 GPU 0 3g.40gb × 2 중형 모델 학습 (메모리 40GB급) GPU 1 2g.20gb × 3 + 1g.10gb × 1 중소형 실험 GPU 2 2g.20gb × 3 + 1g.10gb × 1 중소형 실험 GPU 3 1g.10gb × 7 Jupyter 노트북, 가벼운 배치 작업 대형 모델 학습이 필요하면 MIG를 꺼둔 GPU를 별도로 남겨두는 방법도 있다. 우리는 LLM 실험 수요가 늘면서 일부 GPU의 MIG를 끄기도 했다.\n주의. MIG 슬라이스를 섞어 구성할 때 물리적 제약이 있다. GPU 내부 메모리 슬라이스는 왼쪽에서 오른쪽으로 할당되며 수직으로 공유할 수 없다. NVIDIA 공식 문서에서 지원하는 조합을 꼭 확인해야 한다.\nKubernetes 연동 # MIG 인스턴스를 Kubernetes에서 쓰려면 NVIDIA device plugin 설정을 바꿔야 한다.\nMIG Strategy 이해하기 # device plugin에는 세 가지 MIG 전략이 있다.\n전략 리소스 이름 설명 none nvidia.com/gpu MIG를 인식하지 않음. 물리 GPU 단위 할당 single nvidia.com/gpu MIG 인스턴스를 자동 할당. 기존 워크로드 호환 mixed nvidia.com/mig-{profile} MIG 프로필별로 별도 리소스 노출 single 전략은 기존 워크로드가 nvidia.com/gpu: 1로 요청하던 걸 수정 없이 MIG 인스턴스로 연결해준다. 편하긴 한데 어떤 크기의 인스턴스를 받을지 제어할 수가 없다.\nmixed 전략은 nvidia.com/mig-3g.40gb: 1처럼 프로필을 명시해서 요청한다. GPU별로 다른 분할을 쓴다면 mixed가 맞다.\ndevice plugin 설정 변경 # # values.yaml migStrategy: mixed helm upgrade -i nvdp nvdp/nvidia-device-plugin \\ --namespace nvidia-device-plugin \\ --create-namespace \\ --version 0.12.3 \\ -f ./values.yaml 적용 후 nvidia-device-plugin Pod를 재시작해야 노드 리소스에 반영된다. kubectl describe node로 MIG 리소스가 보이는지 확인한다.\nAllocatable: nvidia.com/mig-1g.10gb: 9 nvidia.com/mig-2g.20gb: 6 nvidia.com/mig-3g.40gb: 2 JupyterHub에서 MIG 사용 # JupyterHub 프로필에서 리소스 요청을 MIG 타입으로 바꾼다.\n# 변경 전 (물리 GPU 할당) extra_resource_limits: nvidia.com/gpu: \u0026#34;1\u0026#34; # 변경 후 (MIG 인스턴스 할당) extra_resource_limits: nvidia.com/mig-1g.10gb: \u0026#34;1\u0026#34; mixed 전략에서 nvidia.com/gpu: 1을 요청하면 물리 GPU 전체가 할당되니까 주의해야 한다. MIG 인스턴스를 쓰려면 반드시 프로필 이름을 명시해야 한다.\nAirflow KubernetesPodOperator에서 MIG 사용 # Airflow에서 GPU 작업 실행할 때도 리소스 요청을 MIG 타입으로 바꾼다.\nresources = { \u0026#34;cpu\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;16Gi\u0026#34;, \u0026#34;nvidia.com/mig-2g.20gb\u0026#34;: \u0026#34;1\u0026#34;, } KubernetesPodOperator( ... container_resources=k8s.V1ResourceRequirements( requests=resources, limits=resources, ), ... ) 운영 팁 # MIG 설정 변경 타이밍 # MIG 구성을 바꾸려면 그 GPU의 모든 프로세스를 종료해야 한다. 운영 환경에서는 배치 작업이 없는 시간대를 골라서 작업한다. 우리는 배치 스케줄이 새벽 4시 이후에 몰려 있어서 밤 9시 이후에 MIG 재설정을 진행했다.\n서버 전체 GPU에 MIG를 걸어야 한다 # 한 서버에서 일부 GPU만 MIG를 켜고 나머지는 끄는 구성은 동작하지 않는다. 서버 안의 모든 GPU가 MIG enabled이거나 전부 disabled여야 한다. 대형 모델 학습용으로 MIG 없는 GPU가 필요하면 별도 서버를 쓰거나 MIG config에서 7g.80gb: 1로 설정해서 GPU 전체를 하나의 인스턴스로 노출하면 된다.\nGPU Capacity 변화 # MIG 적용 전후로 Kubernetes 클러스터의 GPU capacity가 크게 달라진다. 우리 환경에서는 GPU 4장(capacity 4)이 MIG 적용 후 인스턴스 기준으로 20개 이상이 됐다. 모니터링이랑 알럿 기준을 같이 조정해야 한다.\n마치며 # MIG는 GPU 활용률을 높이는 괜찮은 방법이다. 만능은 아니다. 실전에서 배운걸 정리하면 이렇다.\n지원 GPU를 먼저 확인하라. A100, H100 같은 특정 모델만 MIG를 지원한다. A40이나 V100은 안 된다. VM 환경은 피하라. GPU passthrough에서는 GPU reset이 안 돼서 MIG 활성화가 까다롭다. 베어메탈이 편하다. 균일 분할로 시작하되 커스텀 분할로 넘어가라. 워크로드를 분석해서 GPU별로 프로필을 섞는 게 실용적이다. 대형 모델은 MIG로 안 된다. LLM이나 대규모 추천 모델은 GPU 전체 메모리가 필요하다. MIG 없는 GPU를 따로 확보해야 한다. GPU는 비싸지만 잘 쪼개면 기존 인프라에서 더 많은걸 해낼 수 있다.\n참고 자료.\nNVIDIA MIG User Guide nvidia-mig-parted (GitHub) NVIDIA Device Plugin for Kubernetes Getting the Most Out of the A100 GPU with MIG ","date":"2026년 2월 23일","externalUrl":null,"permalink":"/posts/nvidia-mig-setup-guide/","section":"글 목록","summary":"A100 GPU 4장으로 최대 28개의 독립 GPU 인스턴스를 만들 수 있다. MIG 개념부터 mig-parted 설치, 슬라이스 구성, Kubernetes 연동까지 실전 경험을 정리했다.","title":"NVIDIA MIG 설정 가이드. A100 GPU 하나를 여러 개로 쪼개 쓰기","type":"posts"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/olap/","section":"Tags","summary":"","title":"Olap","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/open-source/","section":"Tags","summary":"","title":"Open-Source","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/orchestration/","section":"Tags","summary":"","title":"Orchestration","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/performance-tuning/","section":"Tags","summary":"","title":"Performance-Tuning","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/real-time/","section":"Tags","summary":"","title":"Real-Time","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/remote-work/","section":"Tags","summary":"","title":"Remote-Work","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/ruff/","section":"Tags","summary":"","title":"Ruff","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/starburst/","section":"Tags","summary":"","title":"Starburst","type":"tags"},{"content":"Trino를 프로덕션 쿼리 엔진으로 운영하는 팀이라면 2025년 한 해 동안 느꼈을 거다. 릴리스가 뜸해졌다. 감각이 아니라 숫자로 드러나는 변화다. 2024년 30개였던 Trino 오픈소스 릴리스가 2025년에는 11개로 줄었다. 63% 감소.\n이 글에서는 Starburst가 왜, 어떻게 AI 중심 플랫폼 기업으로 전환했는지 살펴보고 Trino 오픈소스를 쓰는 입장에서 이 변화를 어떻게 바라봐야 하는지 정리한다.\nTrino 릴리스, 무슨 일이 일어났나 # 릴리스 빈도가 급격히 줄었다 # Trino 오픈소스 릴리스 패턴을 분기별로 보면 감소 추세가 뚜렷하다.\n기간 릴리스 수 평균 주기 비고 2024 Q4 9개 주 1회 안정적 패턴 2025 Q1 6개 2주 1회 감소 시작 2025 Q2 2개 월 1회 급격한 감소 2025 Q3 1개 분기 1회 최저점 2025 Q4 2개 1.5개월 1회 여전히 저조 2024년 Q4까지만 해도 매주 새 릴리스가 나왔다. 그런데 2025년 Q2부터 월 1회 수준으로 떨어졌고 Q3에는 분기 통틀어 릴리스가 단 1개였다. Starburst Enterprise 릴리스도 마찬가지로 극소수에 그쳤다.\n숫자만 놓고 보면 심각해 보이지만 Trino가 \u0026ldquo;쇠퇴\u0026quot;한다는 뜻은 아니다. Starburst가 내린 전략적 선택이다.\n왜 줄었는가 # Trino 오픈소스에 Starburst가 얼마나 기여했는지 보면 답이 나온다. 2024년 기준 Starburst 팀이 Trino 전체 커밋의 84% 를 차지했다. 기여자 138명에 커밋 2,822개, 참여 기업 50곳 이상이었지만 실질적인 개발 동력은 Starburst였다. 그 Starburst가 엔지니어링 리소스를 다른 곳에 쏟기 시작한 거다.\n그 \u0026ldquo;다른 곳\u0026quot;이 바로 AI다.\nStarburst의 AI 피벗 # 포지셔닝이 바뀌었다 # Starburst 공식 메시징 변화를 추적해보면 전환이 얼마나 의도적이었는지 알 수 있다.\n시기 포지셔닝 핵심 메시지 ~2023 Open Data Lakehouse Company Trino 기반 분산 쿼리 엔진 2024 Data Lake Analytics Platform 페더레이션 쿼리 + Iceberg 2025 Data Platform for Apps and AI AI Agent + Agentic Workforce \u0026ldquo;Open Data Lakehouse\u0026quot;에서 \u0026ldquo;Data Platform for Apps and AI\u0026quot;로. 단순한 마케팅 변화가 아니라 제품 로드맵 전체가 이 방향으로 정렬됐다.\n2025년 주요 발표 타임라인 # 2025년 5월 — Launch Point\nStarburst는 AI Agent와 AI Workflows를 공식 발표했다.\nAI Agent. 자연어로 데이터를 쿼리하는 인터페이스다. \u0026ldquo;What were our sales in Europe last quarter?\u0026rdquo; 같은 질문이 자동으로 SQL로 변환된다. Air-gapped 환경(금융, 의료, 정부)을 명시적으로 지원하며 Google Agent2Agent 프로토콜과 Anthropic Model Context Protocol(MCP)도 지원한다. AI Workflows. Vector embeddings를 Iceberg 테이블에 저장하고 구조화/반구조화/비구조화 데이터를 AI 학습에 활용하는 파이프라인이다. RAG(Retrieval-Augmented Generation)를 네이티브로 지원한다. 기타. Starburst Data Catalog(Hive Metastore 대체), Automated Table Maintenance(파일 정리 및 컴팩션 자동화), Native ODBC Driver, Role-based Query Routing 2025년 10월 — AI \u0026amp; Datanova 2025\n여기서 Starburst는 Agentic Workforce 플랫폼과 Lakeside AI Architecture를 발표하며 한 단계 더 나아갔다.\n핵심은 Model-to-Data 아키텍처다. 기존에는 데이터를 중앙 웨어하우스로 모은 뒤 AI 모델을 돌렸다. Starburst는 반대로 AI 모델을 데이터가 있는 곳으로 보낸다.\n기존 접근: Data → Centralized Warehouse → AI Model Starburst: AI Model → Federated Data (where it lives) 데이터를 이동시키지 않으므로 데이터 주권(GDPR, Schrems II)을 유지하면서도 통합 분석이 가능하다는 논리다. Citi, HSBC 같은 글로벌 금융기관이 165개국에 흩어진 데이터를 이 방식으로 통합하고 있다고 한다.\n기술 스택이 달라졌다 # Starburst 기술 스택을 레이어로 표현하면 2025년에 뭐가 추가됐는지 보인다.\n┌─────────────────────────────────────┐ │ AI Agent \u0026amp; Agent2Agent Protocol │ ← 2025 NEW ├─────────────────────────────────────┤ │ AI Workflows (Vector Store) │ ← 2025 NEW ├─────────────────────────────────────┤ │ Starburst Data Catalog │ ← 2025 NEW ├─────────────────────────────────────┤ │ Lakehouse (Trino + Iceberg) │ ├─────────────────────────────────────┤ │ Federated Data Sources (50+) │ └─────────────────────────────────────┘ Trino는 여전히 기반 레이어에 있지만 새 기능은 전부 위에서 만들어지고 있다. Trino 오픈소스 릴리스가 줄어든 이유가 여기 있다. 엔지니어링 리소스가 상위 레이어로 이동한 거다.\nVector Store on Iceberg. 주목할 만한 기술적 시도 # 2025년 Starburst 발표 중 기술적으로 가장 흥미로운 건 Apache Iceberg 테이블에 직접 vector embeddings를 저장하는 접근이다.\n왜 의미가 있는가.\n별도 벡터 DB가 필요 없다. Pinecone, Weaviate, Milvus 같은 전용 벡터 데이터베이스를 운영 안 해도 된다. 기존 데이터 엔지니어링 스킬을 그대로 쓴다. Iceberg 테이블을 다루던 방식 그대로 벡터 데이터를 관리할 수 있다. Iceberg가 제공하는 기능이 벡터 데이터에도 먹힌다. Time travel, ACID 트랜잭션, 스키마 진화, 파티셔닝 전부 벡터 데이터에서도 쓸 수 있다. 거버넌스 정책을 일관되게 적용 가능하다. 구조화 데이터와 벡터 데이터에 동일한 접근 제어, 감사 로그, 데이터 마스킹 정책을 걸 수 있다. 오픈 포맷이므로 vendor lock-in이 없다. AI 워크로드가 데이터 플랫폼에서 빠질 수 없는 요소가 되는 미래를 가정하면 꽤 실용적인 접근이다. 전용 벡터 DB 대비 검색 성능에는 트레이드오프가 있겠지만 운영 복잡도를 줄이고 거버넌스를 통합할 수 있다는 점은 엔터프라이즈 환경에서 매력적이다.\n비즈니스 성과. 시장에서 통하고 있는가 # AI 피벗이 단순한 마케팅이 아니라는 건 비즈니스 지표가 보여준다.\nFY25 실적 (2025년 2월 발표)\n지표 성과 신규 고객 전년 대비 20% 증가 Galaxy(SaaS) 고객 전년 대비 76% 증가 Galaxy 사용량 전년 대비 94% 증가 최대 계약 글로벌 금융기관과 8자리(억 단위) 다년 계약 파트너십 Dell Data Lakehouse의 쿼리 엔진으로 선정 Galaxy(SaaS) 고객이 76% 늘었다는 건 눈여겨볼 만하다. 클라우드 매니지드 서비스 전환이 빨라지고 있다는 뜻이고 오픈소스 Trino를 직접 운영하는 팀 입장에서는 대안이 되기도 한다.\n고객사 면면도 인상적이다.\nHSBC. 165개국 데이터 통합 Citi. 글로벌 데이터 주권을 유지하며 통합 분석 Vectra AI. 120개국 위협 탐지 플랫폼 ZoomInfo. 멀티클라우드 데이터 통합 경쟁 환경에서 어디에 서 있는가 # 데이터 플랫폼 시장에서 Starburst를 경쟁사와 비교하면 차별화 지점이 드러난다.\n기능 Databricks Snowflake Dremio Starburst AI Agent O O X O Federated Query 제한적 제한적 O 핵심 강점 Data Sovereignty 제한적 제한적 제한적 핵심 강점 오픈소스 기반 Spark X Arrow Trino Vector Store O O X O (Iceberg) On-prem + Cloud O 제한적 O 핵심 강점 Starburst가 내세우는 차별화는 두 축이다.\n하나는 페더레이션과 데이터 주권. 50개 이상 데이터 소스에 실시간 쿼리를 지원하면서 데이터를 이동시키지 않는다. 165개국 규제를 준수하면서 통합 분석을 제공하는 건 GDPR, Schrems II 환경에서 결정적이다.\n다른 하나는 하이브리드 배포. On-premise와 멀티 클라우드를 동시에 지원한다. 규제 산업에서 클라우드 전환이 더딘 기업에게 강하게 어필한다.\nDatabricks는 Spark + Delta Lake 중심 AI/ML Lakehouse로 Unity Catalog를 통해 거버넌스를 강화하고 있다. Snowflake는 2024년 Iceberg 지원을 추가하고 Snowpark으로 AI/ML을 밀고 있지만 여전히 데이터 중앙화가 전제다. Dremio는 Arrow Flight 기반 성능과 시맨틱 레이어를 내세우지만 엔터프라이즈 기능에서는 아직 격차가 있다.\n흥미로운 건 Starburst CEO Justin Borgman이 한 말이다. \u0026ldquo;What they\u0026rsquo;ve done for Spark is what we aim to do for Presto(Trino).\u0026rdquo; Databricks가 Spark 오픈소스 위에 강력한 상용 플랫폼을 구축한 것처럼 Starburst도 Trino 위에 같은 구조를 만들겠다는 거다.\nTrino 오픈소스, 괜찮을 것인가 # 오픈소스와 상용 기능이 갈라지고 있다 # 지금 Trino 오픈소스에 남아있는 것과 Starburst 전용으로 넘어간 것을 정리하면 이렇다.\nTrino 오픈소스:\n핵심 쿼리 엔진 기본 커넥터 Fault-tolerant execution SQL MERGE 기본 보안 기능 Starburst 전용:\nWarp Speed (최대 7배 성능 향상) AI Agent \u0026amp; AI Workflows Starburst Data Catalog 고급 거버넌스 (RBAC, 데이터 마스킹, 감사 로그) Automated Table Maintenance Smart Indexing Materialized Views (일부) 가장 눈에 띄는 건 Warp Speed다. 최대 7배 성능을 끌어올리는 독점 인덱싱/캐싱 레이어가 상용 전용이라는 건 대규모 워크로드에서 오픈소스와 상용 제품 사이 성능 차이가 점점 벌어질 수 있다는 뜻이다.\n낙관적으로 볼 수 있는 근거 # Trino 코어 엔진은 이미 성숙 단계다. 분산 SQL 쿼리 엔진으로서 필요한 기능은 대부분 갖추고 있다. 릴리스 빈도가 줄었다고 품질이 떨어지는 건 아니다. 커뮤니티는 여전히 활발하다. Trino Summit 2024에는 Netflix, LinkedIn, Wise 등이 참여했고 Trino Community Broadcast도 계속 운영되고 있다. 50곳 이상 기업이 기여하고 있다. Starburst 기여가 줄더라도 다른 기업이 메울 여지는 있다. 우려할 점 # 커밋의 84%를 담당하던 회사가 다른 곳에 집중하기 시작했다. 나머지 기업이 이 공백을 메울 동기가 충분한지는 불확실하다. 성능 최적화 핵심이 상용 전용이다. Warp Speed 없이 대규모 워크로드를 운영하는 팀은 갈수록 불리해질 수 있다. AI 관련 새 기능이 전부 상용 제품에 몰려 있다. 데이터 플랫폼에 AI가 필수가 되는 미래에서 오픈소스만으로는 경쟁력 확보가 어려워질 수 있다. Trino 운영 팀이 고려해야 할 것 # Trino를 프로덕션에서 운영하는 팀 입장에서 이 상황을 시간 축 두 개로 나눠 생각해볼 수 있다.\n단기 (1~2년). 큰 문제 없다 # Trino 오픈소스는 여전히 안정적이고 프로덕션에서 검증된 기술이다. 핵심 기능은 충분히 성숙했고 기본 쿼리 성능과 커넥터 생태계는 탄탄하다. 당장 대안으로 갈아타야 할 이유는 없다.\n중장기 (3~5년). 전략적 대비가 필요하다 # 오픈소스와 상용 제품 사이 기능 격차가 벌어질 가능성을 감안해야 한다. 특히 다음 영역에서 대비가 필요하다.\n성능 최적화. Warp Speed 없이 대규모 워크로드 성능을 어떻게 확보할 건지. 자체 캐싱 레이어나 인덱싱 전략을 검토하고 StarRocks 같은 보완 엔진 도입도 따져봐야 한다. AI 통합. 데이터 플랫폼에 AI를 통합하는 게 조직 요구사항이 될 때 오픈소스 Trino만으로 충분한지 평가해야 한다. Vector Store on Iceberg 같은 접근을 직접 구현할 수 있는지, 다른 도구와 조합이 필요한지 따져봐야 한다. 거버넌스. 조직이 커지고 규제가 강화될수록 고급 거버넌스 기능(RBAC, 데이터 마스킹, 감사 로그)이 더 절실해진다. 오픈소스만으로 충족할 수 있는지 따져봐야 한다. 대안 평가. Starburst Galaxy 도입이나 다른 쿼리 엔진으로 전환, 하이브리드 접근(배치는 Trino, 실시간은 StarRocks) 등을 주기적으로 비교 평가해야 한다. 마치며 # Starburst의 AI 피벗은 단순한 마케팅이 아니다. Galaxy 고객 76% 증가, 역대 최대 계약 등 비즈니스 지표가 이 전략이 시장에서 먹히고 있음을 보여준다. 쿼리 엔진 회사에서 AI 플랫폼 기업으로 전환하는 흐름은 되돌리기 어렵다.\nTrino 오픈소스는 당장 죽지 않는다. 하지만 \u0026ldquo;충분히 성숙한\u0026rdquo; 상태로 유지보수 모드에 가까워지고 있고 새 기능 개발의 무게 중심은 분명히 상용 제품으로 옮겨갔다. Databricks가 Spark에 대해 했던 것과 같은 패턴이다.\nTrino를 프로덕션에서 운영하는 팀이라면 지금 당장은 안심해도 되지만 3년 뒤를 위한 대비는 지금 시작해야 한다. 오픈소스가 주는 안정성에 기대면서도 성능 격차와 AI 통합이라는 두 축에서 선택지를 확보해두는 게 현명하다.\n기술 부채는 늘 조용히 쌓인다. 이자는 언제나 우리가 생각했던 것보다 비싸다.\n참고 자료:\nTrino Release Notes Starburst Enterprise Release Notes TechTarget: Addition of new AI capabilities shows Starburst\u0026rsquo;s growth BigDataWire: Starburst\u0026rsquo;s New Platform Aims to Close AI\u0026rsquo;s Biggest Gap ","date":"2026년 2월 23일","externalUrl":null,"permalink":"/posts/starburst-trino-ai-pivot/","section":"글 목록","summary":"Starburst가 쿼리 엔진 회사에서 AI 플랫폼 기업으로 전환하면서 Trino 오픈소스 릴리스가 63% 감소했다. Trino를 프로덕션에서 운영하는 팀의 관점에서, 이 변화가 의미하는 것과 앞으로의 전략을 정리한다.","title":"Starburst의 AI 피벗: Trino 오픈소스는 괜찮을까?","type":"posts"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/starrocks/","section":"Tags","summary":"","title":"Starrocks","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/categories/starrocks/","section":"Categories","summary":"","title":"StarRocks","type":"categories"},{"content":" 도입 배경 # 데이터 파이프라인을 운영하다보면 한 가지 고민에 반드시 부딪힌다. 실시간 대시보드를 어떻게 만들 것인가?\n우리 팀도 똑같았다. 기존 파이프라인 구조는 이랬다.\nService → Kafka → Iceberg → S3 → Trino → Airflow(5분) → Dashboard 겉보기엔 잘 돌아갔지만 실무에서 체감하는 문제는 분명했다.\n최소 5분 지연. Airflow 스케줄 주기가 병목이었다 파이프라인 복잡도. Kafka → Flink → Redis → API → Dashboard까지 컴포넌트 5개 이상을 관리해야 했다 반복되는 I/O. Trino가 매 쿼리마다 S3를 풀스캔했다 높은 개발 비용. 실시간 대시보드 하나 새로 만드는 데 약 2주 걸렸다 StarRocks를 도입한 뒤 아키텍처가 이렇게 바뀌었다.\nService → Kafka → StarRocks → Dashboard (서브초 레이턴시) 중간 컴포넌트가 빠지면서 파이프라인이 크게 단순해졌다. Kafka에서 StarRocks로 바로 넣으니 실시간성도 갖출 수 있었다.\n도입 효과 # 약 3개월간 PoC를 거치고 6개월에 걸쳐 단계적으로 도입한 결과다.\n항목 Before After 개선폭 대시보드 지연 5분 \u0026lt; 1초 ~300배 대시보드 개발 기간 ~2주 ~1주 50% 단축 파이프라인 컴포넌트 5개 이상 2개 60% 감소 쿼리 응답 시간 30~50초 5~10초 5~10배 하드웨어 비용 128GB × 18노드 64GB × 3노드 ~75% 절감 Trino는 절대적인 쿼리 시간에서는 빠르지만 Airflow 스케줄 지연까지 포함한 end-to-end 레이턴시와 하드웨어 비용 면에서 StarRocks가 실시간 워크로드에 더 맞았다.\n테이블 모델 선택 가이드 # StarRocks를 처음 도입할 때 가장 신경 써야 할 결정이 테이블 모델이다. 잘못 고르면 나중에 테이블을 다시 만들어야 한다.\n의사결정 흐름 # ┌─────────────────────────────┐ │ 어떤 데이터를 저장하는가? │ └──────────────┬──────────────┘ │ ┌───────▼────────┐ │ UPDATE 필요? │ └───────┬────────┘ │ ┌────────┴────────┐ │ │ [아니오] [예] │ │ ┌─────▼─────┐ ┌─────▼──────┐ │ 집계 필요? │ │ Primary Key │ └─────┬─────┘ │ (빈번한 │ │ │ UPDATE) │ [아니오] [예] └────────────┘ │ │ ┌─────▼───┐ ┌▼──────────┐ │Duplicate│ │Aggregate │ │(원본 저장)│ │(자동 집계) ★│ └─────────┘ └────────────┘ 모델 비교 # 모델 중복 허용 UPDATE 자동 집계 적합한 용도 Duplicate Key O X X 로그, 원본 이벤트 Aggregate Key X 자동 O 실시간 통계 ★ Primary Key X O (고속) X 빈번한 UPDATE Duplicate Key. 원본 데이터 저장 # 클릭 로그나 API 이벤트, 센서 데이터처럼 원본을 그대로 보관해야 할 때 쓴다.\nCREATE TABLE order_events ( event_id BIGINT, event_time DATETIME, order_id VARCHAR(50), user_id BIGINT, event_type VARCHAR(20), amount DECIMAL(10, 2) ) DUPLICATE KEY(event_id, event_time) PARTITION BY date_trunc(\u0026#39;day\u0026#39;, event_time) DISTRIBUTED BY HASH(event_id) BUCKETS 10; Aggregate Key. 실시간 통계 ★ # 데이터가 수집되는 시점에 자동으로 집계가 일어난다. StarRocks 도입에서 가장 값어치있었던 모델이다.\nCREATE TABLE order_stats ( stat_time DATETIME NOT NULL COMMENT \u0026#39;5분 간격\u0026#39;, region VARCHAR(20) NOT NULL, delivery_type VARCHAR(20) NOT NULL, -- 집계 컬럼: 수집 시 자동으로 집계 함수 적용 order_count BIGINT SUM DEFAULT \u0026#34;0\u0026#34;, total_amount DECIMAL(15, 2) SUM DEFAULT \u0026#34;0\u0026#34;, user_bitmap BITMAP BITMAP_UNION, max_amount DECIMAL(10, 2) MAX ) AGGREGATE KEY(stat_time, region, delivery_type) PARTITION BY date_trunc(\u0026#39;day\u0026#39;, stat_time) DISTRIBUTED BY HASH(stat_time) BUCKETS 10; 사용 가능한 집계 함수.\n함수 용도 예시 SUM 합계 주문 건수, 매출 합계 MAX / MIN 최대/최소값 최고가, 최저가 REPLACE 최신 값 덮어쓰기 최종 상태 BITMAP_UNION 정확한 유니크 카운트 순 이용자 수 HLL_UNION 근사 유니크 카운트 대규모 카디널리티 BITMAP_UNION은 HyperLogLog와 달리 정확한 유니크 카운트를 제공한다. 비즈니스 KPI 대시보드처럼 정확도가 중요하면 반드시 이걸 쓰자.\nPrimary Key. 빈번한 UPDATE # 주문 상태를 추적하거나 재고를 관리하는 것처럼 같은 키 데이터가 자주 갱신되는 경우에 맞다.\nCREATE TABLE orders ( order_id VARCHAR(50) NOT NULL, status VARCHAR(20), amount DECIMAL(10, 2), updated_at DATETIME ) PRIMARY KEY(order_id) DISTRIBUTED BY HASH(order_id) BUCKETS 10 PROPERTIES ( \u0026#34;enable_persistent_index\u0026#34; = \u0026#34;true\u0026#34; ); enable_persistent_index를 켜면 UPDATE 성능이 크게 좋아진다.\n데이터 수집 # Routine Load. Kafka 실시간 연동 # Kafka 토픽에서 데이터를 연속으로 가져오는 방식이다. 실시간 파이프라인 대부분이 이걸 쓴다.\nCREATE ROUTINE LOAD order_load ON orders COLUMNS( order_id, user_id, timestamp_ms, amount, order_date = FROM_UNIXTIME(timestamp_ms / 1000) ) PROPERTIES ( \u0026#34;format\u0026#34; = \u0026#34;json\u0026#34;, \u0026#34;jsonpaths\u0026#34; = \u0026#34;[\\\u0026#34;$.orderId\\\u0026#34;,\\\u0026#34;$.userId\\\u0026#34;,\\\u0026#34;$.timestamp\\\u0026#34;,\\\u0026#34;$.amount\\\u0026#34;]\u0026#34; ) FROM KAFKA ( \u0026#34;kafka_broker_list\u0026#34; = \u0026#34;kafka-broker:9092\u0026#34;, \u0026#34;kafka_topic\u0026#34; = \u0026#34;orders\u0026#34; ); Aggregate Key 테이블과 결합하면 수집 시점에 변환과 집계를 한번에 처리할 수 있다.\nCREATE ROUTINE LOAD order_stats_load ON order_stats COLUMNS( timestamp_ms, region, amount, user_id, -- 5분 간격으로 라운딩 stat_time = FROM_UNIXTIME(FLOOR(timestamp_ms / 1000 / 300) * 300), order_count = 1, total_amount = amount, user_bitmap = BITMAP_HASH(user_id) ) WHERE amount \u0026gt; 0 PROPERTIES (\u0026#34;format\u0026#34; = \u0026#34;json\u0026#34;) FROM KAFKA ( \u0026#34;kafka_broker_list\u0026#34; = \u0026#34;kafka-broker:9092\u0026#34;, \u0026#34;kafka_topic\u0026#34; = \u0026#34;orders\u0026#34; ); 이 패턴 하나로 기존에 Flink로 처리하던 집계 로직을 SQL만으로 대체했다.\nStream Load. 벌크 데이터 로딩 # 파일이나 API로 한번에 대량 로딩할 때 쓴다.\n# CSV 파일 로딩 curl --location-trusted \\ -u user:password \\ -H \u0026#34;label:load_$(date +%Y%m%d%H%M%S)\u0026#34; \\ -H \u0026#34;column_separator:,\u0026#34; \\ -T data.csv \\ http://starrocks-fe:8030/api/mydb/mytable/_stream_load 성능 튜닝 실전 팁 # Thread Pool 설정 # 동시 접속이 500 RPS 이상인 고부하 환경에서는 기본 Thread Pool 크기가 부족하다.\n# be.conf pipeline_scan_thread_pool_thread_num = 32 # 기본값: 24 pipeline_exec_thread_pool_thread_num = 32 # 기본값: 24 Bucket Count 가이드라인 # 데이터 크기 권장 Bucket 수 \u0026lt; 10 GB 10 10~50 GB 20 50~100 GB 30 \u0026gt; 100 GB 50+ 산정 공식. buckets = max(1, 데이터_크기_GB / 10)\n파티셔닝 전략 # 파티션 컬럼에 함수를 쓰면 파티션 프루닝이 안 된다. 생각보다 자주 실수하는 부분이다.\n-- ✅ 올바른 사용: 파티션 프루닝 동작 WHERE event_time \u0026gt;= NOW() - INTERVAL 3 DAY -- ❌ 잘못된 사용: 파티션 프루닝 불가 WHERE DATE(event_time) \u0026gt;= CURRENT_DATE - 3 TTL 설정 # 오래된 파티션을 자동 삭제하려면 TTL을 건다.\nPROPERTIES ( \u0026#34;partition_live_number\u0026#34; = \u0026#34;3\u0026#34; -- 최근 3개 파티션만 유지 ) 운영 노하우 # Materialized View 관리 # ASYNC 리프레시가 예고 없이 멈출 때가 있다. 정기적으로 상태를 확인하고 문제가 생기면 수동으로 복구해야 한다.\n-- 상태 확인 SHOW MATERIALIZED VIEWS; -- 강제 동기 리프레시 REFRESH MATERIALIZED VIEW db.mv_name WITH SYNC MODE; -- 비활성화된 MV 재활성화 ALTER MATERIALIZED VIEW db.mv_name ACTIVE; Routine Load 모니터링 # 상태가 PAUSED로 바뀌는 경우가 잦다. Kafka offset이 꼬이거나 비정상 메시지가 들어올 때 그렇다.\n-- 상태 확인 SHOW ROUTINE LOAD FOR db.load_job; -- 재개 RESUME ROUTINE LOAD FOR db.load_job; Scale-in 주의사항 # 노드를 축소할 때는 반드시 Decommission을 먼저 해야 한다. 이 절차를 빼먹고 노드를 줄이면 데이터가 유실된다.\n-- 1. 현재 노드 확인 SHOW PROC \u0026#39;/backends\u0026#39;; -- 2. 디커미션 시작 ALTER SYSTEM DECOMMISSION BACKEND \u0026#34;\u0026lt;BE_IP\u0026gt;:\u0026lt;HEARTBEAT_PORT\u0026gt;\u0026#34;; -- 3. TabletNum이 0이 될 때까지 대기 후 제거 ALTER SYSTEM DROP BACKEND \u0026#34;\u0026lt;BE_IP\u0026gt;:\u0026lt;HEARTBEAT_PORT\u0026gt;\u0026#34;; 도입 시 알아둘 점 # 알려진 제약 사항 # 이슈 설명 대안 Routine Load 비정상 메시지 처리 한계 Kafka 단에서 사전 검증 datetime 파티션 Iceberg datetime 파티션 호환 이슈 대체 파티션 전략 사용 버전 업그레이드 4.x 대에서 버그 경험 스테이징 환경 필수 테스트 버전 업그레이드는 반드시 스테이징에서 충분히 검증하고 프로덕션에 적용하자. 실제로 여러 차례 업그레이드와 다운그레이드를 반복한 적이 있다. 롤백 계획은 항상 준비해두자.\n도입 체크리스트 # 배포 전\n유스케이스와 요구사항 정의 데이터 볼륨 및 증가량 추정 테이블 모델 선택 파티션 전략 설계 배포 후\nRoutine Load 작업 생성 및 검증 사용자 권한 설정 데이터 보관 정책(TTL) 설정 Scale-in/out 절차 문서화 모니터링 대시보드 구성 마치며 # StarRocks를 도입하면서 배운 것을 정리한다.\nAggregate Key 모델이 제일 쓸모있다. 수집 시점에 자동 집계되니까 스토리지와 쿼리 성능을 동시에 잡을 수 있다. BITMAP_UNION으로 정확한 유니크 카운트를 확보하자. 비즈니스 KPI에는 근사치가 아니라 정확한 수치가 필요하다. Routine Load + Aggregate Key 조합이 Flink를 대체한다. SQL만으로 실시간 집계 파이프라인을 구축할 수 있다. 운영 자동화에 투자하자. Materialized View와 Routine Load 모니터링은 빠뜨리면 안 된다. 실시간 분석 워크로드에서 StarRocks는 파이프라인 복잡도를 크게 낮춰준다. 다만 버전 업그레이드나 운영 안정성 쪽은 아직 무르익는 단계라서 충분히 PoC하고 스테이징에서 검증한 뒤 도입하길 권한다.\n참고 자료: StarRocks 공식 문서\n","date":"2026년 2월 23일","externalUrl":null,"permalink":"/posts/starrocks-adoption-guide/","section":"글 목록","summary":"기존 Trino + Airflow 파이프라인의 5분 지연을 서브초 레이턴시로 개선한 StarRocks 도입 과정을 정리했습니다. 테이블 모델 선택, 데이터 수집, 성능 튜닝, 운영 노하우까지 실무에서 얻은 교훈을 공유합니다.","title":"StarRocks 도입기: 실시간 OLAP으로 데이터 파이프라인을 뜯어고친 이야기","type":"posts"},{"content":" 왜 압축 설정을 신경 써야 하나 # StarRocks를 운영하다 보면 데이터가 수십 TB 규모로 불어나는 시점이 꼭 온다. 이때 압축 설정 하나로 스토리지 비용이 30~50%씩 벌어지는 걸 여러 번 겪었다. 저장 공간만의 문제가 아니다. 압축률이 높으면 디스크 I/O가 줄어들어 스캔 성능이 올라가고 반대로 압축/해제에 CPU를 많이 잡아먹으면 지연 시간이 늘어난다. 워크로드 특성에 맞춰 압축 알고리즘을 고르는 게 StarRocks 튜닝에서 빠질 수 없는 부분이다.\n지원되는 압축 알고리즘 비교 # StarRocks는 여러 압축 알고리즘을 지원한다. 실무에서 주로 쓰는 네 가지를 비교해 본다.\n알고리즘 압축률 압축 속도 해제 속도 적합한 워크로드 LZ4 보통 (2~3x) 매우 빠름 매우 빠름 실시간 분석, 저지연 쿼리 ZSTD 높음 (4~6x) 보통 빠름 배치 분석, 콜드 데이터 Snappy 낮음 (1.5~2x) 빠름 빠름 범용, 레거시 호환 ZLIB 높음 (4~5x) 느림 보통 아카이빙, 저빈도 접근 데이터 개인적으로 가장 많이 쓰는 조합은 핫 데이터에 LZ4, 콜드 데이터에 ZSTD다. Snappy는 Hadoop 에코시스템에서 넘어온 데이터 다룰 때 간혹 쓰는데 신규 테이블엔 굳이 권하지 않는다.\n테이블 생성 시 압축 설정 # 테이블 만들 때 PROPERTIES에서 compression 속성을 지정하면 된다. 따로 안 잡으면 기본값인 LZ4가 적용된다.\n실시간 분석용 테이블 (LZ4) # CREATE TABLE analytics.realtime_events ( event_id BIGINT, user_id BIGINT, event_type VARCHAR(64), event_time DATETIME, properties JSON ) ENGINE = OLAP DUPLICATE KEY(event_id) DISTRIBUTED BY HASH(user_id) BUCKETS 32 PROPERTIES ( \u0026#34;replication_num\u0026#34; = \u0026#34;3\u0026#34;, \u0026#34;compression\u0026#34; = \u0026#34;LZ4\u0026#34; ); LZ4는 해제 속도가 압도적으로 빨라서 대시보드 쿼리처럼 수백 밀리초 안에 응답해야 하는 테이블에 잘 맞는다.\n배치 분석용 테이블 (ZSTD) # CREATE TABLE warehouse.order_history ( order_id BIGINT, customer_id BIGINT, order_date DATE, total_amount DECIMAL(18, 2), status VARCHAR(32), items ARRAY\u0026lt;STRUCT\u0026lt;sku STRING, qty INT, price DECIMAL(10,2)\u0026gt;\u0026gt; ) ENGINE = OLAP DUPLICATE KEY(order_id) PARTITION BY RANGE(order_date) ( PARTITION p2025 VALUES LESS THAN (\u0026#39;2026-01-01\u0026#39;), PARTITION p2026 VALUES LESS THAN (\u0026#39;2027-01-01\u0026#39;) ) DISTRIBUTED BY HASH(customer_id) BUCKETS 16 PROPERTIES ( \u0026#34;replication_num\u0026#34; = \u0026#34;2\u0026#34;, \u0026#34;compression\u0026#34; = \u0026#34;ZSTD\u0026#34; ); ZSTD는 압축률이 LZ4보다 1.5~2배 높다. 파티션 단위로 수억 건 이상 쌓이는 히스토리 테이블에서 스토리지 절감 효과가 확실히 드러난다.\n기존 테이블 압축 변경 # 이미 돌아가는 테이블의 압축 알고리즘을 바꾸려면 ALTER TABLE을 쓰면 된다. 다만 바꾼 뒤 새로 적재되는 데이터부터 적용되고 기존 세그먼트는 Compaction이 끝나야 반영된다.\nALTER TABLE warehouse.order_history SET (\u0026#34;compression\u0026#34; = \u0026#34;ZSTD\u0026#34;); 워크로드별 권장 압축 설정 # 실무에서 여러 차례 검증한 결과를 바탕으로 정리하면 이렇다.\n실시간 대시보드 / Ad-hoc 쿼리. LZ4를 권한다. CPU 오버헤드가 거의 없어서 P99 지연 시간에 미치는 영향이 작다. 야간 배치 리포트 / ETL 결과 테이블. ZSTD를 권한다. 쿼리 빈도가 낮고 데이터양이 많으면 스토리지를 아낀만큼 비용에 바로 드러난다. 로그성 대용량 적재. ZSTD를 쓰되 zstd_compression_level을 3 이하로 낮추면 압축 속도와 압축률 사이 균형을 잡을 수 있다. 압축률과 성능 트레이드오프 실측 # 약 50억 건(원본 약 800GB) 이벤트 로그 테이블을 대상으로 압축 알고리즘별 벤치마크를 돌려봤다.\n지표 LZ4 ZSTD (level 3) ZSTD (level 9) 압축 후 크기 320 GB 195 GB 170 GB 압축률 2.5x 4.1x 4.7x 단순 스캔 쿼리 (Avg) 1.2초 1.5초 1.8초 집계 쿼리 (Avg) 3.4초 3.8초 4.5초 데이터 적재 속도 120 MB/s 95 MB/s 60 MB/s LZ4랑 비교하면 ZSTD level 3은 스토리지를 약 39% 줄이면서 쿼리 지연은 10~15%만 늘었다. 반면 ZSTD level 9는 추가로 줄어드는 용량 대비 적재 속도 저하가 커서 대부분 환경에서 level 3이 나은 선택이었다.\n운영 팁과 모니터링 # 마지막으로 압축 관련해서 운영할 때 놓치기 쉬운 부분을 짚어 본다.\nCompaction 모니터링은 꼭 해야 한다. 압축 알고리즘을 바꾼 뒤 Compaction이 안 끝난 상태에서 혼합 세그먼트가 남아 있으면 쿼리 성능이 일시적으로 흔들릴 수 있다. BE의 compaction_score 메트릭을 보면서 Compaction이 밀리고 있지 않은지 확인해야 한다.\n테이블 단위로 압축 전략을 나눠라. 클러스터 안에서 모든 테이블에 같은 압축을 거는 건 비효율적이다. 접근 빈도, 데이터 크기, SLA를 따져서 테이블마다 다르게 잡는 편이 낫다.\n디스크 사용량 추이를 추적하라. 압축을 바꾸고 나서 SHOW DATA 명령으로 테이블별 실제 디스크 사용량을 주기적으로 확인하자. 기대한 압축률이 안 나오면 데이터 특성(카디널리티, NULL 비율 등)을 다시 살펴봐야 한다.\nSHOW DATA FROM warehouse.order_history; 압축 설정은 한 번 정해놓고 끝낼 게 아니다. 데이터 특성과 워크로드가 바뀌면 거기에 맞춰 꾸준히 재검토해야 한다. 이 글이 StarRocks 압축 전략 세우는 데 참고가 됐으면 좋겠다.\n","date":"2026년 2월 23일","externalUrl":null,"permalink":"/posts/starrocks-compression-guide/","section":"글 목록","summary":"StarRocks에서 압축 설정을 최적화해 스토리지 비용을 절감하고 쿼리 성능을 올리는 방법을 실무 경험 바탕으로 정리했다.","title":"StarRocks 압축 설정 가이드: 성능과 스토리지 최적화","type":"posts"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/trends/","section":"Tags","summary":"","title":"Trends","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/vector-store/","section":"Tags","summary":"","title":"Vector-Store","type":"tags"},{"content":"","date":"2026년 2월 23일","externalUrl":null,"permalink":"/tags/workation/","section":"Tags","summary":"","title":"Workation","type":"tags"},{"content":"발리하면 떠오르는 이미지가 있다. 서핑, 열대 자연, 코워킹 스페이스에서 노트북을 펼친 디지털 노마드. 워케이션(workation)의 성지라고 불리는 곳이다.\n나도 그 이미지에 이끌렸다. 아이에게는 해외 유치원이라는 새 경험을 주고 가족에게는 일상에서 벗어난 해방감을 줄 수 있을 거라 기대했다. 2023년 초, 회사의 해외 리모트 근무 제도를 활용해 발리에서 약 한 달간 일하고 생활했다.\n결론부터 말하면, 아이가 있는 가정에게 발리 리모트 근무는 생각보다 별로였다. 이 글은 그 솔직한 기록이다. 발리 리모트 근무를 고민하는 분들, 특히 어린 아이가 있는 가정이라면 판단하는 데 도움이 됐으면 한다.\n출발 전: 준비해야 할것들 # 승인과 세무 이슈 # 해외 리모트 근무는 비행기표 사기 전에 회사 승인부터 받아야 한다. 승인 과정에서 일정이 바뀔 수 있기 때문이다.\n처음에 3개월 체류를 신청했는데 세무 검토 과정에서 세법상 2개월 이하만 가능하다는 회신을 받았다. 해외 체류 기간에 따라 세금 처리가 달라져서다. 비행기표를 이미 끊어놨다면 변경 수수료를 물어야 할 뻔했다. 반드시 승인 확정 후에 항공권을 구매하자.\n항공권은 대한항공 마일리지를 썼는데 비용 대비 차감되는 마일리지가 적어서 가성비가 좋았다.\n숙소. 빌라의 함정 # 발리 숙소는 크게 두 가지 선택지가 있다. 호텔이나 리조트를 전전하거나, 월 단위로 빌라를 빌리거나.\n호텔에서 3일 정도 지내본 뒤 수영장이 딸린 발리스러운 빌라를 월 단위로 계약했다. 사진으로 보면 정말 근사하다. 야자수 아래 프라이빗 풀, 넓은 거실, 열대 정원. 실제로 살아보면 다르다. 결국 빌라를 취소하고 다시 호텔로 돌아왔다.\n빌라의 현실.\n벌레가 많다. 개미와 모기는 기본이고 이름 모를 벌레가 출몰한다. 호텔이나 리조트는 방역을 하지만 빌라는 그렇지 않다. 침구류와 청소 상태가 들쭉날쭉하다. 관리 수준이 숙소마다 천차만별이다. 상수도 문제. 뒤에서 다시 다루겠지만 이게 가장 큰 문제다. 교통이 불편하다. 빌라가 저렴한 이유가 있다. 대부분 도심에서 떨어져 있고 발리에서 걸어다니는 건 사실상 불가능하다. 한 달 이상 체류한다면 빌라가 합리적으로 보이지만 특히 아이가 있다면 호텔이나 서비스드 레지던스가 낫다. 관리와 위생, 편의성 모든 면에서.\n위생. 발리의 가장 큰 리스크 # 인도네시아 여행 커뮤니티에서 가장 자주 나오는 토픽이 있다. \u0026ldquo;병원 어디 가야 해요?\u0026rdquo;, \u0026ldquo;약 좀 주세요\u0026rdquo;, \u0026ldquo;샤워기 필터 구합니다.\u0026rdquo;\n과장이 아니다.\n물 문제 # 발리는 지하수를 끌어다 쓰는 곳이 많다. 오염된 경우가 적지 않아서 비누로 빡빡 씻어도 몸이 계속 미끈미끈한 느낌이 가시지 않는다. 유럽의 석회수와는 또 다른 종류의 불쾌함이다.\n수영장은 발리 곳곳에 널려 있지만 소독약 냄새가 안 나는 곳이 많다. 소독약 냄새가 안 난다는 건 소독을 안 한다는 뜻이다. 세균과 박테리아가 번식하기 좋은 열대 기후에서.\n비치도 안심할 수 없다. 서핑 중 육지에서 흘러들어온 오염된 바닷물을 삼키고 아픈 사람이 많다. 카페에서 나오는 차가운 음료의 얼음이 정수된 물로 만든건지도 확신할 수 없다.\n아이의 입원 # 출국 전 나름 준비를 했다. 장티푸스 예방접종을 맞고 출국 전날 종합병원에서 피검사로 염증 수치와 백혈구 수치를 확인했다. 유산균도 2주 전부터 복용시켰다.\n도착 3일 만에 아이에게 고열이 찾아왔다.\n평소 38도 위로 올라간 적이 없던 아이가 39.5도까지 치솟았다. 응급실을 세 번 갔다. 세 번째 응급실에서는 귀가하겠다고 하니 \u0026ldquo;책임지지 않겠다\u0026quot;는 서명을 하고 가라고 했다. 결국 병동에 입원. 4일간 입원했다.\n이때 경험은 지금 돌이켜봐도 힘들다.\n기본적으로 낙후된 인도네시아 의료 시설에 대한 불신. 말이 잘 통하지도 않는다. 아이가 응급실에 있는데 비가 쏟아지면서 천장이 무너져 내렸다. 굉음과 함께 천장 조명이 떨어지고 놀라서 아이를 안고 뛰쳐나온 직후 천장이 내려앉았다. 그 지역에서 제일 좋다는 병원에서 벌어진 일이다. 응급실 한 번 방문에 검사비 포함 약 50만 원. 총 의료비 300만 원 이상. 여행자 보험 가입은 꼭 해야 한다. 보장 내용을 꼼꼼히 확인하고 의료비 보장 한도가 넉넉한 상품으로 들어야 한다. 솔직히 이 경험 이후 \u0026ldquo;뭘 위해 여기 있어야 하나\u0026ldquo;라는 생각이 머리에서 떠나지 않았다.\n아이와의 일상. 기대와 현실 # 할것이 없다 # 한국에서 아이와 하는 일상을 떠올려 보자. 책 읽어주기, 산책, 놀이터, 키즈카페, 장난감 놀이, 주말 캠핑. 발리에서는 이것 중 거의 아무것도 할 수 없다.\n책이 없다. 한글 동화책을 한 보따리 가져가지 않는 한 읽어줄 책이 없다. 산책이 안 된다. 발리에는 인도(보도)가 거의 없다. \u0026ldquo;3발자국 이상 걸으려면 오토바이를 불러야 한다\u0026quot;는 말이 과장이 아니다. 강렬한 햇빛 아래 매연을 맞으며 걷다보면 오토바이에 발이 깔릴것 같은 공포를 느낀다. 아이와 산책은 상상도 못 한다. 놀이터가 거의 없다. 한국처럼 아파트 단지마다 놀이터가 있는 환경이 아니다. 법적으로도 의무 시설이 아닌 듯하다. 아주 가끔 하나씩 보이는데 시설 수준이 한국에 비해 많이 떨어진다. 실내 놀이 환경이 없다. 집에는 익숙한 장난감이 있지만 여기에는 없다. 결국 한국에서는 안 보여줬던 유튜브를 보여주게 된다. 이게 현실이다.\n기대했던 유치원 # 발리에 호주 교민이 많다 보니 영어 기반 유치원이 잘 되어 있다. 아이에게 영어 환경을 경험시켜줄 수 있을 거라는 기대가 컸다.\n근데 아이 성향을 간과했다. 우리 아이는 소극적이고 먼저 다가가는 성격이 아니다. 원생 대부분이 영어를 쓰는 환경에서 2주 동안 혼자 노는 아이를 지켜봐야 했다. 결국 유치원을 그만 보내기로 했다. 기대가 컸던만큼 실망도 컸다.\n아이 성향에 따라 경험이 완전히 달라진다. 사교적이고 적응이 빠른 아이라면 좋은 경험이 될 수도 있겠지만 모든 아이가 그렇지는 않다. 아이 성격을 냉정하게 따져봐야 한다.\n나의 일상. Vacation이 아니라 Workation이다 # 없던 시간이 생겨나지 않는다 # 한국에서의 일상을 돌아보자. 부지런한 성격은 아니다. 아침에 일어나서 아이 밥 먹이고 유치원 보내고 일하다가 퇴근하면 아이와 놀아주고 아이가 잠들면 짧으면 한 시간 길면 두 시간의 자유 시간. 유튜브 보거나 밀린 일 하거나 운동하거나.\n발리라고 해서 없던 시간이 생기지 않는다. 하루에 내 시간이 한 시간인 사람이 장소를 옮겼다고 갑자기 세 시간이 되지 않는다. 오히려 출퇴근 시간이 늘었다.\n인터넷. VPN의 벽 # 집에서 일하는 건 사실상 불가능했다. 인터넷 속도가 느리고 정전이 잦고 수시로 끊긴다.\n코워킹 스페이스는 다르다. 괜찮은 코워킹 스페이스는 UPS와 자체 발전기를 갖추고 있고 여러 ISP와 계약해서 한쪽이 끊겨도 다른 회선으로 유지된다. 회사에서 제시한 최저 인터넷 속도를 대부분 만족한다.\n문제는 VPN이다. 회사 보안 정책상 VPN을 켜고 일해야 하는데 VPN을 켜는 순간 인터넷 속도가 원래의 10~20% 수준으로 떨어진다. VPN을 켜도 원활하게 일할 수 있는 코워킹 스페이스는 발리 전체에 몇 개 안 된다. 그런 곳은 하루 이용료가 2만 원 정도로 비싸고 숙소에서 가깝지도 않다.\n저녁 시간의 현실 # 퇴근 후 밤 시간. 서핑이나 야외 액티비티는 불가능한 시간이다. 유럽처럼 거리에서 뮤지션이 공연하는 문화도 아니다. 현실적으로 할 수 있는 건 괜찮은 음식점이나 바에서 맛있는 걸 먹고 술 한 잔 하는 정도.\n근데 이것도 녹록지 않다.\n술 문화가 발달하지 않았다. 이슬람 국가라는 배경 때문인지 크래프트 비어는 로컬 브루어리 2~3곳이 전부이고 해외 크래프트 비어는 아예 없다. 증류주는 비싸고 와인은 수입산이라 종류가 한정적이고 가격이 높다. 괜찮은 곳은 한국만큼 비싸다. 음료를 따로 시키고 세금이 15~20% 붙는다. 매일 괜찮은 곳에서 먹기엔 통장 잔고가 부담스럽다. 저렴한 곳에서 먹다보면 남의 나라까지 와서 왜 이 고생을 하고 있나 싶다. 도착 2주 만에 통장 잔고가 급격히 줄어드는 걸 목격했다.\n그래도 좋았던것 # 부정적인 이야기만 한 것 같아서 좋았던것도 솔직하게 적는다.\n코워킹 스페이스 # 발리의 코워킹 스페이스 문화는 확실히 눈에 띄었다. PC방 같기도 하고 도서관 같기도 하고 카페 같기도 한 공간. 여기는 이런 분위기구나, 저기는 이런 메뉴가 맛있네 하면서 코워킹 스페이스를 구경하는 것 자체가 재미있었다. 전 세계에서 온 디지털 노마드와 같은 공간에서 일하는 느낌도 나쁘지 않았다.\n비치클럽 # 뒤에는 수영장, 앞에는 바다. 음악과 맛있는 음식. 발리의 비치클럽은 확실히 특별한 경험이다. 젊은 사람이 즐기는 모습을 구경하는 것만으로도 활력이 됐다.\n주말 # 주말에는 비로소 내가 발리에 놀러 온 것 같았다. 투어도 하고 비치클럽도 가고 서핑도 한다. 평일에는 느낄 수 없었던 발리의 매력이 주말에 몰아서 찾아온다. 역설적이지만 이게 \u0026ldquo;워케이션\u0026quot;이라는 단어의 정확한 뜻이었다. work과 vacation이 동시에 오는 게 아니라 번갈아 오는 것이다.\n정리. 발리 리모트 근무, 누구에게 맞는가 # 한 달간의 경험을 정리하면 발리 리모트 근무의 만족도는 생활 방식에 따라 크게 갈린다.\n조건 만족도 이유 미혼 or 커플 (아이 없음) 높음 액티비티, 자유 시간, 유연한 일정 아이가 있는 가정 낮음 위생 리스크, 할것 없음, 시간 부족 VPN 불필요한 업무 높음 코워킹 스페이스 활용이 자유로움 VPN 필수 업무 보통 코워킹 스페이스 선택지 제한 넉넉한 예산 높음 좋은 숙소 + 좋은 음식 = 좋은 경험 빠듯한 예산 낮음 저렴한 곳 전전하다 지침 가기 전에 체크할것 # 아이가 있는 가정이 그래도 가겠다면.\n여행자 보험. 의료비 보장 한도를 반드시 확인. 응급실 한 번에 50만 원이 나올 수 있다. 장티푸스 예방접종. 출국 최소 2주 전에 접종. 숙소. 빌라보다 호텔이나 서비스드 레지던스. 위생과 관리가 다르다. VPN 테스트. 회사 VPN을 켠 상태에서 일할 수 있는 코워킹 스페이스를 미리 조사. 아이 준비물. 한글 책, 장난감, 태블릿에 오프라인 콘텐츠 다운로드. 유치원. 아이 성향을 냉정하게 판단. 소극적인 아이에게 영어 유치원은 스트레스가 될 수 있다. 예산. 한국 생활비 + 30~50% 여유를 잡아야 한다. 발리가 저렴하다는 건 로컬 음식과 로컬 숙소 기준이다. 한국인이 만족할 수준으로 생활하려면 한국과 비슷하거나 더 들 수 있다. 마치며 # 돌아와서 생각해보면 발리 리모트 근무에서 가장 크게 느낀 건 하나다.\n장소를 바꾼다고 삶이 바뀌지 않는다. 하루에 자유 시간이 한 시간인 사람은 발리에서도 한 시간이다. 아이를 돌봐야 하는 부모는 발리에서도 아이를 돌봐야 한다. 거기에 위생 리스크와 의료 불안, 인프라 불편까지 얹어진다.\n그럼에도 이 경험을 후회하지는 않는다. 해보지 않았으면 계속 궁금했을 것이고 발리에 대한 막연한 환상을 품고 살았을 것이다. 환상을 현실로 확인한 것 자체가 가치 있었다.\n다만 같은 조건으로 다시 해외 리모트 근무를 한다면 발리는 아닐 것이다. 의료 인프라가 탄탄하고 아이와 산책할 수 있는 도시. 보도가 있고 놀이터가 있고 수돗물을 믿을 수 있는 곳. 일상의 기본이 갖춰진 도시에서의 워케이션이 진짜 워케이션이다.\n","date":"2026년 2월 23일","externalUrl":null,"permalink":"/posts/bali-remote-work-with-family/","section":"글 목록","summary":"서핑, 자연, 디지털 노마드의 성지 발리. 아이를 데리고 한 달간 리모트 근무를 했다. 기대했던것과 실제 경험 사이의 간극을 솔직하게 기록한다.","title":"발리 리모트 근무 후기: 아이와 함께한 한 달, 솔직한 기록","type":"posts"},{"content":" 운영자 정보 # 본 블로그는 nanta가 운영하며, 사이트 주소는 https://nanta-data.dev/ 입니다.\n수집하는 정보 # Google Analytics # 본 사이트는 방문자의 이용 현황을 파악하기 위해 Google Analytics 4(GA4)를 사용합니다. GA4는 다음 정보를 수집합니다:\n방문한 페이지 및 체류 시간 대략적인 위치 정보 (국가/도시 수준) 기기 유형, 브라우저, 운영체제 유입 경로 (사이트를 어떻게 찾았는지) Google Analytics는 쿠키를 사용하여 고유 방문자를 구분합니다. 개인을 식별할 수 있는 정보는 의도적으로 수집하지 않습니다.\n자세한 내용은 Google 개인정보처리방침을 참고하세요.\nGoogle AdSense # 본 사이트는 Google AdSense를 통해 광고를 게재할 수 있습니다. AdSense는 쿠키와 웹 비콘을 사용하여 이전 방문 기록을 기반으로 광고를 제공합니다.\nGoogle은 DART 쿠키를 사용하여 브라우징 기록에 기반한 광고를 제공합니다 Google 광고 설정에서 맞춤 광고를 비활성화할 수 있습니다 자세한 내용은 Google AdSense 개인정보처리방침을 참고하세요.\n댓글 # 본 사이트는 현재 댓글 기능을 제공하지 않습니다. 향후 댓글 기능이 추가되면 본 방침을 업데이트하겠습니다.\n쿠키 # 쿠키는 사용자의 기기에 저장되는 작은 텍스트 파일입니다. 본 사이트는 다음 목적으로 쿠키를 사용합니다:\n분석: 사이트 트래픽 측정 (Google Analytics) 광고: 관련 광고 제공 (Google AdSense) 환경설정: 테마 설정 기억 (다크/라이트 모드) 브라우저 설정을 통해 쿠키를 제어할 수 있습니다. 쿠키를 비활성화하면 사이트 기능에 영향을 줄 수 있습니다.\n외부 링크 # 블로그 게시물에는 외부 웹사이트로의 링크가 포함될 수 있습니다. 외부 사이트의 개인정보 보호 관행에 대해서는 책임지지 않습니다.\n이용자의 권리 # 이용자는 다음 권리를 가집니다:\n수집되는 데이터에 대해 알 권리 추적 비활성화 (브라우저 설정 또는 Google 광고 설정) 본인 데이터에 대한 정보 요청 방침 변경 # 본 개인정보처리방침은 수시로 업데이트될 수 있습니다. 변경 사항은 이 페이지에 게시됩니다.\n문의 # 본 개인정보처리방침에 대해 궁금한 점이 있으시면 소개 페이지의 연락처로 문의해 주세요.\n최종 수정일: 2026년 2월 23일\n","externalUrl":null,"permalink":"/privacy-policy/","section":"nanta - 데이터 엔지니어링","summary":"개인정보처리방침","title":"개인정보처리방침","type":"page"},{"content":" 안녕하세요, nanta입니다 # 저는 안정적이고 확장 가능한 데이터 인프라와 파이프라인을 구축하는 데이터 엔지니어입니다. 대규모 데이터의 수집, 변환, 서빙 과정에서 발생하는 다양한 문제를 해결하는 일에 보람을 느끼고 있습니다.\n블로그 소개 # 이 블로그는 실무에서 얻은 데이터 엔지니어링 지식을 공유하는 공간입니다. 실제 프로젝트에서 배운 교훈, 튜토리얼, 아키텍처 관련 의사결정, 그리고 데이터 분야에서 일하는 엔지니어분들께 도움이 될 만한 팁들을 다루고 있습니다.\n기술 스택 # 현재 주로 사용하고 있는 기술들입니다:\n스트림 처리: Apache Kafka 워크플로 오케스트레이션: Apache Airflow 쿼리 엔진: Trino, StarRocks 컨테이너 오케스트레이션: Kubernetes 언어: Python, SQL 인프라: Docker 연락처 # 이메일: nanta0032@naver.com 궁금한 점이나 제안 사항이 있으시면 편하게 연락 주세요!\n","externalUrl":null,"permalink":"/about/","section":"nanta - 데이터 엔지니어링","summary":"블로그 소개","title":"소개","type":"page"}]